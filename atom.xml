<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Louis Li&#39;s Blog</title>
  
  
  <link href="https://blog.louisif.me/atom.xml" rel="self"/>
  
  <link href="https://blog.louisif.me/"/>
  <updated>2022-11-01T13:01:44.179Z</updated>
  <id>https://blog.louisif.me/</id>
  
  <author>
    <name>Louis Li</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Proxmox 安裝 openvpn client 紀錄</title>
    <link href="https://blog.louisif.me/notes/Install-openvpn-client-on-proxmox/"/>
    <id>https://blog.louisif.me/notes/Install-openvpn-client-on-proxmox/</id>
    <published>2022-11-01T13:00:57.000Z</published>
    <updated>2022-11-01T13:01:44.179Z</updated>
    
    <content type="html"><![CDATA[<p>最近需要在 proxmox 上面裝 openvpn 的 client 當作本地所有裝置的跳板 (gateway) 來連到受管理網域，因此這邊紀錄一下怎麼在 proxmox 上面開 LXC 和設定 openvpn client。</p><span id="more"></span><h2 id="Promox-LXC-設置"><a href="#Promox-LXC-設置" class="headerlink" title="Promox LXC 設置"></a>Promox LXC 設置</h2><p>首先從 proxmox 的 WebGUI 建立一個 VM 後，要修改 <code>/etc/pve/lxc/$&#123;LXC_ID&#125;.conf</code>，直接加入下面幾行。讓 LXC 可以存取 openvpn 需要使用的 tun driver。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">lxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file<br>lxc.mount.entry: /dev/net dev/net none bind,create=dir<br>lxc.cgroup.devices.allow: c 10:200 rwm<br>lxc.apparmor.profile: generated<br>lxc.apparmor.allow_nesting: 1<br></code></pre></td></tr></table></figure><p>接著透過 web shell 進入到 LXC 內進行操作。</p><h2 id="Openvpn-client-設置"><a href="#Openvpn-client-設置" class="headerlink" title="Openvpn client 設置"></a>Openvpn client 設置</h2><ul><li><p>安裝 Openvpn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">apk add oepnvpn<br></code></pre></td></tr></table></figure></li><li><p>將從 server 端拿到的 ovpn client file (.ovpn) 修改為 <code>/etc/openvpn/openvpn.conf</code></p></li><li><p>如果使用帳號密碼進行登入，為了要讓 ovpn 可以開機自動工作，我們要修改 openvpn.conf，加入或修改為 <code>auth-user-pass login.conf</code>。然後增加 &#x2F;etc&#x2F;openvpn&#x2F;login.conf，第一行打帳號，第二行打密碼。</p></li><li><p>可以透過 <code>/etc/init.d/openvpn start</code> 指令啟動 openvpn，可能會出現 <code>WARNING: openvpn has started, but is inactive</code> 但是不影響。</p><ul><li>可以透過 <code>ip link</code> 檢查是不是有 <code>tun0</code> 這張介面出現</li></ul></li><li><p>透過 <code>rc-update add openvpn</code> 讓 openvpn 開機自啟動</p></li></ul><h2 id="防火牆、gateway-設置"><a href="#防火牆、gateway-設置" class="headerlink" title="防火牆、gateway 設置"></a>防火牆、gateway 設置</h2><p>為了要讓 LXC 當作 VPN gateway，我們要修改防火牆設置。</p><ul><li><p>開啟 ipv4 轉發</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.conf<br>sysctl -p /etc/sysctl.conf<br></code></pre></td></tr></table></figure></li><li><p>添加防火牆規則</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">apk add iptables<br>rc-update add iptables<br><br>iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE<br>/etc/init.d/iptables save<br></code></pre></td></tr></table></figure><p>這邊是讓本地的封包經過 VPN 時，加上一層 NAT，不然 vpn server 那邊不會認得本地的 IP。</p></li></ul><h2 id="路由設定"><a href="#路由設定" class="headerlink" title="路由設定"></a>路由設定</h2><p>這邊主要是要修改 openvpn.conf，讓 openvpn 只轉發特定的流量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">route-nopull<br>route 10.0.20.0 255.255.255.0 vpn_gateway<br></code></pre></td></tr></table></figure><ul><li>第一行 <code>route-nopull</code> 讓 openvpn 不會去跟 vpn server 要求路由表，而是只轉發我們希望他轉發的流量。</li><li>第二行設定將 10.0.20.0&#x2F;24 這個網段往 vpn server 送。</li><li>重啟 openvpn &#96;&#x2F;etc&#x2F;init.d&#x2F;openvpn restart<br>這樣就能限制只有連線到 10.0.20.0&#x2F;24 這個網段的時候，經過 VPN</li></ul><p>不過這樣只完成 LXC 的路由設定，接著要在需要通過 VPN 的電腦或直接在路由器上將 10.0.20.0&#x2F;24 這個網段送到 VPN gateway 的 IP</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ip route add 10.0.20.0/24 via &lt;VPN gateway LXC&#x27;s ip&gt;<br></code></pre></td></tr></table></figure><p>如果是在路由器上設置好，本地所有的裝置都可以直接透過 VPN 訪問 10.0.20.0&#x2F;24 這個網段而不用在每台機器上設置防火牆了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近需要在 proxmox 上面裝 openvpn 的 client 當作本地所有裝置的跳板 (gateway) 來連到受管理網域，因此這邊紀錄一下怎麼在 proxmox 上面開 LXC 和設定 openvpn client。&lt;/p&gt;</summary>
    
    
    
    <category term="notes" scheme="https://blog.louisif.me/categories/notes/"/>
    
    
    <category term="proxmox" scheme="https://blog.louisif.me/tags/proxmox/"/>
    
    <category term="openvpn" scheme="https://blog.louisif.me/tags/openvpn/"/>
    
  </entry>
  
  <entry>
    <title>Openstack Deployment Serial 2 - 使用 Openstack Ansible 部署 Openstack</title>
    <link href="https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/"/>
    <id>https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/</id>
    <published>2022-11-01T13:00:00.000Z</published>
    <updated>2022-11-01T13:11:32.476Z</updated>
    
    <content type="html"><![CDATA[<p>在前一篇文章中，我們介紹了 Openstack 的幾種 <a href="https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/">網路架構</a>，今天我們要介紹如何使用 Openstack 官方提供的 Openstack-Ansible (OSA) 來完成 Opentack 的部署。</p><blockquote><p>本篇文章以 Openstack Victoria 版本為例，安裝在作業系統為 ubuntu 的設備上，其他版本可能會有所不同。<br>另外本篇文章主要專注在使用 nova 建立虛擬機服務，因此會部分省略掉 cinder 等 opentstack 儲存功能的部分。</p></blockquote><span id="more"></span><h2 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h2><p>Openstack 是由 keystone、glance 等多個組件組成，組件下又需要 dabase base 和 message queue 等服務，再加上需要在每個運算節點上部屬的 nova、neutron，如果要手動部署，會需要花費大量的時間、容易出錯，也不利於擴展。因此 Openstack 官方提供了 Openstack-Ansible (OSA)，使用 Ansible 來自動化部署及擴展 Openstack cluster。</p><p><img src="/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/Pastedimage20221029232804.png" alt="OSA 架構"><br>首先必須要知道透過 OSA 部屬的 openstack clauster 在架構上會與手動安裝文件教的方式有些差異。</p><ul><li>所有 Controller node 上的 openstack 組件被部屬在獨立的 linux contaienr (LXC) 內，LXC 透過 linux bridge 直接暴露在外部網路中，與節點們處在同一個子網段內，可以直接互相訪問</li><li>controller node 上的組建之間不直接互相訪問，而是透過一個 <code>haproxy</code> 做作為進入點，提供附載均衡和高可用。haproxy 會被直接部屬在控制節點上，而不會被部屬在 LXC 內。</li></ul><p>另外在在 OSA 的文件內，用來執行 ansible playbook 的節點稱之為 deployment host，而被部屬 openstack 的 controller node 和 compute node 被統稱為 taget host。不過在實際操作的時候 deployment host 不一定需要是一台獨立的設備，可以直接挑一台 target host 來跑。</p><h2 id="安裝流程"><a href="#安裝流程" class="headerlink" title="安裝流程"></a>安裝流程</h2><h3 id="前置作業"><a href="#前置作業" class="headerlink" title="前置作業"></a>前置作業</h3><p>首先我們要在 deployment host 上安裝 python 還有 OSA。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">system</span><br>apt update<br>apt dist-upgrade<br>apt install build-essential git chrony \<br>  openssh-server python3-dev sudo<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">OSA</span><br>git clone -b victoria-em https://opendev.org/openstack/openstack-ansible/opt/openstack-ansible<br>cd /opt/openstack-ansible<br>scripts/bootstrap-ansible.sh<br></code></pre></td></tr></table></figure><p>接著在 target hosts 上我們也需要先安裝一些必備的套件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt update<br>apt dist-upgrade<br>apt install bridge-utils debootstrap openssh-server \<br>  tcpdump vlan python3<br>apt install linux-modules-extra-$(uname -r)<br></code></pre></td></tr></table></figure><p>Ansible 是透過 ssh 連線到每一台主機上面完成安裝，且 OSA 要求在 deployment 和 target hosts 上都使用 root。<br>OSA 要求的作法是在 deployment 的 root 上帳號下，使用 <code>ssh-keygen</code> 生成 ssh key 並將 public key 放到每個 target host 的 &#x2F;root&#x2F;.ssh&#x2F;authorized_keys 中。<br>另外 OSA 還會將 deployment host 上的 <code>/root/.ssh/id_rsa.pub</code> 檔案複製到每一個 LXC 的 authorized_keys，方便後續訪問。這個可以透過 <code>lxc_container_ssh_key</code> 選項修改，如何設定 OSA 會在後面提到。</p><p>最後是網路相關的設定，在前一篇文章中我們提到 openstack 的網路模式有 <code>flat</code>、<code>vlan</code>、<code>vxlan</code> 三種再加上 OSA 的 LXC 也需要特別的網路，因此這個部分會變得有點複雜，openstack 的虛擬機網路設定會在後面設定 OSA 的章節再來說明。</p><p>在網路的部分，OSA 要求網路基礎架構是預先提供好的，在所有節點上，我們需要有一個 br-mgmt 的 linux bridge，OSA 會為每一個 LXC 建立 veth，連結 br-mgmt 和 LXC 的 eth0 介面。前面有提到 LXC 和節點是在一個 <code>flat</code> 的網路架構，因此 br-mgmt 也需要連接到主機的實體網卡，並給予一個 IP。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-comment"># /etc/netplan/99-openstack.yaml</span><br><span class="hljs-attr">network:</span><br>    <span class="hljs-attr">version:</span> <span class="hljs-number">2</span><br>    <span class="hljs-attr">renderer:</span> <span class="hljs-string">networkd</span><br>    <span class="hljs-attr">bridges:</span><br>        <span class="hljs-attr">br-mgmt:</span><br>            <span class="hljs-attr">addresses:</span><br>                <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.1</span><span class="hljs-string">/24</span><br>            <span class="hljs-attr">interfaces:</span> [ <span class="hljs-string">eth1</span> ]<br></code></pre></td></tr></table></figure><p>在 ubuntu 上我們可以透過 netplan 來設置。</p><p>如果是需要在單節點下啟用 flat 網路做簡單的測試，可以使用下面 netplan 設定</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-meta">---</span><br><span class="hljs-attr">network:</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span><br>  <span class="hljs-attr">renderer:</span> <span class="hljs-string">networkd</span><br>  <span class="hljs-attr">ethernets:</span><br>    <span class="hljs-attr">eth1:</span> &#123;&#125;<br>  <span class="hljs-attr">bridges:</span><br>    <span class="hljs-attr">br-mgmt:</span><br>      <span class="hljs-attr">addresses:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.101</span><span class="hljs-string">/24</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.1</span><span class="hljs-string">/24</span><br>      <span class="hljs-attr">interfaces:</span> [ <span class="hljs-string">eth1</span> ]<br>  <span class="hljs-attr">vlans:</span><br>    <span class="hljs-attr">eth1.10:</span><br>      <span class="hljs-attr">id:</span> <span class="hljs-number">10</span><br>      <span class="hljs-attr">link:</span> <span class="hljs-string">eth1</span><br></code></pre></td></tr></table></figure><ul><li>br-mgmt 多一個 IP 用於 external_lb_vip_address (後面會再提到)</li><li>flat 網路設定時，綁定到 eth1.10 這張網卡</li></ul><blockquote><p>如果要使用 block storage 的服務的話還需要配置 LVM，這邊我們就省略掉</p></blockquote><h3 id="OSA-設定檔設置"><a href="#OSA-設定檔設置" class="headerlink" title="OSA 設定檔設置"></a>OSA 設定檔設置</h3><p>接著我們要調整 OSA 的設定檔。<br>首先我們要把基礎設定複製到設定檔目錄</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cp -r /opt/openstack-ansible/etc/openstack_deploy/etc/openstack_deploy<br></code></pre></td></tr></table></figure><p>這邊我們主要要修改兩個檔案 <code>openstack_user_config.yml</code> 還有 <code>user_variables.yml</code>。</p><ul><li><code>openstack_user_config</code> 是各 node 功能和 IP 等基本資訊的設置，OSA 會讀取 openstack_user_config 來生成 ansible 的 inventory file。</li><li><code>user_variables.yml</code> 則是 OSA 的主要設定檔。</li></ul><blockquote><p>在 &#x2F;etc&#x2F;openstack_deploy 內有各種後綴為 example 的範例文件可以參考</p></blockquote><h3 id="openstack-user-config"><a href="#openstack-user-config" class="headerlink" title="openstack_user_config"></a>openstack_user_config</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-meta">---</span><br><span class="hljs-comment"># openstack_user_config.yml</span><br><span class="hljs-attr">cidr_networks:</span><br>  <span class="hljs-attr">container:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span><br>  <span class="hljs-attr">tunnel:</span> <span class="hljs-number">172.29</span><span class="hljs-number">.240</span><span class="hljs-number">.0</span><span class="hljs-string">/22</span><br>  <span class="hljs-attr">storage:</span> <span class="hljs-number">172.29</span><span class="hljs-number">.244</span><span class="hljs-number">.0</span><span class="hljs-string">/22</span><br><br><span class="hljs-attr">used_ips:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;192.168.56.1,192.168.56.50&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.29.236.1,172.29.236.50&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.29.240.1,172.29.240.50&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.29.244.1,172.29.244.50&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.29.248.1,172.29.248.50&quot;</span><br><br><span class="hljs-attr">global_overrides:</span><br>  <span class="hljs-comment"># The internal and external VIP should be different IPs, however they</span><br>  <span class="hljs-comment"># do not need to be on separate networks.</span><br>  <span class="hljs-attr">external_lb_vip_address:</span> <span class="hljs-number">172.29</span><span class="hljs-number">.236</span><span class="hljs-number">.10</span><br>  <span class="hljs-attr">internal_lb_vip_address:</span> <span class="hljs-number">172.29</span><span class="hljs-number">.236</span><span class="hljs-number">.11</span><br>  <span class="hljs-attr">management_bridge:</span> <span class="hljs-string">&quot;br-mgmt&quot;</span><br>  <span class="hljs-attr">provider_networks:</span><br>   <span class="hljs-string">...</span><br></code></pre></td></tr></table></figure><p>首先我們來設定 <code>/etc/openstack_deploy/openstack_user_config.yml</code>，可以複製 <code>/etc/openstack_deploy/openstack_user_config.yml.example</code> 來做修改。<br><code>cidr_networks</code> 定義了各種網路架構的 IP 區段，最主要的是 containers，是前面提到節點本身還有 controller service LXC 們要分配的 IP 段，這邊指定為 <code>192.168.56.0/24</code>。另外兩個 tunnel 和 storage 分別是給 vxlan 虛擬機網路架構和 block storage 用的，在 OSA 的架構內，control plane、storage 和虛擬機網路希望是分開的獨立網路，因此如果有使用 block storage 或 vxlan 的話這邊要設置對應的 IP 段。</p><p>接著 <code>used_ips</code> 是不允許被分配給 LXC 容器的 IP，<code>&quot;172.29.236.1,172.29.236.50&quot;</code> 表示.1 到.50 這 50 個 IP 都不允許被使用，當 OSA 生成 inventory file 時會在 container IP 段內隨機為每一個 LXC 分配 IP，因此需要透過此設定規避掉 gateway router 還有其他非 Openstack 使用的 IP。在單 controller node 的部屬環境下 LXC 會用掉 13 個 IP。</p><p>接著 <code>external_lb_vip_address</code> 和 <code>internal_lb_vip_address</code> 是 load balancer 的地址，這邊可以直接指定一個 controller node 的 IP，直接使用 controller node 上的 haproxy，或著在多 controller node 的情況下，使用一個獨立的 load balancer (不在 OSA 自動部屬的範圍)，提供控制平面的高可用 (HA)。</p><p>這邊 external 和 internal 的差別對應到 keystone 設定時需要為 endpoint 設置 <code>public</code>、<code>private</code> 和 <code>admin</code> 三種的 url，public 就會使用 external 的 address，後倆著則會使用 internal 的 address。</p><p>另外 address 也可以設置為 domain name，如 <code>openstack.example.com</code>，不過要特別注意的是 domain 要能夠被 dns 解析，而且要在 user_variables.yml 設置 <code>haproxy_keepalived_external_vip_cidr:&quot;&lt;external_vip_address&gt;/&lt;netmask&gt;&quot;</code>。根據文件，不建議將 external address 和 internal address 設置相同，特別是當兩者分別使用 http 和 https 時，一定要把兩個 address 分開。</p><p>接著要設置的是 provider_networks，這邊包含 LXC 網路和虛擬機網路，provider_networks 是一個 list，列出了所有主機上可用的網路。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">provider_networks:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">network:</span><br><span class="hljs-attr">container_bridge:</span> <span class="hljs-string">&quot;br-mgmt&quot;</span><br><span class="hljs-attr">container_type:</span> <span class="hljs-string">&quot;veth&quot;</span><br><span class="hljs-attr">container_interface:</span> <span class="hljs-string">&quot;eth1&quot;</span><br><span class="hljs-attr">ip_from_q:</span> <span class="hljs-string">&quot;container&quot;</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;raw&quot;</span><br><span class="hljs-attr">group_binds:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">all_containers</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">hosts</span><br><span class="hljs-attr">is_container_address:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>首先我們要提供前面提到給 LXC 和節點使用的控制網路，這邊主要要注意的是 <code>container_bridge</code> 要指定成節點上建立好的 bridge 名稱，可以直接沿用慣例的 <code>br-mgmt</code>。</p><p>另外 <code>type</code> 這邊要設置為 <code>raw</code>，type 總共有四種可能 <code>raw</code>, <code>flat</code>, <code>vlan</code>, <code>vxlan</code>，<code>raw</code> 是提供給 mgmt 網路和 storage 網路使用的，後三著則對應到虛擬機網路的三種可能架構。</p><p>接著就來到了 OSA 文件裡面最有問題的地方了，就是關於虛擬機網路架構的設定，在幾乎所有的 OSA 設定文件裡面都會要求在 <code>flat</code> 和 <code>vlan</code> type 的網路中，設置 container_bridge 為 <code>br-vlan</code>。然而在少數 <a href="https://docs.openstack.org/openstack-ansible/victoria/user/network-arch/example.html">文件</a> 內，可以找到這樣一段話。</p><blockquote><p> The “br-vlan” bridge is no longer necessary for deployments unless Neutron agents are deployed in a container. Instead, a direct interface such as bond1 can be specified via the “host_bind_override” override when defining provider networks.</p></blockquote><p>原來在現有的版本內，已經不需要設置 <code>br-vlan</code> 這個東西了。參考 OSA 容器網路的架構圖，我們可以得知需要 br-vlan 是因為 compute node 上的 neutron 是跑在一個 LXC 內，因此需要多一個 br-vlan 還有 vath pair 將網卡接到 LXC 內。<br><img src="/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/Pastedimage20221030151923.png" alt="Neutron in LXC"><br>而在現行的架構下 neutron agent 是直接跑在 compute node 的 host 上的。<br><img src="/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/Pastedimage20221030152125.png" alt="Neutron not in LXC"></p><p>然後回顧本系列文章前一篇就會知道，不論是 vlan 還是 flat 架構下，我們都只需要指定一張 interface 作為對外出口，bridge 的部分是歸屬於 neutron agent 管理的。<br><img src="/Openstack/Openstack-Deployment-Serial-2-Deployment-with-Openstack-Ansible/Pastedimage20221030152330.png" alt="Openstack vxlan network"></p><p>這邊是正確設置 flat 網路需要的設定檔。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">provider_networks:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">network:</span><br>      <span class="hljs-attr">host_bind_override:</span> <span class="hljs-string">&quot;eth1.10&quot;</span><br>      <span class="hljs-attr">type:</span> <span class="hljs-string">&quot;flat&quot;</span><br>      <span class="hljs-attr">net_name:</span> <span class="hljs-string">&quot;flat&quot;</span><br>      <span class="hljs-attr">group_binds:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">neutron_linuxbridge_agent</span><br></code></pre></td></tr></table></figure><ul><li>host_bind_override 指定 flat 網路連到節點外的網卡名稱。</li><li>type: flat 網路模式下設置為 <code>flat</code></li><li>net_name: neutron physical_network mapping 的 name</li></ul><p>同理於 vlan 網路設定，和 flat 的差別是要指定可用的 vlan id range (<code>range</code>)</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">provider_networks:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">network:</span><br>  <span class="hljs-attr">host_bind_override:</span> <span class="hljs-string">&quot;eth2&quot;</span><br>      <span class="hljs-attr">type:</span> <span class="hljs-string">&quot;vlan&quot;</span><br>  <span class="hljs-attr">net_name:</span> <span class="hljs-string">&quot;vlan&quot;</span><br>  <span class="hljs-attr">range:</span> <span class="hljs-string">&quot;101:200,301:400&quot;</span><br>  <span class="hljs-attr">group_binds:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">neutron_linuxbridge_agent</span><br></code></pre></td></tr></table></figure><p>最後是 vxlan 的部分，vxlan 的設定與 <code>vlan</code> 和 <code>flat</code> 有比較大的差異，在前一篇文章我們提到，vxlan 網路架構要指定一個 local ip 作為 vxlan 封包對外的 IP，並自動綁定到該 IP 對應的網路介面。</p><p>在 OSA 裡面 vxlan 則是需要設定 <code>container_bridge</code>，指定一個 bridge，OSA 會將該 bridge 的 IP 作為 local ip。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">provider_networks:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">network:</span><br>        <span class="hljs-attr">container_bridge:</span> <span class="hljs-string">&quot;br-vxlan&quot;</span><br>        <span class="hljs-attr">ip_from_q:</span> <span class="hljs-string">&quot;tunnel&quot;</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">&quot;vxlan&quot;</span><br>        <span class="hljs-attr">range:</span> <span class="hljs-string">&quot;1:1000&quot;</span><br>        <span class="hljs-attr">net_name:</span> <span class="hljs-string">&quot;vxlan&quot;</span><br>        <span class="hljs-attr">group_binds:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-string">neutron_linuxbridge_agent</span><br></code></pre></td></tr></table></figure><ul><li>container_bridge: 指定為 vxlan 對外綁定的 bridge</li><li>ip_from_q: 指定為 vxlan 封包傳輸的子網域，雖然 OSA 會從 bridge 提取 local IP，但是還是從該欄位取得 local address 的子網域遮罩</li><li>range: 指定可用的 vxlan id (vni)</li></ul><p>到這邊就完成 <code>global_overrides</code> 部分的設定。接下來是 <code>openstack_user_config</code> 對每個不同 service 還有節點的設定。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-comment">###</span><br><span class="hljs-comment">### Infrastructure</span><br><span class="hljs-comment">###</span><br><br><span class="hljs-attr">shared-infra_hosts:</span><br>  <span class="hljs-attr">infra1:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.101</span><br><span class="hljs-string">...</span><br><br><span class="hljs-comment">### OpenStack</span><br><span class="hljs-comment">###</span><br><br><span class="hljs-comment"># keystone</span><br><span class="hljs-attr">identity_hosts:</span><br>  <span class="hljs-attr">infra1:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.101</span><br>  <span class="hljs-attr">infra2:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.102</span><br><br><span class="hljs-comment"># glance</span><br><span class="hljs-attr">image_hosts:</span><br>  <span class="hljs-attr">infra1:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.101</span><br><span class="hljs-string">...</span><br><br><span class="hljs-comment"># nova hypervisors</span><br><span class="hljs-attr">compute_hosts:</span><br>  <span class="hljs-attr">compute1:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.103</span><br>    <span class="hljs-attr">host_vars:</span><br>      <span class="hljs-string">...</span><br>  <span class="hljs-attr">compute2:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.104</span><br><br><span class="hljs-attr">storage_hosts:</span><br>  <span class="hljs-attr">infra1:</span><br>    <span class="hljs-attr">ip:</span> <span class="hljs-number">172.29</span><span class="hljs-number">.236</span><span class="hljs-number">.11</span><br>    <span class="hljs-attr">container_vars:</span><br>      <span class="hljs-string">...</span><br><br></code></pre></td></tr></table></figure><p>這個部分的格式是</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">SERVICE_NAME:</span><br>   <span class="hljs-attr">HOST1_NAME:</span><br>     <span class="hljs-attr">ip:</span> <span class="hljs-string">HOST1_MGMT_IP</span><br>   <span class="hljs-attr">HOST2_NAME:</span><br>     <span class="hljs-attr">ip:</span> <span class="hljs-string">HOST1_MGMT_IP</span><br>   <span class="hljs-string">...</span><br></code></pre></td></tr></table></figure><p>對於 openstack 的不同組件，我們可以自由控制要在那些節點上部屬，並在多個節點上部屬單個服務來達到高可用。如果希望 OSA 不要部屬，則可以直接省略對應的 service，OSA 在部屬的時候就會跳過該服務。</p><p>特別要注意的是 <code>compute_hosts</code> 這個 service，<code>compute_hosts</code> 定義了要在哪些節點上部屬 nova_compute 和 neutron agent，也就是作為 openstack 的運算節點。</p><p>另外還可以透過 <code>host_vars</code> 和 <code>container_vars</code> 覆蓋 OSA 的預設值，來對每個節點做單獨調整。</p><p>完整設定範例可以參考 OSA 的 <a href="https://docs.openstack.org/openstack-ansible/victoria/user/prod/example.html">部屬範例</a>，及 <code>openstack_user_config</code> 的 reference <a href="https://docs.openstack.org/openstack-ansible/victoria/reference/inventory/openstack-user-config-reference.html">文件</a>。</p><h3 id="user-variables"><a href="#user-variables" class="headerlink" title="user_variables"></a>user_variables</h3><p>另外一個要設定的設定檔是 <code>/etc/openstack_deploy/user_variables.yml</code>。</p><blockquote><p>透過 openstack-ansible 指令執行 ansible 時，會將 &#x2F;etc&#x2F;openstack_deploy 目錄下的 user_*.yml 檔案會作為 ansible 的 varible file，在執行 playbook 的時候被自動加入。</p></blockquote><p>在 user_variables.yml 中，可以對 OSA 所有組件的部屬和設定進行調整。參照 OSA 的 [官方文件](<a href="https://docs.openstack.org/project-deploy-guide/openstack-ansible/victoria/configure.html#advanced-service-configuration">https://docs.openstack.org/project-deploy-guide/openstack-ansible/victoria/configure.html#advanced-service-configuration</a> “Permalink to this headline)，這邊每一個 role 對應到 OSA 部屬的每個 service，裡面有列出每個 service 每個設定的預設值。<br>通常設定會使用 service name 當作 prefix，例如 <code>glance_etc_dir</code> 可以修改 glance 的設定檔位置，預設是 “&#x2F;etc&#x2F;glance”，可能可以修改為 “&#x2F;usr&#x2F;local&#x2F;etc&#x2F;glance”。另外一份是進階設定的 <a href="https://docs.openstack.org/openstack-ansible/victoria/reference/configuration/using-overrides.html#top">reference</a>，有比較詳細對 user_varialbes 設定的說明。</p><p>這邊提幾個可能比較需要注意到的設定項目</p><ul><li>install_method: 選項有 source 和 distro，這個設定項會控制 openstack 組件的來源<ul><li>預設是 source，表示 OSA 會直接從 openstack git 拉取原始碼在本地進行安裝，根據官方文件優點是這樣的部屬比較有彈性還有可客製化的程度會比較高，甚至可以將 repo 切成自己的 repostory 來直接修改原始碼，不過缺點是安裝時間比較久</li><li>distro 則是比較像 openstack 手動安裝教學文件的方式，從 linux 發行版的 repository 直接安裝，優點是可能會有針對發行版的優化與修復還有安裝速度，但是缺點就是更新不會像官方這麼及時，而且會缺乏一些可設定的選項</li></ul></li><li>keystone_service_publicuri_proto 和 haproxy_ssl<ul><li>在 openstack 上每個 service 的 api endpoint 分為 public, private, admin 三種，通常來說 public 因為是要公開給外部使用的加上 ssl (可以參考這份 <a href="https://docs.openstack.org/openstack-ansible/latest/user/security/ssl-certificates.html">文件</a>)。</li><li>在 OSA 中，external_lb_vip_address 會用於 public endpoint，並預設會加上自簽 ssl 憑證，然而在 user_variables.yml.example 中 keystone_service_publicuri_proto 設置為 http，因此透過 public endpoint 操作 openstack api 時，會被 keystone 導向到 http，導致 openstack client 沒辦法正確處理 haproxy ssl 加密的數據，因此要注意將 keystone_service_publicuri_proto 設置為 https</li></ul></li></ul><h3 id="user-secrets"><a href="#user-secrets" class="headerlink" title="user_secrets"></a>user_secrets</h3><p>最後需要生成 openstack 組件間溝通使用的密碼，可以簡單透過指令生成，路徑在 &#x2F;etc&#x2F;openstack&#x2F;user_secrets.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /opt/openstack-ansible &amp;&amp; ./scripts/pw-token-gen.py --file /etc/openstack_deploy/user_secrets.yml<br></code></pre></td></tr></table></figure><h3 id="執行-openstack-ansible"><a href="#執行-openstack-ansible" class="headerlink" title="執行 openstack ansible"></a>執行 openstack ansible</h3><p>完成前置作業和 openstack_user_config.yml、user_variables.yml 兩個檔案的設置後，就可以正式來進行部屬了。首先要移動到 <code>/opt/openstack-ansible/playbooks</code>，所有指令在這個目錄內執行。</p><p>接著為了讓 ansible 可以 ssh 連線到 target hosts，我們要使用 ssh agent，ssh agent 會再進行 ssh 連線時，自動嘗試預先列好的 ssh key file，進行登入。透過 ssh-add 將所有可能的 ssh key file 加入，由於前面我們將 id_rsa.pub 加入到 target host 的 authorized_keys 中，因此這邊只需要加這個 key。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">eval $(ssh-agent)<br>ssh-add ~/.ssh/id_rsa<br></code></pre></td></tr></table></figure><p>接著要 export 兩個環境變數，這個是要解決 ansible 本身的 bug，如果再執行 OSA 過程中出現 <code>failed to transfer file to...</code> 錯誤可以嘗試加這兩個環境變數，<a href="https://github.com/ansible/ansible/issues/21562">詳情</a> 可以查看 ansible 的 issue。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">export ANSIBLE_LOCAL_TEMP=$HOME/.ansible/tmp export ANSIBLE_REMOTE_TEMP=/home/vagrant/.ansible/tmp<br></code></pre></td></tr></table></figure><p>接著執行第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">openstack-ansible setup-infrastructure.yml --syntax-check<br></code></pre></td></tr></table></figure><p>OSA 會根據 <code>openstack_user_config.yml</code> 生成 ansible 的 inventory 文件，放置在 <code>/etc/openstack_deploy/openstack_inventory.json</code>，並替每個 LXC 分配 IP。</p><p>接著就是執行正是安裝部屬的腳本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">openstack-ansible setup-hosts.yml<br>openstack-ansible setup-infrastructure.yml<br>openstack-ansible setup-openstack.yml<br></code></pre></td></tr></table></figure><p>這邊分成三個步驟，分別是</p><ul><li>在 target hosts 上安裝 LXC 等必要套件</li><li>安裝 database, msg queue, repo server 等 intra</li><li>安裝設置 openstack 組件<br>理論上三個指令都完整執行成功的話，就代表 openstack 正確建立起來了</li></ul><p>更簡單的指令是執行 <code>setup-everything</code>，他會依據執行上面三個指令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">openstack-ansible setup-everything.yml<br></code></pre></td></tr></table></figure><blockquote><p>另外當執行後，ansible 的 facts 會被保存在 <code>/etc/openstack_deploy/ansible_facts/</code>。</p></blockquote><h2 id="除錯"><a href="#除錯" class="headerlink" title="除錯"></a>除錯</h2><p>在我的安裝經驗中最容易出問題的是 haproxy 的部分。在 OSA 的部屬環境下，所有元件的 log 會被收到 systemd 的 log 內。因為 haproxy 是安裝在 host 上所以可直接在 target host 上，下 <code>journalctl -xe</code> 查看 haproxy 的 log。各個 openstack 組件的 log 則要到各個 LXC 內去查看 systemd log。</p><blockquote><p>在 Rocky 版本後預設使用 systemd log 取代 rsyslog，因此如果在 openstack_user_config 中指定部屬 log_hosts 是沒有意義的，需要額外將 rsyslog_server_enabled 和 rsyslog_client_enabled 給 enable</p></blockquote><p>如果是 LXC 有問題，可以在 target host 的 root 權限下</p><ul><li>使用 <code>lxc-ls -f</code> 查看 LXC 的狀態、IP</li><li>使用 <code>lxc-attach -n &lt;container_name&gt;</code> 進入 LXC 內查看</li></ul><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>到這邊就完成整個 OSA 基本的安裝部屬流程了，下一篇我們會簡單看一下在部屬完成 openstack 後，如何測試和使用 openstack 的基本功能。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在前一篇文章中，我們介紹了 Openstack 的幾種 &lt;a href=&quot;https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/&quot;&gt;網路架構&lt;/a&gt;，今天我們要介紹如何使用 Openstack 官方提供的 Openstack-Ansible (OSA) 來完成 Opentack 的部署。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本篇文章以 Openstack Victoria 版本為例，安裝在作業系統為 ubuntu 的設備上，其他版本可能會有所不同。&lt;br&gt;另外本篇文章主要專注在使用 nova 建立虛擬機服務，因此會部分省略掉 cinder 等 opentstack 儲存功能的部分。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Openstack" scheme="https://blog.louisif.me/categories/Openstack/"/>
    
    
    <category term="Openstack" scheme="https://blog.louisif.me/tags/Openstack/"/>
    
    <category term="Ansible" scheme="https://blog.louisif.me/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 9 - eBPF helper functions</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-9-eBPF-helper-functions/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-9-eBPF-helper-functions/</id>
    <published>2022-10-31T10:14:40.000Z</published>
    <updated>2022-10-31T10:33:52.590Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是本系列文章的最後一篇，在 eBPF 程式裡面要與 kernel 交互很重要的是 helper function，因此在最後的兩天時間，我們要把所有的 helper function 速覽過一遍。這邊介紹以 bpf-helper 的 <a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">man 文件</a> 的內容為主，部分的 helper function 可能因為文件更新而有遺漏。</p><span id="more"></span><p>接下來的介紹會稍微對 helper function 做一定程度的分類，但是具體不同的 eBPF program type 支援那些 helper function 可能還是要根據 <a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md">bcc 文件</a>、每個 helper function 對應的 commit 資訊等查詢。</p><h2 id="eBPF-map-操作類"><a href="#eBPF-map-操作類" class="headerlink" title="eBPF map 操作類"></a>eBPF map 操作類</h2><ul><li><p>array, map 類型 map 的操作函數，對應到查詢、插入或更新、刪除 map 內的元素。其中 update 可以透過 flag (<code>BPF_NOEXIST</code>, <code>BPF_EXIST</code>, <code>BPF_ANY</code>) 決定 key 是不是不能先存在或一定要存在於 map 內。</p><ul><li>bpf_map_lookup_elem</li><li>bpf_map_update_elem</li><li>bpf_map_delete_elem</li></ul></li><li><p>用於 stack, queue 類型 map 的操作函數。</p><ul><li>bpf_map_peek_elem</li><li>bpf_map_pop_elem</li><li>bpf_map_push_elem</li></ul></li><li><p>用於 ringbuff 的操作函數 (改進原本 perf event map 的問題)</p><ul><li>bpf_ringbuf_output</li><li>bpf_ringbuf_reserve</li><li>bpf_ringbuf_submit</li><li>bpf_ringbuf_discard</li><li>bpf_ringbuf_query</li></ul></li></ul><h2 id="通用函數"><a href="#通用函數" class="headerlink" title="通用函數"></a>通用函數</h2><ul><li><p>生成隨機數</p><ul><li>get_prandom_u32</li></ul></li><li><p><code>atol</code>, <code>atoul</code></p><ul><li>bpf_strtol</li><li>bpf_strtoul</li></ul></li><li><p>取得當前執行 eBPF 程式的 (SMP) processor ID。由於 eBPF 是 no preemption 的所以在整個執行過程中 processor id 不會變。</p><ul><li>bpf_get_smp_processor_id</li></ul></li><li><p>取得當前 NUMA (Non-uniform memory access) 的 node id。受於匯流排限制，CPU 核心可以比較快存取同節點上的 memory，透過 node id 區分。通常是當 attach 的 socket 有啟用 <code>SO_ATTACH_REUSEPORT_EBPF</code> 選項時會用到。</p><ul><li>bpf_get_numa_node_id</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_PROG_ARRAY</code> map 去執行 tail call。</p><ul><li>bpf_tail_call</li></ul></li><li><p>取得開機到當下經過的時間，單位是 ns，差別在於後者會多包含 suspend (暫停) 的時間</p><ul><li>bpf_ktime_get_ns</li><li>bpf_ktime_get_boot_ns</li></ul></li><li><p>取得 jiffies64</p><ul><li>bpf_jiffies64</li></ul></li><li><p>將字串訊息發送到 &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;trace ，主要用於開發除錯</p><ul><li>bpf_trace_printk</li></ul></li><li><p>寫入 seq_file</p><ul><li>bpf_seq_write</li><li>bpf_seq_printf</li></ul></li><li><p>搭配 <code>struct bpf_spin_lock</code> 提供一個給 <code>BPF_MAP_TYPE_HASH</code> 和 <code>BPF_MAP_TYPE_ARRAY</code>(目前只支援這兩著) 裡面 value 使用的 lock，由於一個 map 裡面只能有一個 spin_lock，所以通常是使用把之前提過，整個 map 固定只有一個元素，把整個 map 當作一個 global variable 的用法</p><ul><li>bpf_spin_lock</li><li>bpf_spin_unlock</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_PERF_EVENT_ARRAY</code> 使用，傳輸資料到 user space</p><ul><li>bpf_perf_event_output</li></ul></li></ul><h2 id="Tracing-相關-kprobe-tracepoint-perf-event"><a href="#Tracing-相關-kprobe-tracepoint-perf-event" class="headerlink" title="Tracing 相關 (kprobe, tracepoint, perf event)"></a>Tracing 相關 (kprobe, tracepoint, perf event)</h2><ul><li><p>取得當前的 tgid, uid, gid, command name, task structure</p><ul><li>bpf_get_current_pid_tgid</li><li>bpf_get_current_uid_gid</li><li>bpf_get_current_comm</li><li>bpf_get_current_task</li></ul></li><li><p>發 signal 到當前 (process, thread)</p><ul><li>bpf_send_signal</li><li>bpf_send_signal_thread</li></ul></li><li><p>用於讀取記憶體資料、字串及寫入記憶體。帶 user 的版本用於 user space memory，其餘用於 kernel space memory。</p><ul><li>bpf_probe_read (通常使用後倆著)</li><li>bpf_probe_read_user</li><li>bpf_probe_read_kernel</li><li>bpf_probe_read_str (通常使用後倆著)</li><li>bpf_probe_read_user_str</li><li>bpf_probe_read_kernel_str</li><li>bpf_probe_write_user</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_STACK_TRACE</code> 使用，取得一個 stack address hash 過的 stack id</p><ul><li>bpf_get_stackid</li></ul></li><li><p>取得 userspace 或 kernel space 的 stack 資料</p><ul><li>bpf_get_stack</li><li>bpf_get_task_stack</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_PERF_EVENT_ARRAY</code> 取得 perf-event counter 的讀數</p><ul><li>bpf_perf_event_read</li><li>bpf_perf_event_read_value (建議使用)</li></ul></li><li><p>用於 <code>BPF_PROG_TYPE_PERF_EVENT</code> 取得 struct perf_branch_entry</p><ul><li>bpf_read_branch_records</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_CGROUP_ARRAY</code> 使用，檢查是否在某個 cgroup v2 節點內</p><ul><li>bpf_current_task_under_cgroup</li></ul></li><li><p>查看當前上下文的 cgroup 節點的祖先節點 id</p><ul><li>bpf_get_current_ancestor_cgroup_id</li></ul></li><li><p>取得當前上下文對應的 cgroup id</p><ul><li>bpf_get_current_cgroup_id</li></ul></li><li><p>用於 kprobe，修改函數回傳值</p><ul><li>bpf_override_return</li></ul></li></ul><h3 id="Cgroup-相關"><a href="#Cgroup-相關" class="headerlink" title="Cgroup 相關"></a>Cgroup 相關</h3><ul><li><p>取得一個當前 network namespace 對應的 cookie (identifer)</p><ul><li>bpf_get_netns_cookie</li></ul></li><li><p>取得 local storage 的指標 (cgroup 相關可使用的一個儲存區)</p><ul><li>bpf_get_local_storage</li></ul></li><li><p>用於 <code>BPF_PROG_TYPE_CGROUP_SYSCTL</code></p><ul><li>取得、更新 sysctl 資訊<ul><li>bpf_sysctl_get_name</li><li>bpf_sysctl_get_current_value</li><li>bpf_sysctl_get_new_value</li><li>bpf_sysctl_set_new_value</li></ul></li></ul></li></ul><h2 id="其他類別"><a href="#其他類別" class="headerlink" title="其他類別"></a>其他類別</h2><ul><li>LIRC 紅外線收發相關 (BPF_PROG_TYPE_LIRC_MODE2)<ul><li>bpf_rc_repeat </li><li>bpf_rc_keydown</li><li>bpf_rc_pointer_rel</li></ul></li></ul><h2 id="XDP-相關"><a href="#XDP-相關" class="headerlink" title="XDP 相關"></a>XDP 相關</h2><ul><li><p>於 XDP 修改封包大小 (可以增大或縮小)</p><ul><li>bpf_xdp_adjust_head</li><li>bpf_xdp_adjust_tail</li></ul></li><li><p>XDP_TX redirect 使用</p><ul><li>bpf_redirect_map</li></ul></li><li><p>XDP 輸出封包內容到 perf event</p><ul><li>bpf_xdp_output</li></ul></li><li><p>調整 <code>xdp_md-&gt;data_meta</code></p><ul><li>bpf_xdp_adjust_meta</li></ul></li><li><p>查詢 fid (Forward Information Base, L2)</p><ul><li>bpf_fib_lookup (也可用在 TC)</li></ul></li></ul><h2 id="LWT-相關"><a href="#LWT-相關" class="headerlink" title="LWT 相關"></a>LWT 相關</h2><ul><li>attach 在 routing table<ul><li>替 L3 封包進行 tunnel header encap<ul><li>bpf_lwt_push_encap</li></ul></li><li>外封包 (underlay) 內容修改<ul><li>bpf_lwt_seg6_store_bytes</li><li>bpf_lwt_seg6_adjust_srh</li></ul></li><li>套用 IPv6 Segment Routing action 決策<ul><li>bpf_lwt_seg6_action</li></ul></li></ul></li></ul><h2 id="socket-socket-buffer-相關"><a href="#socket-socket-buffer-相關" class="headerlink" title="socket, socket buffer 相關"></a>socket, socket buffer 相關</h2><ul><li><p>用於 <code>BPF_PROG_TYPE_CGROUP_SOCK_ADDR</code>，修改 bind address</p><ul><li>bpf_bind</li></ul></li><li><p>讀取封包內容</p><ul><li>bpf_skb_load_bytes</li><li>bpf_skb_load_bytes_relative</li></ul></li><li><p>修改封包內容，可自動更新 chekcsum</p><ul><li>bpf_skb_store_bytes</li></ul></li><li><p>改寫 l3, l4 的 checksum</p><ul><li>bpf_l3_csum_replace</li><li>bpf_l4_csum_replace</li></ul></li><li><p>用於計算 check sum，可搭配前兩個 replace 函數使用</p><ul><li>bpf_csum_diff</li></ul></li><li><p>取得 xfrm (IPsec 相關)</p><ul><li>bpf_skb_get_xfrm_state</li></ul></li><li><p>將封包發到其他的 device。後者會複製一分封包。</p><ul><li>bpf_redirect</li><li>bpf_clone_redirect</li></ul></li><li><p>取得 classid，參考 cgroup 的 net_cls，使用於 TC egress path。</p><ul><li>bpf_get_cgroup_classid</li></ul></li><li><p>增減 vlan header</p><ul><li>bpf_skb_vlan_push</li><li>bpf_skb_vlan_pop</li></ul></li><li><p>取得、修改封包的 tunnel (ex. GRE) 的 tunnel key 資訊</p><ul><li>bpf_skb_get_tunnel_key</li><li>bpf_skb_set_tunnel_key</li></ul></li><li><p>取得、修改封包的 tunnel 資訊</p><ul><li>bpf_skb_get_tunnel_opt</li><li>bpf_skb_set_tunnel_opt</li></ul></li><li><p>取得 skb 的 tclassid 欄位，用於 clsact TC egress</p><ul><li>bpf_get_route_realm</li></ul></li><li><p>修改封包 prtocol (ipv4, ipv6)</p><ul><li>bpf_skb_change_proto</li></ul></li><li><p>修改封包類型 (broadcast, multicast, unitcast..)</p><ul><li>bpf_skb_change_type</li></ul></li><li><p>搭配 <code>BPF_MAP_TYPE_CGROUP_ARRAY</code> 使用，檢查 skb 是不是在某個 cgroup v2 節點的子節點內。</p><ul><li>bpf_skb_under_cgroup</li></ul></li><li><p>取得 skb 對應的 cgroup id</p><ul><li>bpf_skb_cgroup_id</li></ul></li><li><p>向上查找 skb 對應 cgroup 節點的祖先節點 id</p><ul><li>bpf_sk_ancestor_cgroup_id</li></ul></li><li><p>取得、修改 <code>skb-&gt;hash</code></p><ul><li>bpf_get_hash_recalc</li><li>bpf_set_hash</li></ul></li><li><p>修改封包大小</p><ul><li>bpf_skb_change_tail</li></ul></li><li><p>用於封包 payload 存取，具體內容有點難理解 (non-linear data)</p><ul><li>bpf_skb_pull_data</li></ul></li><li><p>修改 <code>skb-&gt;csum</code></p><ul><li>bpf_csum_update</li></ul></li><li><p>修改 <code>skb-&gt;csum_level</code></p><ul><li>bpf_csum_level</li></ul></li><li><p>標註 <code>skb-&gt;hash</code> 為無效，觸發重算</p><ul><li>bpf_set_hash_invalid</li></ul></li><li><p>將 <code>skb-&gt;sk</code> 轉成所有欄位都可以訪問的版本</p><ul><li>bpf_sk_fullsock</li></ul></li><li><p>從 <code>skb-&gt;sk</code> 取得 <code>struct bpf_tcp_sock</code></p><ul><li>bpf_tcp_sock</li></ul></li><li><p>向 tcp-sock 對應的方向發一個 tcp-ack</p><ul><li>bpf_tcp_send_ack</li></ul></li><li><p>從 <code>skb-&gt;sk</code> 取得 <code>bpf_sock</code> </p><ul><li>bpf_get_listener_sock</li></ul></li><li><p>設置 <code>skb-&gt;sk</code></p><ul><li>bpf_sk_assign</li></ul></li><li><p>取得 bpf_sock 對應的 cgroupv2 id</p><ul><li>bpf_sk_cgroup_id</li></ul></li><li><p>取得 bpf_sock 對應 cgroupv2 節點的祖先 id</p><ul><li>bpf_sk_ancestor_cgroup_id</li></ul></li><li><p>強制轉型 sk 成特定的 XXX_sock 結構</p><ul><li>bpf_skc_to_tcp6_sock</li><li>bpf_skc_to_tcp_sock</li><li>bpf_skc_to_tcp_timewait_sock</li><li>bpf_skc_to_tcp_request_sock</li><li>bpf_skc_to_udp6_sock</li></ul></li><li><p>增加 packet header 區 (headroom) 長度，用於為 L3 封包直接加上 L2 的 header</p><ul><li>bpf_skb_change_head</li></ul></li><li><p>幫 socket 建立一個 cookie，作為 socket 的 identifier，用於追蹤</p><ul><li>bpf_get_socket_cookie (sk_buff)</li><li>bpf_get_socket_cookie (bpf_sock_addr)</li><li>bpf_get_socket_cookie (bpf_sock_ops)</li></ul></li><li><p>取得 socket 的 owner UID</p><ul><li>bpf_get_socket_uid</li></ul></li><li><p>模擬呼叫 getsocketopt、setsockopt</p><ul><li>bpf_getsockopt</li><li>bpf_setsockopt</li></ul></li><li><p>存取 skb 的 ebpf local storage</p><ul><li>bpf_sk_storage_get</li><li>bpf_sk_storage_delete</li></ul></li><li><p>將封包內容輸出到 perf event</p><ul><li>bpf_skb_output</li></ul></li><li><p>修改封包 payload 大小，可以從 L2 或 L3 的角度來看</p><ul><li>bpf_skb_adjust_room</li></ul></li><li><p>產生、尋找封包對應的 <code>SYN cookie ACK</code><br>  + bpf_tcp_gen_syncookie<br>  + bpf_tcp_check_syncookie</p></li><li><p><code>BPF_PROG_TYPE_SK_SKB</code> (ingress 方向)</p><ul><li>搭配 <code>BPF_MAP_TYPE_SOCKMAP</code> 做 socket redierct <ul><li>bpf_sk_redirect_map</li><li>bpf_sk_redirect_hash</li></ul></li><li>搜尋滿足對應 5-tuple 的 socket<ul><li>bpf_sk_lookup_tcp</li><li>bpf_skc_lookup_tcp</li><li>bpf_sk_lookup_udp</li></ul></li><li>釋放上面兩個找到的 socket 的 reference<ul><li>bpf_sk_release</li></ul></li></ul></li><li><p><code>BPF_PROG_TYPE_SK_MSG</code> (egress 方向)</p><ul><li>搭配 <code>BPF_MAP_TYPE_SOCKMAP</code> 做 socket redierct<ul><li>bpf_msg_redirect_map</li><li>bpf_msg_redirect_hash</li></ul></li><li>替特定長度的內容作 verdict (SK_PASS…)，可用於截短封包，優化處理速度 (tc 相關的東西不太熟…&#x2F;)<ul><li>bpf_msg_apply_bytes</li></ul></li><li>跳過接下來某長度的內容不做 veridct<ul><li>bpf_msg_cork_bytes</li></ul></li><li>讀寫修改特定長度的資料<ul><li>bpf_msg_pull_data</li><li>bpf_msg_pop_data</li><li>bpf_msg_push_data</li></ul></li></ul></li><li><p>更新 <code>BPF_MAP_TYPE_SOCKMAP</code> </p><ul><li>bpf_sock_map_update</li></ul></li><li><p>設置 <code>bpf_sock_ops-&gt;bpf_sock_ops_cb_flags</code> 欄位</p><ul><li>bpf_sock_ops_cb_flags_set</li></ul></li><li><p>更新 sockhash</p><ul><li>bpf_sock_hash_update</li></ul></li><li><p>用於 <code>BPF_PROG_TYPE_SK_REUSEPORT</code>(將多個程式綁定在同一個 port 上)</p><ul><li>sk_select_reuseport</li></ul></li><li><p>用於 <code>BPF_PROG_TYPE_CGROUP_SKB</code>，設置 ECN (Explicit Congestion Notification)</p><ul><li>bpf_skb_ecn_set_ce</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇是本系列文章的最後一篇，在 eBPF 程式裡面要與 kernel 交互很重要的是 helper function，因此在最後的兩天時間，我們要把所有的 helper function 速覽過一遍。這邊介紹以 bpf-helper 的 &lt;a href=&quot;https://man7.org/linux/man-pages/man7/bpf-helpers.7.html&quot;&gt;man 文件&lt;/a&gt; 的內容為主，部分的 helper function 可能因為文件更新而有遺漏。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 8 - cgroups &amp; socket map</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-8-cgroups-socket-map/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-8-cgroups-socket-map/</id>
    <published>2022-10-31T09:39:47.000Z</published>
    <updated>2022-10-31T10:33:44.025Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是 BCC 學習歷程的最後一篇，這篇文章會介紹 linux cgroups、eBPF socketmap 的功能，並以 <a href="https://github.com/iovisor/bcc/blob/master/examples/networking/sockmap.py">sockmap.py</a> 作為範例。</p><span id="more"></span><h2 id="cgroups-介紹"><a href="#cgroups-介紹" class="headerlink" title="cgroups 介紹"></a>cgroups 介紹</h2><p>cgroups 是 Linux kernel 內建的一個機制，可以以進程為最小單位，對可使用的 CPU、memory、裝置 I&#x2F;O 等資源進行限制、分割。</p><blockquote><p>cgroups 目前有 v1 和 v2 兩個版本，在分組策略架構上有所差異，這邊介紹只以 v1 為主</p></blockquote><p>在 cgroup 的架構內，我們可以針對不同的資源類型進行獨立管理 (稱為不同的 subsystem 或 controller) ，一些可能的資源類型和一部份的功能簡介如下</p><ul><li>cpu: 對一定時間週期內，可使用的 cpu 時間長度限制</li><li>memory: 限制記憶體使用上限以及超出上限時的行為</li><li>blkio: 控制對硬碟等設備的訪問速度上限</li><li>cpuacct: 用來統計目前的 CPU 使用情況</li><li>devices: 控制可以訪問那些 device</li><li>pids: 限制 cgroup 內可建立的 pid 數量，也就是進程數量</li></ul><p>接著是 <code>hierarchy</code>，cgroup 使用樹狀結構來管理資源，一個 <code>hierarchy</code> 預設會有一個根結點，所有的 process (pid 都會 attach 在這個節點上)。</p><p>一個 <code>hierarchy</code> 可以對應到零個或多個上述的 subsystem，並在一個節點內設置上述的那些限制，那這些限制就會套用到在這個節點內的所有 process。</p><p>可以在 <code>hierarchy</code> 內建立子節點，那子節點就會預設套用父節點的所有設置，然後可以只針對有興趣的項目作更細緻的調正。</p><p>一個 process 在一棵 <code>hierarchy</code> 只能 attach 在一個節點上，可以對 process 設定所在的節點。從 process fork 出來的 process 會在同一個節點上，但是搬運 process 到不同的節點，並不會影響子 process。</p><p>Linux 透過虛擬檔案系統來提供修改調整 cgroups 的 user space 介面。<br>通常來說介面會被掛載在 <code>/sys/fs/cgroup</code> 這個路徑下。</p><p>我們可以透過 mount 來建立 <code>hierarchy</code> 並把他關連到一個或多個 subsystem</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"> 關連到 CPU</span><br>mkdir /sys/fs/cgroup/cpu<br>mount -t cgroup -o cpu none /sys/fs/cgroup/cpu<br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 關連到 CPU 和 CPUACCT</span><br>mkdir /sys/fs/cgroup/cpu,cpuacct<br>mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct<br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 不過 /sys/fs/cgroup 目錄可能會被系統設置為 < span class="hljs-built_in">read</span> only，避免隨意變更，而且通常不需要增減 hierarchy 本身，只是在 hierarchy 內增減節點管理 </span><br></code></pre></td></tr></table></figure><p>查看所有目前的 hierarchy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">ls /sys/fs/cgroup/-l<br>total 0<br>dr-xr-xr-x 4 root root  0  十  11 22:50 blkio<br>lrwxrwxrwx 1 root root 11  十  11 22:50 cpu -&gt; cpu,cpuacct<br>lrwxrwxrwx 1 root root 11  十  11 22:50 cpuacct -&gt; cpu,cpuacct<br>dr-xr-xr-x 4 root root  0  十  11 22:50 cpu,cpuacct<br>dr-xr-xr-x 2 root root  0  十  11 22:50 cpuset<br>dr-xr-xr-x 4 root root  0  十  11 22:50 devices<br>dr-xr-xr-x 2 root root  0  十  11 22:50 freezer<br>dr-xr-xr-x 2 root root  0  十  11 22:50 hugetlb<br>dr-xr-xr-x 4 root root  0  十  11 22:50 memory<br>dr-xr-xr-x 2 root root  0  十  11 22:50 misc<br>lrwxrwxrwx 1 root root 16  十  11 22:50 net_cls -&gt; net_cls,net_prio<br>dr-xr-xr-x 2 root root  0  十  11 22:50 net_cls,net_prio<br>lrwxrwxrwx 1 root root 16  十  11 22:50 net_prio -&gt; net_cls,net_prio<br>dr-xr-xr-x 2 root root  0  十  11 22:50 perf_event<br>dr-xr-xr-x 4 root root  0  十  11 22:50 pids<br>dr-xr-xr-x 2 root root  0  十  11 22:50 rdma<br>dr-xr-xr-x 5 root root  0  十  11 22:50 systemd<br>dr-xr-xr-x 5 root root  0  十  11 22:50 unified<br><br></code></pre></td></tr></table></figure><p>接著查看 cpu 的根結點</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ls /sys/fs/cgroup/cpu/-l<br>total 0<br>-rw-r--r--  1 root root 0  十  11 21:39 cgroup.clone_children<br>-rw-r--r--  1 root root 0  十  11 21:39 cgroup.procs<br>-r--r--r--  1 root root 0  十  11 21:39 cgroup.sane_behavior<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.stat<br>-rw-r--r--  1 root root 0  十  11 21:39 cpuacct.usage<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_all<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_percpu<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_percpu_sys<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_percpu_user<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_sys<br>-r--r--r--  1 root root 0  十  11 21:39 cpuacct.usage_user<br>-rw-r--r--  1 root root 0  十  11 21:39 cpu.cfs_period_us<br>-rw-r--r--  1 root root 0  十  11 21:39 cpu.cfs_quota_us<br>-rw-r--r--  1 root root 0  十  11 21:39 cpu.shares<br>-r--r--r--  1 root root 0  十  11 21:39 cpu.stat<br>drwxr-xr-x  4 root root 0  八  24 14:50 docker<br>-rw-r--r--  1 root root 0  十  11 21:39 notify_on_release<br>-rw-r--r--  1 root root 0  十  11 21:39 release_agent<br>drwxr-xr-x 96 root root 0  十  11 06:05 system.slice<br>-rw-r--r--  1 root root 0  十  11 21:39 tasks<br>drwxr-xr-x  2 root root 0  十  11 21:31 user.slice<br></code></pre></td></tr></table></figure><p>由於前面可以看到 cpu 被 link 到 cpu,cpuacct，所以可以同時查看到 cpu.* 和 cpuacct.* 的選項。</p><p>透過 cpu.cfs_quota_us 和 cpu.cfs_period_us 我們就能控制這個節點上所有 process 在 period 內可使用的 CPU 時間 (quota)。</p><p>透過 <code>cat tasks</code> 我們可以看到所有 attach 在這個節點上的 pid。</p><p>可以看到有三個資料夾 <code>docker</code>, <code>system.slice</code>, <code>user.slice</code>，是三個 hierarchy 上的子節點，我們可以簡單的透過 <code>mkdir</code> 的方式建立子節點。由於這台設備上有跑 docker，所以 docker 會在 &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;docker&#x2F; 目錄下為每個 container 建立獨立的子節點，透過 cgroup 的方式限制容器的資源使用量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker ps --format=&quot;&#123;&#123;.ID&#125;&#125;&quot;<br>90f64cb70ee0<br>177d1a3920ec<br><br>ls /sys/fs/cgroup/cpu/docker -l<br>total 0<br>drwxr-xr-x 2 root root 0  八  24 14:50 177d1a3920ec9....<br>drwxr-xr-x 2 root root 0  八  24 14:50 90f64cb70ee068...<br>-rw-r--r-- 1 root root 0  十  11 21:39 cgroup.clone_children<br>-rw-r--r-- 1 root root 0  十  11 21:39 cgroup.procs<br>...<br></code></pre></td></tr></table></figure><blockquote><p>在許多發行版上使用 systemd 來做為核心系統管理程式，也就會透過 systemd 來管理 cgroup，因此在設置 kubelet 時會建議將 cgroup driver 從 cgroupfs 改成 systemd，統一由 systemd 來管理，避免同時有兩個系統在調整 cgroup</p></blockquote><p>cgroup v2 調整了管理介面的結構，只保留了單一個 hierarchy (&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;unified) 管理所有的 subsystem，因為切出多個 hierarchy 來管理的方式被認為是不必要且增加系統複雜度的。</p><p>到這邊大概介紹完了 cgroup，由於這次 sockmap.py 使用的 program type 的 hook point 會在 cgroup 上，所以趁這個機會詳細了解了一下 cgroup。</p><h2 id="socketmap-介紹"><a href="#socketmap-介紹" class="headerlink" title="socketmap 介紹"></a>socketmap 介紹</h2><p>這邊我們拿 Cilium CNI <a href="https://www.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel">介紹</a> 的一張圖來說明。<br><img src="/eBPF/Learn-eBPF-Serial-8-cgroups-socket-map/cilium_socket_redirect_1.png" alt="無 socket redirect 架構"><br>圖中是一個使用 envoy sidecar 的 kubernetes pod 網路連線示意圖，簡單來說 kubernetes 上面容器 (Pod) 服務 (Service) 的網路流量會透過 iptables 的機制全部重新導向到跑在同一個容器內的 sidecar，透過 sidecar 當作中介完成網路監控、服務發現等功能後才會真正離開容器。進入容器的流量同樣先都重導向到 sidecar 處理。</p><p>這樣的好處是可以完全不對 service 本身修改，完全由獨立的 sidecar 來提供附加的網路功能，但是也有一個很明顯的問題，一個封包在傳輸的過程中，要經過 3 次 Linux kernel 的 network stack 處理，大大降低了封包的傳輸效率。</p><p>其中由於都是在同一台設備的同一個網路空間內傳輸，因此 TPC&#x2F;IP&#x2F;ethernet 等底層網路完全可以省略。</p><p><img src="/eBPF/Learn-eBPF-Serial-8-cgroups-socket-map/cilium_socket_redirect_1.png" alt="socket redirect 架構"><br>因此我們可以透過 eBPF 的 socket redirect 技術來簡化這個封包的傳輸過程，簡單來說，在同一個設備的兩個 socket 間的傳輸，我們完全可以直接跳過底層的網路堆疊，直接在 socket layer 將封包內容從一個 socket 搬到另外一個 socket，跳過底層 TCP&#x2F;IP&#x2F;ethernet 處理。</p><h2 id="sockmap-py-介紹"><a href="#sockmap-py-介紹" class="headerlink" title="sockmap.py 介紹"></a>sockmap.py 介紹</h2><p>bcc 的 <code>sockmap.py</code> 提供的就是 socket redirect 的功能，他會監聽機器上的所有 socket，將 local to local 的 tcp 連線資料封包直接透過 socket redirect 的方式進行搬運。</p><blockquote><p>socket redirect 機制好像同時也節省了 packet 在 userspace 和 kernel space 之間複製搬運的過程，不過這件事情沒有完全確定。</p></blockquote><p>我們一樣先看看執行起來怎麼樣，我們透過 python 建立一個 http server 並透過 curl 來測試</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">python3 -m http.server &amp;<br>curl 127.0.0.1:8000<br></code></pre></td></tr></table></figure><p>接著是 eBPF 程式的執行解果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">python3 sockmap.py -c /sys/fs/cgroup/unified/<br>b&#x27;curl-3043    [000] d...1  7164.673950: bpf_trace_printk: remote-port: 8000, local-port: 46246&#x27;<br>b&#x27;curl-3043    [000] dN..1  7164.673973: bpf_trace_printk: Sockhash op: 4, port 46246 --&gt; 8000&#x27;<br>b&#x27;curl-3043    [000] dNs11  7164.673985: bpf_trace_printk: remote-port: 46246, local-port: 8000&#x27;<br>b&#x27;curl-3043    [000] dNs11  7164.673988: bpf_trace_printk: Sockhash op: 5, port 8000 --&gt; 46246&#x27;<br>b&#x27;curl-3043    [000] d...1  7164.674643: bpf_trace_printk: try redirect port 46246 --&gt; 8000&#x27;<br>b&#x27;python3-3044    [000] d...1  7164.675211: bpf_trace_printk: try redirect port 8000 --&gt; 46246&#x27;<br>b&#x27;python3-3044    [000] d...1  7164.675492: bpf_trace_printk: try redirect port 8000 --&gt; 46246&#x27;<br></code></pre></td></tr></table></figure><blockquote><p>這邊可以看到 sockmap 要指定一個 - c 的參數，後面是指定一個 cgroup，sockmap 只會監控在這個 cgroup 節點上的 socket 連線。這邊 unified 是 cgroup v2 的 hierarchy，在 cgroup v2 只有 unified 一個 hierarchy，所有 subsystem 都在這個 hierarchy 上。</p></blockquote><p>首先是 <code>curl remote-port: 8000, local-port: 46246&#39; Sockhash op: 4, port 46246 --&gt; 8000&#39;</code>，這兩條是 curl 發起連線時，記錄下來的 socket 連線請求。</p><p>接著 <code>curl remote-port: 46246, local-port: 8000&#39; Sockhash op: 5, port 8000 --&gt; 46246&#39;</code>，是 curl 跟 http server 之間連線建立成功後，返回給 curl 的 socket 通知。</p><p>接著可以看到 3 條 <code>try redirect</code> 是 curl 傳遞 http request 和 http server 返回 http response 的 msg，直接透過 socket redirect 的方式在兩個 socket 之間交互。</p><p>這邊我們使用 tcpdump 去監聽 <code>lo</code> interface 的方式來驗證 socket redirect 有真的運作到。同樣是透過 <code>curl 127.0.0.1:8000</code> 發起連線傳輸資料。在沒有啟用 sockmap 的情況下 tcpdump 捕捉到 12 個封包。而開啟 socketmap 後只會捕捉到 6 個封包。</p><p>透過封包內容會發現，在 socketmap 啟動後，只能夠捕捉到帶 <code>SYN</code>、<code>FIN</code> 等 flag 的 TCP 控制封包，不會捕捉到中間純粹的資料交換封包。</p><h2 id="SOCK-OPS-program-type"><a href="#SOCK-OPS-program-type" class="headerlink" title="SOCK_OPS program type"></a>SOCK_OPS program type</h2><p>完成驗證後，我們接著來介紹這次用到的兩種 eBPG program type，分別是 <code>BPF_PROG_TYPE_SOCK_OPS</code> 和 <code>BPF_PROG_TYPE_SK_MSG</code>。</p><p><code>BPF_PROG_TYPE_SOCK_OPS</code> 可以 attach 在一個 cgroup 節點上，當該節點上任意 process 的 socket 發生特定事件時，該 eBPF program 會被觸發。可能的事件定義在 <a href="https://elixir.bootlin.com/linux/v6.0/source/include/uapi/linux/bpf.h">bpf.h</a>。其中 CB 結尾的表示特定事件完成後觸發，例如 <code>BPF_SOCK_OPS_TCP_LISTEN_CB</code> 表示在 socket tcp 連線轉乘 LISTEN 狀態後觸發。有些則是觸發來透過回傳值設置一些控制項，<code>BPF_SOCK_OPS_TIMEOUT_INIT</code> 是在 TCP Timeout 後觸發，透過 eBPF 的 return value 設置 RTO，-1 表示使用系統預設。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">enum</span> &#123;</span><br>BPF_SOCK_OPS_VOID,<br>BPF_SOCK_OPS_TIMEOUT_INIT,<br>BPF_SOCK_OPS_RWND_INIT,<br>BPF_SOCK_OPS_TCP_CONNECT_CB,<br>BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB,<br>BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,<br>BPF_SOCK_OPS_NEEDS_ECN,<br>BPF_SOCK_OPS_BASE_RTT,<br>BPF_SOCK_OPS_RTO_CB,<br>BPF_SOCK_OPS_RETRANS_CB,<br>BPF_SOCK_OPS_STATE_CB,<br>BPF_SOCK_OPS_TCP_LISTEN_CB,<br>BPF_SOCK_OPS_RTT_CB,<br>BPF_SOCK_OPS_PARSE_HDR_OPT_CB,<br>BPF_SOCK_OPS_HDR_OPT_LEN_CB,<br>BPF_SOCK_OPS_WRITE_HDR_OPT_CB,<br>&#125;;<br><br></code></pre></td></tr></table></figure><p>這邊要特別介紹的是 <code>BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB</code> 和 <code>BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB</code> 分別是在主動建立連線時 (發送 SYN，tcp 三手交握第一手)，和被動建立連線時 (發送 SYN+ACK，tcp 三手交握第二手) 觸發。</p><p>觸發後會拿到 bpf_sock_ops 上下文，並根據事件不同，eBPF 回傳值也代表不同的意義。其中 <code>bpf_sock_ops-&gt;op</code> 對應到上述的事件類型。args 則是不同 op 可能帶的一些特殊參數。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_sock_ops</span> &#123;</span><br>__u32 op;<br><span class="hljs-class"><span class="hljs-keyword">union</span> &#123;</span><br>__u32 args [<span class="hljs-number">4</span>];<span class="hljs-comment">/* Optionally passed to bpf program */</span><br>__u32 reply;<span class="hljs-comment">/* Returned by bpf program    */</span><br>__u32 replylong [<span class="hljs-number">4</span>];<span class="hljs-comment">/* Optionally returned by bpf prog  */</span><br>&#125;;<br>__u32 family;<br>__u32 remote_ip4;<span class="hljs-comment">/* Stored in network byte order */</span><br>__u32 local_ip4;<span class="hljs-comment">/* Stored in network byte order */</span><br>__u32 remote_ip6 [<span class="hljs-number">4</span>];<span class="hljs-comment">/* Stored in network byte order */</span><br>__u32 local_ip6 [<span class="hljs-number">4</span>];<span class="hljs-comment">/* Stored in network byte order */</span><br>__u32 remote_port;<span class="hljs-comment">/* Stored in network byte order */</span><br>__u32 local_port;<span class="hljs-comment">/* stored in host byte order */</span><br>__u32 is_fullsock;<br>...<br></code></pre></td></tr></table></figure><h2 id="SK-MSG-SK-SKB-program-type"><a href="#SK-MSG-SK-SKB-program-type" class="headerlink" title="SK_MSG SK_SKB program type"></a>SK_MSG SK_SKB program type</h2><p>接著要介紹另外兩個 program type <code>BPF_PROG_TYPE_SK_SKB</code> 和 <code>BPF_PROG_TYPE_SK_MSG</code>。</p><p>首先他們不 attach linux 本身的某個地方而是 attach 在一個 eBPF map 上，這個 map 必須是 <code>BPF_MAP_TYPE_SOCKMAP</code> 或 <code>BPF_MAP_TYPE_SOCKHASH</code>。兩個 map 都是某個 key 對應到 socket，可以使用 sock_hash_update 更新 sockhash map，將 sock_ops 的上下文 bpf_sock_ops 結構當作 value 去插入。</p><p>當 sockmap 裡面的 socket 有訊息要送出，封包要被放到 socket 的 TXQueue 時會觸發 <code>BPF_PROG_TYPE_SK_MSG</code>，而當封包從外界送入被主機接收，要放到 socket 的 RXQueue 時則會觸發 <code>BPF_PROG_TYPE_SK_SKB</code>。</p><p>以這次會用到的 <code>BPF_PROG_TYPE_SK_MSG</code> 來說，當 userspace 呼叫 sendmsg 時，就會被 eBPF 程式攔截。</p><p>可以透過回傳 <code>__SK_DROP</code>, <code>__SK_PASS</code>, <code>__SK_REDIRECT</code> 來決定是要丟棄、接收或做 socket redirect。</p><p>透過 socket redirect，封包會從發送端 socket 直接被丟到接收端 socket RXQ。</p><blockquote><p>目前 redirect 的功能只能用於 TCP 連線。</p></blockquote><h2 id="sockmap-實作"><a href="#sockmap-實作" class="headerlink" title="sockmap 實作"></a>sockmap 實作</h2><h3 id="eBPF-實作"><a href="#eBPF-實作" class="headerlink" title="eBPF 實作"></a>eBPF 實作</h3><p>大致上的概念介紹完了就讓我們回到 bcc sockmap 的程式碼。</p><p>首先一樣先看 eBPF 的程式碼。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> MAX_SOCK_OPS_MAP_ENTRIES 65535</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sock_key</span> &#123;</span><br>    u32 remote_ip4;<br>    u32 local_ip4;<br>    u32 remote_port;<br>    u32 local_port;<br>    u32 family;<br>&#125;;<br>BPF_SOCKHASH (sock_hash, <span class="hljs-keyword">struct</span> sock_key, MAX_SOCK_OPS_MAP_ENTRIES);<br></code></pre></td></tr></table></figure><p>這邊定義了一個 <code>sock_key</code>，作為 BPF_SOCKHASH socket map 的 key，透過 five tuple (IP src&#x2F;dst, sct&#x2F;dst port 及 TCP&#x2F;UDP) 來定位一個連線。</p><p>接著我們看到第一種 program type <code>SOCK_OPS</code> 的入口函數。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">int bpf_sockhash (struct bpf_sock_ops *skops) &#123;<br>    u32 op = skops-&gt;op;<br>    /* ipv4 only */<br>    if (skops-&gt;family != AF_INET)<br>return 0;<br>    switch (op) &#123;<br>        case BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB:<br>        case BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB:<br>            bpf_sock_ops_ipv4 (skops);<br>            break;<br>        default:<br>            break;<br>    &#125;<br>    return 0;<br>&#125;<br></code></pre></td></tr></table></figure><p>這邊做的事情很簡單，在 socket 建立連線 (ACTIVE_ESTABLISHED_CB) 和接收連線 (PASSIVE_ESTABLISHED_CB) 時，呼叫 bpf_sock_ops_ipv4 將 socket 放到 sock map 內，讓 socket 被第二個 program type <code>SK_MSG</code> 的程式能夠在 socket 呼叫 sendmsg 等 API 時被攔截處理。由於 socker redirect 只能處裡 TCP 連線，所以非 <code>AF_INET</code> 的連線會被過濾掉。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> __always_inline <span class="hljs-type">void</span> <span class="hljs-title function_">bpf_sock_ops_ipv4</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> bpf_sock_ops *skops)</span> &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sock_key</span> <span class="hljs-title">skk</span> =</span> &#123;<br>        .remote_ip4 = skops-&gt;remote_ip4,<br>        .local_ip4  = skops-&gt;local_ip4,<br>        .local_port = skops-&gt;local_port,<br>        .remote_port  = bpf_ntohl (skops-&gt;remote_port),<br>        .family = skops-&gt;family,<br>    &#125;;<br>    <span class="hljs-type">int</span> ret;<br>    bpf_trace_printk (...);<br>    ret = sock_hash.sock_hash_update (skops, &amp;skk, BPF_NOEXIST);<br>    <span class="hljs-keyword">if</span> (ret) &#123;<br>        bpf_trace_printk (<span class="hljs-string">&quot;bpf_sock_hash_update () failed. % d\\n&quot;</span>, -ret);<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    bpf_trace_printk (...);<br>&#125;<br></code></pre></td></tr></table></figure><p>這邊的 bpf_sock_ops_ipv4 其實也很簡單，從 sock_opt 裡面提取出 IP 地址 &#x2F; TCP port 的資訊，填充 sock_key 結構，然後呼叫 sock_hash_update 把 key-value piar 塞進去 sock_hash。後面的 flag 有 <code>BPF_NOEXIST</code>, <code>BPF_EXIST</code>, <code>BPF_ANY</code>。<code>BPF_NOEXIST</code> 表示只有 key 不在 map 裡面的時候可以插入。</p><p>接著是 <code>BPF_PROG_TYPE_SK_MSG</code> 的入口函數。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">bpf_redir</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> sk_msg_md *msg)</span> &#123;<br>    <span class="hljs-keyword">if</span> (msg-&gt;family != AF_INET)<br>        <span class="hljs-keyword">return</span> SK_PASS;<br>    <span class="hljs-keyword">if</span> (msg-&gt;remote_ip4 != msg-&gt;local_ip4)<br>        <span class="hljs-keyword">return</span> SK_PASS;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sock_key</span> <span class="hljs-title">skk</span> =</span> &#123;<br>        .remote_ip4 = msg-&gt;local_ip4,<br>        .local_ip4  = msg-&gt;remote_ip4,<br>        .local_port = bpf_ntohl (msg-&gt;remote_port),<br>        .remote_port = msg-&gt;local_port,<br>        .family = msg-&gt;family,<br>    &#125;;<br>    <span class="hljs-type">int</span> ret = <span class="hljs-number">0</span>;<br>    ret = sock_hash.msg_redirect_hash (msg, &amp;skk, BPF_F_INGRESS);<br>    bpf_trace_printk (...);<br>    <span class="hljs-keyword">if</span> (ret != SK_PASS)<br>        bpf_trace_printk (...);<br>    <span class="hljs-keyword">return</span> ret;<br>&#125;<br></code></pre></td></tr></table></figure><p>首先一樣我們只能處裡 TCP 連線所有把非 <code>AF_INET</code> 的連線透過 <code>return SK_PASS;</code> 交回 linux kernel 處理。</p><p>接著由於 socket redirect 只在本機起作用，所以這邊簡單判斷 src ip 和 dst ip 相不相同，來判斷是否是 local to local 連線。</p><p>接著由於 socket redirect 時要從發送端的 socket redirect 到接收端的 socket，因此我們要從 socket map 中找到接收端的 socket，對發送端和接收端的 socket 來說 src addres 和 dst address 的是顛倒的，所以這邊在生 sock_key 時會把 local 和 remote 顛倒。</p><p>接著這邊的 <code>msg_redirect_hash</code> 是對 <code>bpf_msg_redirect_hash</code> helper function 的包裝，會嘗試從 socket map 找到對應的 socket，然後完成 redirect 的設置，不過成功是回傳是 SK_PASS 而不是 SK_REDIRECT。</p><p>到這邊就完成 eBPF 程式的部分了，接下來 python 的部分就很簡單，只是把 eBPG 程式掛進去。</p><h3 id="python-實作"><a href="#python-實作" class="headerlink" title="python 實作"></a>python 實作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">examples = <span class="hljs-string">&quot;&quot;&quot;examples:</span><br><span class="hljs-string">    ./sockmap.py -c /root/cgroup # attach to /root/cgroup</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>parser = argparse.ArgumentParser (<br>        description=<span class="hljs-string">&quot;pipe data across multiple sockets&quot;</span>,<br>        formatter_class=argparse.RawDescriptionHelpFormatter,<br>        epilog=examples)<br>parser.add_argument (<span class="hljs-string">&quot;-c&quot;</span>, <span class="hljs-string">&quot;--cgroup&quot;</span>, required=<span class="hljs-literal">True</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;Specify the cgroup address. Note. must be cgroup2&quot;</span>)<br>args = parser.parse_args ()<br></code></pre></td></tr></table></figure><p>前面有提到 SOCK_OPS 要掛在一個 cgroup 下面，所以先吃一個 cgroup 路徑參數來。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">bpf = BPF (text=bpf_text)<br>func_sock_ops = bpf.load_func (<span class="hljs-string">&quot;bpf_sockhash&quot;</span>, bpf.SOCK_OPS)<br>func_sock_redir = bpf.load_func (<span class="hljs-string">&quot;bpf_redir&quot;</span>, bpf.SK_MSG)<br></code></pre></td></tr></table></figure><p>編譯 eBPF 程式，取得兩個入口函數</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># raise if error</span><br>fd = os.<span class="hljs-built_in">open</span>(args.cgroup, os.O_RDONLY)<br>map_fd = lib.bpf_table_fd (bpf.module, <span class="hljs-string">b&quot;sock_hash&quot;</span>)<br>bpf.attach_func (func_sock_ops, fd, BPFAttachType.CGROUP_SOCK_OPS)<br>bpf.attach_func (func_sock_redir, map_fd, BPFAttachType.SK_MSG_VERDICT)<br></code></pre></td></tr></table></figure><p>前面提到 cgroup 介面是一個虛擬檔案系統，所以當然要透過 open 去取得對應的 file descriptor。接著就是 attach func_sock_ops 到 SOCK_OPS。<br>由於 func_sock_redir 要 attach 到 sock map，所以先透過 bcc 的 API 取得 sock_hash map 的 file descripter，然後 attach 上去。</p><p>這樣就完成 sockemap 的設置，可以成功提供 socket redirect 的服務了！</p><h2 id="參考文件"><a href="#參考文件" class="headerlink" title="參考文件"></a>參考文件</h2><ul><li><a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups man page</a></li><li>[第一千零一篇的 cgroups 介紹](<a href="https://medium.com/starbugs/%">https://medium.com/starbugs/%</a> E7% AC% AC% E4% B8%80% E5%8D%83% E9%9B% B6% E4% B8%80% E7% AF%87% E7%9A%84-cgroups-% E4% BB%8B% E7% B4% B9-a1c5005be88c)</li><li><a href="https://blog.csdn.net/qq_46595591/article/details/107634756">Cgroups 中的资源管理 hierarchy 层级树</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇是 BCC 學習歷程的最後一篇，這篇文章會介紹 linux cgroups、eBPF socketmap 的功能，並以 &lt;a href=&quot;https://github.com/iovisor/bcc/blob/master/examples/networking/sockmap.py&quot;&gt;sockmap.py&lt;/a&gt; 作為範例。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
    <category term="cgroups" scheme="https://blog.louisif.me/tags/cgroups/"/>
    
    <category term="socketmap" scheme="https://blog.louisif.me/tags/socketmap/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 7 - tc &amp; BCC neighbor_sharing</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-7-tc-BCC-neighbor-sharing/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-7-tc-BCC-neighbor-sharing/</id>
    <published>2022-10-31T09:16:36.000Z</published>
    <updated>2022-10-31T10:33:30.646Z</updated>
    
    <content type="html"><![CDATA[<p>接續前一篇主題 <code>XDP</code>，今天我們要繼續來聊聊 eBPF 在 linux netowrk data path 上的另外一個進入點 <code>tc</code>，並同樣以 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/examples/networking/neighbor_sharing/">neighbor_sharing</a> 作為範例。</p><span id="more"></span><h2 id="Linux-tc-介紹"><a href="#Linux-tc-介紹" class="headerlink" title="Linux tc 介紹"></a>Linux tc 介紹</h2><p>首先我們要先聊聊 <code>tc</code> 是什麼東西。Traffic Control (tc) 是 linux kernel 網路系統裡面和 netfilter&#x2F;iptables 同等重要的一個組件。不過 netfilter 主要著重在 packet mangling (封包修改) 和 filter (過濾)。而 tc 的重點是在調控流量，提供限速、整形等功能。</p><p>tc 的工作時機點分成 <code>ingress tc</code> 和 <code>egress tc</code>，以 <code>ingress tc</code> 來說，他發生在 skb allocation 之後，進入 netfilter 之前。<code>ingress tc</code> 主要用於輸入流量控制，<code>egress tc</code> 則用於流量優先級、QoS 的功能。在傳統使用上，tc 更主要是用在 <code>egress tc</code>，<code>ingress tc</code> 本身有比較大的功能限制。</p><p>在 <code>tc</code> 裡面有三個主要的概念，<code>qdisc</code>、<code>class</code> 和 <code>filter (classifier)</code>。</p><p>tc 的基礎是 queue，封包要進出主機時，會先進入 queue，根據特定的策略重新排序、刪除、延遲後再交給網卡送出，或 netfilter 等系統收入。</p><p><code>qdisc</code> 是套用在這個 queue 上面的策略規則。下列舉例一部份:</p><ul><li>最基本的策略規則是 pfifo，就是一個簡單的 FIFO queue，只能設定 queue 的可儲存的封包大小和封包個數。</li><li>更進階的如 pfifo_fast，會根據 ip 封包內的 <code>ToS</code> 欄位將封包分成三個優先度，每個優先度內是走 FIFO 規則，但是會優先清空高優先度的封包。</li><li><a href="https://man7.org/linux/man-pages/man8/tc-sfq.8.html">sfq</a> 則是會根據 tcp&#x2F;udp&#x2F;ip 欄位 hash 的結果區分出不同的連線，將不同連線的封包放入獨立的 bucket 內，然後 bucket 間使用輪尋的方式，來讓不同連線均等的輸出。</li><li>ingress 是專門用在 ingress tc 的 qdisc<br>上面的 qdisc 都歸為 classless QDisc，因為我們不能透過自定義的方式對流量進行分類，提供不同的策略。</li></ul><p>與 classless 相反的是 classful qdisc，在 classful qdisc 內，我們可以以定義出多個 <code>class</code>，針對不同的 class 設定不同的限速策略等規則。也可以將多個 class 附屬在另外一個 class 下，讓子 class 共用一個父 class 的最大總限速規則，但是子分類又獨立有限速規則等等。</p><p>而要對流量進行分類就會用到 <code>filter</code>, 對於某個 qdisc (classless&#x2F;classful 皆可) 或著父 class 上的封包，如果滿足 filter 的條件，就可以把封包歸到某個 class 上。<br>除了歸類到某個 class 上，filter 也可以設置為執行某個 action，包括丟棄封包、複製封包流量到另外一個網路介面上之類的…</p><p>對於 qdisc 和 class 在建立時需指定或自動分配一個在網卡上唯一的 handle 作為識別 id，格式是 <code>&lt;major&gt;:&lt;minor&gt;</code>(數字)，對於 qdisc 來說只有 major 的部分 <code>&lt;major&gt;:</code>，對 class 來說 major 必須與對應 qdisc 相同。</p><p>另外在 egress pipeline 可以有多個 qdisc，其中一個作為 root，其他的藉由 filter 從 root qdisc dispatch 過去，所以需要有 major 這個欄位。</p><p>在 linux 上面主要透過 <code>tc</code> 這個指令來設置 <code>qdisc</code>、<code>class</code> 和 <code>filter</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"> 添加 eth0 egress 的 root qdisc，類型是 htb，後面是 htb 的參數 </span><br>tc qdisc add dev enp0s3 root handle 1: htb default 30<br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 添加 eth 的 ingress qdisc</span><br>tc qdisc add dev enp0s3 ingress<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 設置一個 class，速度上下限都是 20mbps，附屬於 eth0 的 root qdisc (1:) 下 </span><br>tc class add dev enp0s3 partent 1: classid 1:1 htb rate 20mbit ceil 20mbit<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 當封包為 ip, dst port 80 時分類到上述分類 </span><br>tc filter add dev enp0s3 protocol ip parent 1: prio 1 u32 match ip dport 80 0xffff flowid 1:1<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"> 查看 egress filter</span><br>tc filter show dev eth0<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"> 查看 ingress filter</span><br>tc filter show dev eth0 ingress<br></code></pre></td></tr></table></figure><h2 id="eBPF-與-tc"><a href="#eBPF-與-tc" class="headerlink" title="eBPF 與 tc"></a>eBPF 與 tc</h2><p>eBPF 在 tc 系統裡面是在 <code>filter</code> 的部分作用，並可分成兩種模式，classifier (BPF_PROG_TYPE_SCHED_CLS) 和 action (BPF_PROG_TYPE_SCHED_ACT)。</p><ul><li><p>classifier: 前者分析封包後，決定是否 match，並可以將封包分類給透過 tc 指令預設的 classid 或著重新指定 classid。</p><ul><li>0: mismatch</li><li>-1: match, 使用 filter 預設的 classid</li><li>直接回傳一個 classid</li></ul></li><li><p>action: 作為該 <code>filter</code> 的 action，當 tc 設置的 filter 規則 match 後，呼叫 eBPF 程式決定 action 是 drop (2), 執行預設 action (-1) 等。<br>  下列是 action 的完整定義</p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_UNSPEC(-1)</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_OK0</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_RECLASSIFY1</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_SHOT2</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_PIPE3</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_STOLEN4</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_QUEUED5</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_REPEAT6</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_REDIRECT7</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> TC_ACT_JUMP0x10000000</span><br></code></pre></td></tr></table></figure><h2 id="BCC-neighbor-sharing"><a href="#BCC-neighbor-sharing" class="headerlink" title="BCC neighbor_sharing"></a>BCC neighbor_sharing</h2><h3 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h3><p>這次要看的是 <code>examples/networking/neighbor_sharing</code>。(<a href="https://github.com/iovisor/bcc/blob/master/examples/networking/neighbor_sharing/">原始碼</a>)</p><p>這次的 eBPF 程式會提供 QoS 的服務，對經過某張網卡的針對往特定的 IP 提供不同的限速群組。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">                         /------------\                        |<br>neigh1 --|-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-|            |                        |<br>neigh2 --|-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-|    &lt;-128kb-|        /------\        |<br>neigh3 --|-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-|            |  wan0  | wan  |        |<br>         | ^             |   br100    |-&lt;-&lt;-&lt;--| sim  |        |<br>         | clsfy_neigh () |            |   ^    \------/        |<br>lan1 ----|-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-|    &lt;--1Mb--|   |                    |<br>lan2 ----|-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-|            |   classify_wan ()       |<br>           ^             \------------/                        |<br>           pass ()                                              |<br></code></pre></td></tr></table></figure><p>上圖是 neighbor_sharing 自帶的網路拓譜圖，neight1-3, lan1-2, wan0 是獨立的 network namespace 擁有獨立的 IP，neighbor_sharing 會在 wansim 到 br100 的介面上建立 <code>ingress tc</code>，針對 neigh1-3 的 IP 提供總共 128kb&#x2F;s 的網路速度，對其他 IP 提供總共 1024kb&#x2F;s 的網路速度。</p><p>首先在測試之前要先安裝 pyroute2 和 netperf，前者是 python 接接 tc 指令的 library，後者是用來測試網速的工具。另外要記得設置防火牆規則不然 br100 不會轉發封包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip3 install pyroute2<br>apt install netperf<br>iptables -P FORWARD ACCEPT<br>sysctl -w net.ipv4.ip_forward=1<br></code></pre></td></tr></table></figure><p>neight1-3 會被分配 172.16.1.100-102 的 IP, lan 則是 172.16.1.150-151。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo ip netns exec wan0 netperf -H 172.16.1.100 -l 2 -k<br>MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.100 () port 0 AF_INET : demo<br>Recv   Send    Send<br>Socket Socket  Message  Elapsed<br>Size   Size    Size     Time     Throughput<br>bytes  bytes   bytes    secs.    10^6bits/sec<br><br> 131072  16384  16384    6.00      161.45<br></code></pre></td></tr></table></figure><p>透過 netperf 可以測出來到 neight1 的封包流量被限制在約 161.45 kbits&#x2F;sec。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">ip netns exec wan0 netperf -H 172.16.1.150 -l 2 -f k<br>MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.150 () port 0 AF_INET : demo<br>Recv   Send    Send                          <br>Socket Socket  Message  Elapsed              <br>Size   Size    Size     Time     Throughput  <br>bytes  bytes   bytes    secs.    10^3bits/sec  <br><br>131072  16384  16384    2.67     1065.83 <br></code></pre></td></tr></table></figure><p>而到 lan1 大約是 1065.83kbits&#x2F;sec，接近預先設置的規則。</p><h3 id="python-實作"><a href="#python-實作" class="headerlink" title="python 實作"></a>python 實作</h3><p>這次會先看 python 的程式碼，由於這次的程式碼包含大量用來建立測試環境的部分，所以會跳過只看相關的內容。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c">b = BPF (src_file=<span class="hljs-string">&quot;tc_neighbor_sharing.c&quot;</span>, debug=<span class="hljs-number">0</span>)<br><br>wan_fn = b.load_func (<span class="hljs-string">&quot;classify_wan&quot;</span>, BPF.SCHED_CLS)<br>pass_fn = b.load_func (<span class="hljs-string">&quot;pass&quot;</span>, BPF.SCHED_CLS)<br>neighbor_fn = b.load_func (<span class="hljs-string">&quot;classify_neighbor&quot;</span>, BPF.SCHED_CLS)<br></code></pre></td></tr></table></figure><p>首先這次的 eBPF 程式包含三個部分，因此會分別載入，並且全部都是 classifier (BPF_PROG_TYPE_SCHED_CLS)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">ipr.tc (<span class="hljs-string">&quot;add&quot;</span>, <span class="hljs-string">&quot;ingress&quot;</span>, wan_if [<span class="hljs-string">&quot;index&quot;</span>], <span class="hljs-string">&quot;ffff:&quot;</span>)<br>ipr.tc (<span class="hljs-string">&quot;add-filter&quot;</span>, <span class="hljs-string">&quot;bpf&quot;</span>, wan_if [<span class="hljs-string">&quot;index&quot;</span>], <span class="hljs-string">&quot;:1&quot;</span>, fd=wan_fn.fd,<br>   prio=<span class="hljs-number">1</span>, name=wan_fn.name, parent=<span class="hljs-string">&quot;ffff:&quot;</span>, action=<span class="hljs-string">&quot;drop&quot;</span>,<br>   classid=<span class="hljs-number">1</span>, rate=<span class="hljs-string">&quot;128kbit&quot;</span>, burst=<span class="hljs-number">1024</span> * <span class="hljs-number">32</span>, mtu=<span class="hljs-number">16</span> * <span class="hljs-number">1024</span>)<br>ipr.tc (<span class="hljs-string">&quot;add-filter&quot;</span>, <span class="hljs-string">&quot;bpf&quot;</span>, wan_if [<span class="hljs-string">&quot;index&quot;</span>], <span class="hljs-string">&quot;:2&quot;</span>, fd=pass_fn.fd,<br>   prio=<span class="hljs-number">2</span>, name=pass_fn.name, parent=<span class="hljs-string">&quot;ffff:&quot;</span>, action=<span class="hljs-string">&quot;drop&quot;</span>,<br>   classid=<span class="hljs-number">2</span>, rate=<span class="hljs-string">&quot;1024kbit&quot;</span>, burst=<span class="hljs-number">1024</span> * <span class="hljs-number">32</span>, mtu=<span class="hljs-number">16</span> * <span class="hljs-number">1024</span>)<br></code></pre></td></tr></table></figure><p>接著會建立 wan_if 的 ingress qdisc (wan_if 是 wan0 接到 br100 的介面)，並且會 ingress qdisc 下建立兩條 filter，首先它的 type 指定為 bpf 並透過 <code>fd=wan_fn.fd</code> 選定 eBPF program，所以會交由 eBPF classifier 來決定是不是要 match。</p><blockquote><p>classifier match 後就會執行下屬的 policing action，跟 classid 無關，且在這次的範例中並不存在 class，所以 classid 其實是無意義的，不一定要設置。</p></blockquote><p>後半段 <code>action=&quot;drop&quot;, rate=&quot;128kbit&quot;, burst=1024 * 32, mtu=16 * 1024</code> 定義了一條 policing action，只有當封包滿足 policy 條件時才會觸發具體的 action，這邊指定是流量超出 128kbit 時執行 drop，也就達到了限制 neigh 流量的效果。</p><p>第二條同理，match pass_fn 並且流量到達 1024kbit 時執行 drop，由於 pass_fn 顧名思義是無條件 match 的意思，所以等價於所有非 neigh 的流量共用這一條的 1024kbit 流量限制。</p><p>因此總結來說，eBPF 程式 wan_fn 透過某種方式判斷封包是否是往 neigh 的 ip，是的話就 match 第一條 filter 執行 policing action 來限流，不然就 match 第二條 filter 來做限流。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">ret = self._create_ns (<span class="hljs-string">&quot;neighbor% d&quot;</span> % i, ipaddr=ipaddr,<br>                                  fn=neighbor_fn, cmd=cmd)<br></code></pre></td></tr></table></figure><p>接著就會看到，在建立 neigh1-3 的 namespace 時，attach 了 neighbor_fn 到網卡上，因此就很好理解了 neighbor_fn 監聽了從 neigh 發出的封包，解析拿到 neigh 的 IP 後，透過 map share 給 wan_fn，讓 wan_fn 可以根據 ip 決定要不要 match 第一條 policing action。</p><h3 id="eBPF-實作"><a href="#eBPF-實作" class="headerlink" title="eBPF 實作"></a>eBPF 實作</h3><p>到這裡其實就分析出整個程式的執行邏輯了，我們接續來看看 neighbor_sharing 的 eBPF 程式，這次的 eBPF 程式分成三個部分，首先是接在每個 neigh ingress 方向的 classify_neighbor，接著是接在 wan0 ingress 方向的 classify_wan 和 pass。</p><p>前面說到出來 <code>classify_neighbor</code> 要做的事情就是紀錄 neigh1-3 的 IP，提供給 <code>classify_wan</code> 判斷是否要 match 封包，執行 128kbits 的流量限制。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipkey</span> &#123;</span><br>  u32 client_ip;<br>&#125;;<br><br>BPF_HASH (learned_ips, <span class="hljs-keyword">struct</span> ipkey, <span class="hljs-type">int</span>, <span class="hljs-number">1024</span>);<br></code></pre></td></tr></table></figure><p>首先定義了一個 hash map 用 key 來儲存所有 neigh 的 IP</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">classify_neighbor</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> __sk_buff *skb)</span> &#123;<br>  u8 *cursor = <span class="hljs-number">0</span>;<br>  ethernet: &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethernet_t</span> *<span class="hljs-title">ethernet</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ethernet));<br>    <span class="hljs-keyword">switch</span> (ethernet-&gt;type) &#123;<br>      <span class="hljs-keyword">case</span> ETH_P_IP: <span class="hljs-keyword">goto</span> ip;<br>      <span class="hljs-keyword">default</span>: <span class="hljs-keyword">goto</span> EOP;<br>    &#125;<br>  &#125;<br>  ip: &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ip_t</span> *<span class="hljs-title">ip</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ip));<br>    u32 sip = ip-&gt;src;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipkey</span> <span class="hljs-title">key</span> =</span> &#123;.client_ip=sip&#125;;<br>    <span class="hljs-type">int</span> val = <span class="hljs-number">1</span>;<br>    learned_ips.insert (&amp;key, &amp;val);<br>    <span class="hljs-keyword">goto</span> EOP;<br>  &#125;<br>EOP:<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>接著 <code>classify_neighbor</code> 就會用 cursor 解析出 source ip，將其作為 hash map 的 key 放到 learned_ips 裡面，value 則都設為 1。不論如何都會 return 1 放行封包。雖然其實這是 neighbor ingress 方向上唯一的一條 filter，所以不論回傳值為多少其實都可以，不影響執行結果。</p><blockquote><p>這邊就要提到第一次學習 tc 還有 classifier 時會感到很困惑的地方了，首先 classifier 的回傳值 0 表示 mismatch, 1 表示 match 並轉移到預設的 class，其餘回傳值表示直接指定 classid 為回傳的數值。接著不論 classid 是多少，都會執行 filter 上面綁定的 action。在這次的範例中，所有的 filter 其實都不存在任何的 class，因此 return 值唯一的意義是控制是否要執行 action。這邊 classify_neighbor 綁定的 action 是 ok，表示放行封包的意思</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">classify_wan</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> __sk_buff *skb)</span> &#123;<br>  u8 *cursor = <span class="hljs-number">0</span>;<br>  ethernet: &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethernet_t</span> *<span class="hljs-title">ethernet</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ethernet));<br>    <span class="hljs-keyword">switch</span> (ethernet-&gt;type) &#123;<br>      <span class="hljs-keyword">case</span> ETH_P_IP: <span class="hljs-keyword">goto</span> ip;<br>      <span class="hljs-keyword">default</span>: <span class="hljs-keyword">goto</span> EOP;<br>    &#125;<br>  &#125;<br>  ip: &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ip_t</span> *<span class="hljs-title">ip</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ip));<br>    u32 dip = ip-&gt;dst;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipkey</span> <span class="hljs-title">key</span> =</span> &#123;.client_ip=dip&#125;;<br>    <span class="hljs-type">int</span> *val = learned_ips.lookup (&amp;key);<br>    <span class="hljs-keyword">if</span> (val)<br>      <span class="hljs-keyword">return</span> *val;<br>    <span class="hljs-keyword">goto</span> EOP;<br>  &#125;<br>EOP:<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>接著看到 <code>classify_wan</code>，他會提取封包的 dst ip address，並嘗試搜尋 learned_ips，如果找的到就表示這個是 neighbor 的 ip，回傳 map 對應的 value，前面提到所有的 value 都會設置為 1，因此表示 match 的意思，不然就跳轉到 EOP 回傳 0，表示 mismatch。同樣由於這邊不存在 class，因此 value 只要是非 0 即可，只是用來 match 執行 policing action。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">pass</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> __sk_buff *skb)</span> &#123;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>最後的 <code>pass</code> 其實就是一條無條件回傳 1 表示 match，來執行 wan0 方向第二條 1024kbits&#x2F;sec 的限流政策用的。</p><h2 id="tc-與-XDP-比較"><a href="#tc-與-XDP-比較" class="headerlink" title="tc 與 XDP 比較"></a>tc 與 XDP 比較</h2><p>在 eBPF 裡面，XDP 和 TC 兩個功能常常被拿來一起討輪，前面有提到 eBPF 可以做為 tc actions 使用來達到封包過濾之類的效果，雖然實行效果上是比不上 XDP 的，但是 tc ingress 的 eBPF hook point 也在 kernel data path 的最早期，因此也能夠提供不錯的效能，加上 tc ebpf program 的 context 是 <code>sk_buff</code>，相較於 <code>xdp_buff</code>，可以直接透過 <code>__sk_buff</code> 取得和修改更多的 meta data，加上 tc 在 ingress 和 egress 方向都有 hook point，不像 XDP 只能作用在 ingress 方向，且 tc 完全不需要驅動支援即可工作，因此 tc 在使用彈性和靈活度上是比 XDP 更占優的。</p><blockquote><p>不過 tc 其實也有提供 offload 的功能，將 eBPF 程式 offload 到網卡上面執行。</p></blockquote><h2 id="Direct-action"><a href="#Direct-action" class="headerlink" title="Direct action"></a>Direct action</h2><p>然而由於 tc 的 hook point 分成 classifier 和 action 因此無法透過單一個 eBPF 程式做到 match-action 的效果，然而大多數時候 eBPF tc 程式的開發並不是要利用 tc 系統的功能做限速等功能，而是要利用 tc 在 kernel path 極早期這點做 packet mangling 和 filter 等事項，再加上 tc 系統的使用學習難度相對高，因此 eBPC 在 tc 後引入了 <a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=045efa82ff563cd4e656ca1c2e354fa5bf6bbda4">direct-action</a> 和 <a href="%5B%601f211a1b929c%60%5D(https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=1f211a1b929c804100e138c5d3d656992cfd5622)">clsact</a> 這兩個功能。</p><p>首先介紹 direct-action (da)，這個是在 classifier (BPF_PROG_TYPE_SCHED_CLS) 可啟用的一個選項，如果啟用 da，classifier 的回傳值就變成是 action，和 BPF_PROG_TYPE_SCHED_ACT 相同，而原本的 classid 改成設置__skb_buff-&gt;tc_classid 來傳輸。</p><blockquote><p>在 kernel code 內使用 prog-&gt;exts_integrated 標示是否啟用 da 功能</p></blockquote><p>透過 da 可以透過單一個 eBPF 程式完成 classifier 和 action 的功能，降低了 tc hook point 對原本 tc 系統框架的依賴，能夠透過 eBPF 程式簡潔的完成功能。</p><p>在 da 的使用上可以參考 bcc 的範例 <code>examples/networking/tc_perf_event.py</code>，使用上與普通的 classifer 幾乎無異，只要在載入時 <code>ipr.tc (&quot;add-filter&quot;,&quot;bpf&quot;, me,&quot;:1&quot;, fd=fn.fd, ... ,direct_action=True)</code> 加上 direct_action 選項即可。</p><p>透過 tc 指令查看時也可以看到 <code>direct-action</code> 字樣。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">tc filter show dev t1a<br>filter parent 1: protocol all pref 49152 bpf chain 0 <br>filter parent 1: protocol all pref 49152 bpf chain 0 handle 0x1 flowid :1 hello direct-action not_in_hw id 308 tag 57cd311f2e27366b jited <br>action order 1: gact action pass<br> random type none pass val 0<br> index 2 ref 1 bind 1<br><br></code></pre></td></tr></table></figure><h2 id="clsact"><a href="#clsact" class="headerlink" title="clsact"></a>clsact</h2><p>後來 tc 加入了 clsact，clsact 是一個專為 eBPF 設計的偽 qdisc。首先 clsact 同時作用在 ingress 和 egress 方向，也進一步簡化了 ebpf 程式的掛載。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tc qdisc add dev em1 clsact<br>tc filter add dev em1 ingress bpf da obj tc-example.o sec ingress<br>tc filter add dev em1 egress bpf da obj tc-example.o sec egress<br></code></pre></td></tr></table></figure><p>同時 clsact 工作在真的 qdisc 本身的 lock 之前，因此可以避免 lock 的開銷，預先完成比較複雜繁重的封包分類，在進入到真的 queue filter 時只根據更簡單的欄位 (如 tc_index) 做分類。另外 da 本來只能使用在 ingress 方向，透過 clsact，da 可以工作在 egress 方向。</p><p>關於 eBPF tc 的部分就大致上介紹到這裡，對於 tc 這個子系統相對來說是真的蠻陌生的，因此介紹這個部分的確是有比較大的難度和說不清楚的地方。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://tldp.org/HOWTO/Adv-Routing-HOWTO/lartc.qdisc.filters.html">Classifying packets with filters</a></li><li><a href="https://man7.org/linux/man-pages/man8/tc.8.html">tc man page</a></li><li><a href="https://arthurchiao.art/blog/lartc-qdisc-zh/">用 tc qdisc 管理 Linux 网络带宽</a></li><li><a href="https://cloud.tencent.com/developer/article/1409664">TC (Traffic Control) 命令</a></li><li><a href="https://developer.aliyun.com/article/4000">Linux TC (Traffic Control) 简介</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;接續前一篇主題 &lt;code&gt;XDP&lt;/code&gt;，今天我們要繼續來聊聊 eBPF 在 linux netowrk data path 上的另外一個進入點 &lt;code&gt;tc&lt;/code&gt;，並同樣以 bcc 的 &lt;a href=&quot;https://github.com/iovisor/bcc/blob/master/examples/networking/neighbor_sharing/&quot;&gt;neighbor_sharing&lt;/a&gt; 作為範例。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
    <category term="tc" scheme="https://blog.louisif.me/tags/tc/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 6 - XDP &amp; BCC xdp_redirect_map</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-6-XDP-BCC-xdp-redirect-map/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-6-XDP-BCC-xdp-redirect-map/</id>
    <published>2022-10-31T09:00:33.000Z</published>
    <updated>2022-10-31T10:33:11.754Z</updated>
    
    <content type="html"><![CDATA[<p>這篇文章會介紹 eBPF 一個比較知名的用途，express data path (XDP)，並使用 bcc 的 <a href="(https://github.com/iovisor/bcc/tree/master/examples/networking/xdp/xdp_redirect_map.py">xdp_redirect_map.py</a> 作為範例。</p><span id="more"></span><h2 id="XDP-介紹"><a href="#XDP-介紹" class="headerlink" title="XDP 介紹"></a>XDP 介紹</h2><p>會說 linux 的網路慢主要是因為封包在進出 linux 設備時要經過 linux kernel 的 network stack，經過大家熟悉的 iptables, routing table.. 等網路子系統的處理，然而經由這麼多複雜的系統處理就會帶來延遲，降低 linux 網路的效能。</p><p><img src="/eBPF/Learn-eBPF-Serial-6-XDP-BCC-xdp-redirect-map/linux_network_stack_ingress.png" alt="Linux network stack ingress"></p><p>上圖是封包在經由 linux 網路子系統到進入路由前的截圖，可以看到在封包剛進入到 linux kernel，甚至連前面看過，linux 用來維護每一個封包的 skb 結構都還沒建立前，就會呼叫到 XDP eBPF 程式，因此如果我們能夠在 XDP 階段就先過濾掉大量的封包，或封包轉發、改寫，能夠避免掉進入 linux 網路子系統的整個過程，降低 linux 處理封包的成本、提高性能。</p><p>前面提到 XDP 工作在封包進入 linux kernel 的非常早期，甚至早於 skb 的建立，其實 XDP 的 hook point 直接是在 driver 內，因此 XDP 是需要 driver 特別支援的，為此 XDP 其實有三種工作模式: <code>xdpdrv</code>, <code>xdpgeneric</code>,<code>xdpoffload</code>。<br><code>xdpdrv</code> 指的是 native XDP，就是標準的 XDP 模式，他的 hook point 在 driver 層，因此是網卡接收到封包送至系統的第一位，可以提供極好的網路性能。<br><code>xdpgeneric</code>: generic XDP 提供一個在 skb 建立後的 XDP 進入點，因此可以在 driver 不支援的情況下提供 XDP 功能，但也由於該進入點比較晚，所以其實不太能提供好的網路效能，該進入點主要是讓新開發者在缺乏支援網卡的情況下用於測試學習，以及提供 driver 開發者一個標準用。<br><code>xdpoffload</code>: 在某些網卡下，可以將 XDP offload 到網卡上面執行，由於直接工作在網卡晶片上，因此能夠提供比 native XDP 還要更好的性能，不過缺點就是需要網卡支援而且部分的 map 和 helper function 會無法使用。</p><p>XDP 的 return 數值代表了封包的下場，總共有五種結果，定義在 xdp_action</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">xdp_action</span> &#123;</span><br>XDP_ABORTED = <span class="hljs-number">0</span>,<br>XDP_DROP,<br>XDP_PASS,<br>XDP_TX,<br>XDP_REDIRECT,<br>&#125;;<br></code></pre></td></tr></table></figure><ul><li><p>XDP_ABORTED, XDP_DROP 都代表丟棄封包，因此使用 XDP 我們可以比較高效的丟棄封包，用於防禦 DDoS 攻擊。<br>不過 XDP_ABORTED 同時會產生一個 eBPF 系統錯誤，可以透過 tracepoint 機制來查看。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">echo 1 &gt; /sys/kernel/debug/tracing/events/xdp/xdp_exception/enable<br>cat /sys/kernel/debug/tracing/trace_pipe <br>systemd-resolve-512     [000] .Ns.1  5911.288420: xdp_exception: prog_id=91 action=ABORTED ifindex=2<br>...<br></code></pre></td></tr></table></figure></li><li><p>XDP_PASS 就是正常的讓封包通過不處理。</p></li><li><p>XDP_TX 是將封包直接從原始網卡送出去，我們可以透過在 XDP 程式內修改封包內容，來修改目的地 IP 和 MAC，一個使用前景是用於 load balancing，可以將封包打到 XDP 主機，在修改封包送去後端主機。</p></li><li><p>XDP_REDIRECT 是比較後來新加入的一個功能，它可以將封包</p><ul><li>直接轉送到另外一張網路卡，直接送出去</li><li>指定給特定的 CPU 處理</li><li>將封包直接送給特定的一個 AF_XDP 的 socket 來達到跳過 kernel stack 直接交由 user space 處理的效過</li></ul></li></ul><p>最後，前面提到 XDP 早於 skb 的建立，因此 XDP eBPF program 的上下文不是__skb__buff，而是使用自己的 <code>xdp_md</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">xdp_md</span> &#123;</span><br>__u32 data;<br>__u32 data_end;<br>__u32 data_meta;<br><span class="hljs-comment">/* Below access go through struct xdp_rxq_info */</span><br>__u32 ingress_ifindex; <span class="hljs-comment">/* rxq-&gt;dev-&gt;ifindex */</span><br>__u32 rx_queue_index;  <span class="hljs-comment">/* rxq-&gt;queue_index  */</span><br>&#125;;<br></code></pre></td></tr></table></figure><p>可以看到 xdp_md 是一個非常精簡的資料結構，因為 linux 還沒對其做解析提取出更多資訊。</p><h2 id="xdp-redirect-map-介紹"><a href="#xdp-redirect-map-介紹" class="headerlink" title="xdp_redirect_map 介紹"></a>xdp_redirect_map 介紹</h2><p>這次使用的 eBPF program type 理所當然是 <code>BPF_PROG_TYPE_XDP</code>。</p><p>這隻程式的功能很簡單，執行時指定兩個 interface <code>in_intf</code> 和 <code>out_intf</code>，所有從 <code>in_intf</code> 進入的封包會直接從 <code>out_intf</code> 送出去，並且交換 src mac address 和 dst mac address，同時記錄每秒鐘通過該介面的封包個數。</p><p>從 out_intf 進入的封包則正常交給 linux network 系統處理。<br><img src="/eBPF/Learn-eBPF-Serial-6-XDP-BCC-xdp-redirect-map/xdp_example_topo.png" alt="XDP example topology"><br>首先我們一樣要先驗證程式的執行，首先建立一個 network namespace net0。然後把兩個網卡 <code>veth_in_intf</code>, <code>veth_out_intf</code> 放進去，作為 xdp_redirect_map 使用的網卡。為了方便打流量，我們幫 <code>in_intf</code> 指定一個 ip 10.10.10.1，並幫加入一個不存在的遠端 ip 10.10.10.2，接著我們就可以透過 ping 10.10.10.2 來從 in_intf 打流量，透過 tcpdump 捕捉 out_intf 的封包，應該就可以看到從 10.10.10.1 過來的封包，同時 mac address 被交換了，所以可以看到 src mac 變成 ee:11:ee:11:ee:11。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ip netns add net0<br>ip link add in_intf type veth peer name veth_in_intf<br>ip link add out_intf type veth peer name veth_out_intf<br>ip link set veth_in_intf netns net0<br>ip link set veth_out_intf netns net0<br>ip link set in_intf up<br>ip link set out_intf up<br>ip netns exec net0 ip link set veth_in_intf up<br>ip netns exec net0 ip link set veth_out_intf up<br>ip address add 10.10.10.1/24 dev in_intf<br>ip neigh add 10.10.10.2 lladdr ee:11:ee:11:ee:11 dev in_intf<br></code></pre></td></tr></table></figure><blockquote><p>目前這個部分其實沒有驗證成功，雖然根據 xdp redirect 的 log，封包是真的有成功被轉送到 veth_out_intf 的，然後透過 tcpdump 卻沒有在 out_intf 上收到封包，可惜的是具體原因沒能確定。</p></blockquote><h2 id="xdp-redirect-map-實作"><a href="#xdp-redirect-map-實作" class="headerlink" title="xdp_redirect_map 實作"></a>xdp_redirect_map 實作</h2><h3 id="eBPF-實作"><a href="#eBPF-實作" class="headerlink" title="eBPF 實作"></a>eBPF 實作</h3><p>這次的程式非常簡短，首先是一個 swap_src_dst_mac 函數，用於交換封包的 src mac address 和 dst mac address。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-keyword">inline</span> <span class="hljs-type">void</span> <span class="hljs-title function_">swap_src_dst_mac</span><span class="hljs-params">(<span class="hljs-type">void</span> *data)</span><br>&#123;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> *p = data;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> dst [<span class="hljs-number">3</span>];<br>    dst [<span class="hljs-number">0</span>] = p [<span class="hljs-number">0</span>];<br>    dst [<span class="hljs-number">1</span>] = p [<span class="hljs-number">1</span>];<br>    dst [<span class="hljs-number">2</span>] = p [<span class="hljs-number">2</span>];<br>    p [<span class="hljs-number">0</span>] = p [<span class="hljs-number">3</span>];<br>    p [<span class="hljs-number">1</span>] = p [<span class="hljs-number">4</span>];<br>    p [<span class="hljs-number">2</span>] = p [<span class="hljs-number">5</span>];<br>    p [<span class="hljs-number">3</span>] = dst [<span class="hljs-number">0</span>];<br>    p [<span class="hljs-number">4</span>] = dst [<span class="hljs-number">1</span>];<br>    p [<span class="hljs-number">5</span>] = dst [<span class="hljs-number">2</span>];<br>&#125;<br></code></pre></td></tr></table></figure><p>由於 mac address 在 ethernet header 的前 12 個 bit 所以可以很簡單地進行交換。</p><p>接著就直接進入到了 attach 在 in interface 上的 XDP 函數</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">xdp_redirect_map</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> xdp_md *ctx)</span> &#123;<br>    <span class="hljs-type">void</span>* data_end = (<span class="hljs-type">void</span>*)(<span class="hljs-type">long</span>) ctx-&gt;data_end;<br>    <span class="hljs-type">void</span>* data = (<span class="hljs-type">void</span>*)(<span class="hljs-type">long</span>) ctx-&gt;data;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethhdr</span> *<span class="hljs-title">eth</span> =</span> data;<br>    <span class="hljs-type">uint32_t</span> key = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">long</span> *value;<br>    <span class="hljs-type">uint64_t</span> nh_off;<br>    nh_off = <span class="hljs-keyword">sizeof</span>(*eth);<br>    <span class="hljs-keyword">if</span> (data + nh_off  &gt; data_end)<br>        <span class="hljs-keyword">return</span> XDP_DROP;<br>    value = rxcnt.lookup (&amp;key);<br>    <span class="hljs-keyword">if</span> (value)<br>        *value += <span class="hljs-number">1</span>;<br>    swap_src_dst_mac (data);<br>    <span class="hljs-keyword">return</span> tx_port.redirect_map (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>首先 data 及 data_end 是分別指到封包頭尾的指標，由於封包頭都是 ethernet header，因此可以直接將 data 轉成 <code>ethhdr</code> 指標。<br>首先對 ethernet 封包做一個完整性檢查，<code>data + nh_off &gt; data_end</code> 表示封包大小小於一個 ethernet header，表示封包表示不完整，就直接將封包透過 <code>XDP_DROP</code> 丟棄。</p><p>接著 <code>rxcxt</code> 是預先定義的一個 <code>BPF_PERCPU_ARRAY (rxcnt, long, 1);</code>，PER_CPU map 的特性是每顆 CPU 上都會保有一份獨立不同步的資料，因此可以避免 cpu 之間的 race condition，減少 lock 的開銷。<br>這邊指定每個 CPU 上的 array 長度為 1，可以參考 Day11 有介紹過，是一個特別的使用技巧，可以簡單看成一個可以跟 user space share 的全域變數。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">uint32_t</span> key = <span class="hljs-number">0</span>;<br>value = rxcnt.lookup (&amp;key);<br><span class="hljs-keyword">if</span> (value)<br>*value += <span class="hljs-number">1</span>;<br>    <br></code></pre></td></tr></table></figure><p>這邊的用途是用來統計經過的封包個數，因此這邊非常簡單，統一使用 0 當作 key 去存取唯一的 value，然後每經過一個封包就將 value 加一，這邊可以注意到 lookup 回傳的是 pointer，因此可以直接對他做修改即可保存。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">swap_src_dst_mac (data);<br><span class="hljs-keyword">return</span> tx_port.redirect_map (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>最後會呼叫 <code>swap_src_dst_mac</code> 來交換封包，然後透過 <code>redirect_map</code> 來將封包送到對應的 out interface。</p><p>BPF_MAP_TYPE_DEVMAP 和 BPF_MAP_TYPE_CUPMAP 是用來搭配 XDP_REDIRECT，將封包導向透定的 CPU 或著從其他 interface 送出去的。</p><p>而這邊的 redirect_map 在編譯時會被修改為呼叫 bpf_redirect_map 這個 helper function。其定義為 <code>long bpf_redirect_map (struct bpf_map *map, u32 key, u64 flags)</code>，透過接收 map 可以根據對應到的 value 來將封包導向到 interface 或著 CPU，設置方法會在後面的 python code 介紹。<br>由於我們今天只為有一個 out interface，因此可以很簡單的指定 key 為 0</p><p>後面的 flags 目前只有使用最後兩個 bit，可以當作 key 找不到時 redirect_map 的回傳值，因此以本次的 code 來說，預設的回傳數值是 0，也就對應到 XDP_ABORTED。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">xdp_dummy</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> xdp_md *ctx)</span> &#123;<br>    <span class="hljs-keyword">return</span> XDP_PASS;<br>&#125;<br></code></pre></td></tr></table></figure><p>最後一段程式碼 <code>xdp_dummy</code> 是用來皆在 out interface 上的 XDP 程式，但他就只是簡單的 <code>XDP_PASS</code>，讓進入的封包繼續交由 linux kernel 來處理。</p><h3 id="python-實作"><a href="#python-實作" class="headerlink" title="python 實作"></a>python 實作</h3><p>接下來就進入到 python code 的部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">in_if = sys.argv [<span class="hljs-number">1</span>]<br>out_if = sys.argv [<span class="hljs-number">2</span>]<br><br>ip = pyroute2.IPRoute ()<br>out_idx = ip.link_lookup (ifname=out_if)[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>首先將兩張網卡的名稱讀進來，接著透過 pyroute2 的工具去找到 out interface 的 ifindex</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tx_port = b.get_table (<span class="hljs-string">&quot;tx_port&quot;</span>)<br>tx_port [<span class="hljs-number">0</span>] = ct.c_int (out_idx)<br></code></pre></td></tr></table></figure><p>接著是設定 tx_port 這張 DEVMAP 的 key 0 為 out interface 的 index，因此所有經過 eBPF 程式的封包都會丟到 out interface</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">in_fn = b.load_func (<span class="hljs-string">&quot;xdp_redirect_map&quot;</span>, BPF.XDP)<br>out_fn = b.load_func (<span class="hljs-string">&quot;xdp_dummy&quot;</span>, BPF.XDP)<br>b.attach_xdp (in_if, in_fn, flags)<br>b.attach_xdp (out_if, out_fn, flags)<br></code></pre></td></tr></table></figure><p>接著就是將 eBPF 程式 attach 到兩張網卡上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">rxcnt = b.get_table (<span class="hljs-string">&quot;rxcnt&quot;</span>)<br>prev = <span class="hljs-number">0</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>val = rxcnt.<span class="hljs-built_in">sum</span>(<span class="hljs-number">0</span>).value<br><span class="hljs-keyword">if</span> val:<br>delta = val - prev<br>prev = val<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125; pkt/s&quot;</span>.<span class="hljs-built_in">format</span>(delta))<br>time.sleep (<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>將 eBPF 程式 attach 上去之後就完成了封包重導向的工作，剩下的部分是用來統計每秒鐘經過的封包的，這邊的做法很簡單，每秒鐘都去紀錄一次通過封包總量和前一秒鐘的差異就可以算出來這一秒內經過的封包數量。<br>這邊比較特別的是 <code>rxcnt.sum</code>，前面提到 rxcnt 是一個 per cpu 的 map，因此這邊使用 sum 函數將每顆 cpu 的 key 0 直接相加起來，就可以得到經過所有 CPU 的封包總量。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;這篇文章會介紹 eBPF 一個比較知名的用途，express data path (XDP)，並使用 bcc 的 &lt;a href=&quot;(https://github.com/iovisor/bcc/tree/master/examples/networking/xdp/xdp_redirect_map.py&quot;&gt;xdp_redirect_map.py&lt;/a&gt; 作為範例。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
    <category term="xdp" scheme="https://blog.louisif.me/tags/xdp/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 5 - BCC HTTP Filter &amp; Socket Filter</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-5-BCC-HTTP-Filter-Socket-Filter/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-5-BCC-HTTP-Filter-Socket-Filter/</id>
    <published>2022-10-31T08:18:42.000Z</published>
    <updated>2022-10-31T10:32:58.105Z</updated>
    
    <content type="html"><![CDATA[<p>這篇文章會介紹 eBPF socket filter 的概念，並使用 bcc 的 <a href="https://github.com/iovisor/bcc/tree/master/examples/networking/http_filter">http-parse-simple.py</a> 作為範例，來說明如何使用 eBPF socket filter 來過濾 HTTP 請求，並且會深入底下 eBPF 的 socket filter 底層部分的實作。</p><span id="more"></span><h2 id="Socket-filter-介紹"><a href="#Socket-filter-介紹" class="headerlink" title="Socket filter 介紹"></a>Socket filter 介紹</h2><p>前一篇文章介紹的 tcpconnect 是使用 <code>BPF_PROG_TYPE_KPROBE</code> 這個 program type，透過 kprobe&#x2F;kretprobe 機制在 kernel function 被呼叫和回傳的時候執行。</p><p>這次使用的是 <code>BPF_PROG_TYPE_SOCKET_FILTER</code>，socket filter 可以對進出 socket 的封包進行截斷或過濾。特別注意這邊如果會需要擷取封包 (長度不等於原始封包長度) 則會觸發對封包進行複製，然後修改封包大小。</p><p>socket filter program 會在 socket 層被呼叫 (在 net&#x2F;core&#x2F;sock.c 的 sock_queue_rcv_skb 被呼叫)，並傳入 <a href="https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/bpf.h#L5745">_sk_buff 結構</a> 取得 socket 上下文及封包的內容。</p><p>透過回傳的數值來決定如何處理該封包，如果回傳的數值大於等於封包長度，等價於保留完整封包，如果長度小於封包長度，則截斷只保留回傳數值長度的封包。其中兩個特例是回傳 0 和 - 1。回傳 0 等價解取一個長度為 0 的封包，也就是直接丟棄該封包。回傳 - 1 時，由於封包長度是無號整數，-1 等價於整數的最大數值，因此保證保留整個完整的封包。</p><p>另外一個關鍵技術是 raw socket，我們可以將 raw socket 監聽某個網路介面上所有進出封包。</p><p>因此整個程式的執行方式是這樣的，在目標網路卡上開啟一個 raw socket，透過 eBPF 程式過濾掉所有非 http 的封包，只保留 http 封包送出到 raw socket，userspace client 接收到封包時，可以直接解析封包欄位提取出 http 封包資訊。</p><h2 id="http-parse-simple-介紹"><a href="#http-parse-simple-介紹" class="headerlink" title="http-parse-simple 介紹"></a>http-parse-simple 介紹</h2><p>首先一樣先了解一下這支程式的功能，http-parse 能夠綁定到一張網路卡上面執行，然後提取經過 http 流量，將 http version, method, uri 和 status 輸出顯示。(當然如果經過 tls 加密的話是沒辦法的)</p><p>執行結果如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">python http-parse-complete.py <br>GET /pipermail/iovisor-dev/ HTTP/1.1<br>HTTP/1.1 200 OK<br>GET /favicon.ico HTTP/1.1<br>HTTP/1.1 404 Not Found<br>GET /pipermail/iovisor-dev/2016-January/thread.html HTTP/1.1<br>HTTP/1.1 200 OK<br>GET /pipermail/iovisor-dev/2016-January/000046.html HTTP/1.1<br>HTTP/1.1 200 OK<br></code></pre></td></tr></table></figure><h2 id="http-parse-simple-實作"><a href="#http-parse-simple-實作" class="headerlink" title="http-parse-simple 實作"></a>http-parse-simple 實作</h2><h3 id="eBPF-實作"><a href="#eBPF-實作" class="headerlink" title="eBPF 實作"></a>eBPF 實作</h3><p>在這次的程式中 eBPF c code 直接寫在一個獨立的 http-parse-simple.c 檔案中。</p><p>這次的 ebpf 程式很簡單只有單一個函數 <code>http_filter</code>，作為 socket filter 的進度點。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">http_filter</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> __sk_buff *skb)</span> &#123;<br><br>u8 *cursor = <span class="hljs-number">0</span>;<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethernet_t</span> *<span class="hljs-title">ethernet</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ethernet));<br><span class="hljs-comment">//filter IP packets (ethernet type = 0x0800)</span><br><span class="hljs-keyword">if</span> (!(ethernet-&gt;type == <span class="hljs-number">0x0800</span>)) &#123;<br><span class="hljs-keyword">goto</span> DROP;<br>&#125;<br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ip_t</span> *<span class="hljs-title">ip</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ip));<br><br><span class="hljs-comment">//drop the packet returning 0</span><br>DROP:<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>...<br></code></pre></td></tr></table></figure><p>相信很多人跟我一樣第一眼看到這個程式會覺得非常疑惑，首先看到的是 <code>cursor</code> 和 <code>cursor_advance</code> 這兩個東西，從 ip 那行大概可以猜的出來，cursor 是對封包內容存取位置的指標，cursor_advance 會輸出當前 cursor 的位置，然後將 cursor 向後移動第二個參數的長度。<br>由於我們要分析的是 http 封包，所以他的 ether type 勢必得是 0x0800 (IP)，所以對於不滿足的封包，我們直接 goto 到 drop，return 0。表示我們要擷取一個長度為 0 的封包等價於丟棄該封包。</p><p>在 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/src/cc/export/helpers.h">helpers.h</a> 輔助函數標頭檔裡面可以看到 cursor_advane 的定義。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">//packet parsing state machine helpers<br>#define cursor_advance (_cursor, _len) \<br>  (&#123; void *_tmp = _cursor; _cursor += _len; _tmp; &#125;)<br></code></pre></td></tr></table></figure><p>果然符合我們的預期，先將原先 cursor 指標的數值保留起來，將 cursor 向後移動 len 後回傳原始數值。</p><p>後面的程式碼其實就很簡單，首先一路解析封包確保他是一個 ip&#x2F;tcp&#x2F;http 封包、封包長度夠長塞的下一個有效的 http 封包內容</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c">payload_offset = ETH_HLEN + ip_header_length + tcp_header_length;<br>...<br><span class="hljs-type">unsigned</span> <span class="hljs-type">long</span> p [<span class="hljs-number">7</span>];<br><span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">7</span>; i++) &#123;<br>p [i] = load_byte (skb, payload_offset + i);<br>&#125;<br></code></pre></td></tr></table></figure><p>接著將 http packet 的前 7 個 byte 讀出來，load_byte 同樣是定義在 <a href="https://github.com/iovisor/bcc/blob/master/src/cc/export/helpers.h">helpers.h</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">unsigned long long load_byte (void *skb,<br>unsigned long long off) asm (&quot;llvm.bpf.load.byte&quot;);<br></code></pre></td></tr></table></figure><p>他會直接轉譯成 BPF_LD_ABS，從 payload_offset 位置開始讀一個 byte 出來，payload_offset，是前面算出來從 ethernet header 開始到 http payload 的位移。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//HTTP</span><br><span class="hljs-keyword">if</span> ((p [<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;H&#x27;</span>) &amp;&amp; (p [<span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;T&#x27;</span>) &amp;&amp; (p [<span class="hljs-number">2</span>] == <span class="hljs-string">&#x27;T&#x27;</span>) &amp;&amp; (p [<span class="hljs-number">3</span>] == <span class="hljs-string">&#x27;P&#x27;</span>)) &#123;<br><span class="hljs-keyword">goto</span> KEEP;<br>&#125;<br><span class="hljs-comment">//GET</span><br><span class="hljs-keyword">if</span> ((p [<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;G&#x27;</span>) &amp;&amp; (p [<span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;E&#x27;</span>) &amp;&amp; (p [<span class="hljs-number">2</span>] == <span class="hljs-string">&#x27;T&#x27;</span>)) &#123;<br><span class="hljs-keyword">goto</span> KEEP;<br>&#125;<br>...<br><span class="hljs-comment">//no HTTP match</span><br><span class="hljs-keyword">goto</span> DROP;<br><br><span class="hljs-comment">//keep the packet and send it to userspace returning -1</span><br>KEEP:<br><span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br></code></pre></td></tr></table></figure><p>接著檢查如果封包屬於 HTTP (以 HTTP, GET, POST, PUT, DELETE HEAD… 開頭)，就會跳到 keep，保留整個完整的封包送到 userspace client program。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">GET /favicon.ico HTTP/1.1<br>HTTP/1.1 200 OK<br></code></pre></td></tr></table></figure><p>HTTP request 會以 method 開頭、response 會以 HTTP 開頭，所以需要查找這些字樣開頭的封包。</p><h3 id="python-實作"><a href="#python-實作" class="headerlink" title="python 實作"></a>python 實作</h3><p>接著我們很快速的來看一下 python 程式碼的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">bpf = BPF (src_file = <span class="hljs-string">&quot;http-parse-simple.c&quot;</span>,debug = <span class="hljs-number">0</span>)<br>function_http_filter = bpf.load_func (<span class="hljs-string">&quot;http_filter&quot;</span>, BPF.SOCKET_FILTER)<br>BPF.attach_raw_socket (function_http_filter, interface)<br>socket_fd = function_http_filter.sock<br>sock = socket.fromfd (socket_fd,socket.PF_PACKET,socket.SOCK_RAW,socket.IPPROTO_IP)<br>sock.setblocking (<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>首先我們一樣透過 BPF 物件完成 bpf 程式碼的編譯，不一樣的是是這邊直接指定 src_file 從檔案讀取。<br>接著透過 load_func，指定 socket filter 這個 program type type 和 http_filter 這個入口函數，並載入 ebpf bytecode 到 kernel<br>接著透過 bcc 提供的 attach_raw_socket API 在 interface 上建立 row socket 並將 socket filter program attach 上去。<br>接著從 <code>function_http_filter.sock</code> 取得 raw socket 的 file descripter 並封裝成 python 的 socket 物件。<br>由於後面需要 socket 是阻塞的，但是 attach_raw_socket 建立出來的 socket 是非阻塞的，所以這邊透過 <code>sock.setblocking (True)</code> 阻塞 socket</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">while 1:<br>  #retrieve raw packet from socket<br>  packet_str = os.read (socket_fd,2048)<br>  packet_bytearray = bytearray (packet_str)<br>  ...<br>  for i in range (payload_offset,len (packet_bytearray)-1):<br>    if (packet_bytearray [i]== 0x0A): # \n<br>      if (packet_bytearray [i-1] == 0x0D): \r<br>        break # 遇到 http 的換行 \r\n 則結束 < br>    print (&quot;% c&quot; % chr (packet_bytearray [i]), end = &quot;&quot;)<br></code></pre></td></tr></table></figure><p>後面的程式碼其實就和 ebpf 的部分大同小異，從 socket 讀取封包內容、解析到 http payload 後，將 http payload 的第一行輸出出來。</p><p>到此我們就完成了 <code>http-parse-simple</code> 的解析。</p><h3 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h3><ol><li>cursor 指標數值為 0，但是可以存取到封包的內容。</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c">u8 *cursor = <span class="hljs-number">0</span>;<br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethernet_t</span> *<span class="hljs-title">ethernet</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ethernet));<br><span class="hljs-keyword">if</span> (!(ethernet-&gt;type == <span class="hljs-number">0x0800</span>)) &#123;<br><span class="hljs-keyword">goto</span> DROP;<br>&#125;<br><br></code></pre></td></tr></table></figure><ol start="2"><li>特別的 load_bytes 函數呼叫，來取得封包內容<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">load_byte (skb, payload_offset + i);<br></code></pre></td></tr></table></figure></li></ol><p>首先雖然 ebpf 使用 c 來編寫，但是經由 LLVM 編譯後會轉換成 eBPF bytecode，在進入 kernel 後會再經過 verifier 的修改。(經過這次的探索，可以理解 verifier 雖然叫做 verifier，但是他的功能確包羅萬象，對 eBPF 架構來說非常重要)</p><h2 id="深入了解-Socket-filter"><a href="#深入了解-Socket-filter" class="headerlink" title="深入了解 Socket filter"></a>深入了解 Socket filter</h2><p>為了理解這段 eBPF code 後面發生了什麼事，我們先查看 LLVM 編譯出來的 eBPF bytecode。</p><h3 id="BCC-Debug"><a href="#BCC-Debug" class="headerlink" title="BCC Debug"></a>BCC Debug</h3><p>在 BCC 編譯時，我們可以透過 <code>debug</code> 這個參數取得編譯過程中的資訊，當然也包含取得 LLVM 編譯出來的 eBPF bytecode，可以使用的 debug 選項如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Debug flags</span><br><br><span class="hljs-comment"># Debug output compiled LLVM IR.</span><br>DEBUG_LLVM_IR = <span class="hljs-number">0x1</span><br><span class="hljs-comment"># Debug output loaded BPF bytecode and register state on branches.</span><br>DEBUG_BPF = <span class="hljs-number">0x2</span><br><span class="hljs-comment"># Debug output pre-processor result.</span><br>DEBUG_PREPROCESSOR = <span class="hljs-number">0x4</span><br><span class="hljs-comment"># Debug output ASM instructions embedded with source.</span><br>DEBUG_SOURCE = <span class="hljs-number">0x8</span><br><span class="hljs-comment"># Debug output register state on all instructions in addition to DEBUG_BPF.</span><br>DEBUG_BPF_REGISTER_STATE = <span class="hljs-number">0x10</span><br><span class="hljs-comment"># Debug BTF.</span><br>DEBUG_BTF = <span class="hljs-number">0x20</span><br></code></pre></td></tr></table></figure><h3 id="解析-simple-http-parse-編譯結果"><a href="#解析-simple-http-parse-編譯結果" class="headerlink" title="解析 simple-http-parse 編譯結果"></a>解析 simple-http-parse 編譯結果</h3><p>透過 <code>BPF (src=&#39;simple-http-parse.c&#39;, debug=DEBUG_PREPROCESSOR)</code>，我們可以看到上面的 code 被 LLVM 重新解釋為</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span> *cursor = <span class="hljs-number">0</span>;<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ethernet_t</span> *<span class="hljs-title">ethernet</span> =</span> cursor_advance (cursor, <span class="hljs-keyword">sizeof</span>(*ethernet));<br><br><span class="hljs-comment">//filter IP packets (ethernet type = 0x0800)</span><br><span class="hljs-keyword">if</span> (!(bpf_dext_pkt (skb, (u64) ethernet+<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, <span class="hljs-number">16</span>) == <span class="hljs-number">0x0800</span>)) &#123;<br><span class="hljs-keyword">goto</span> DROP;<br>&#125;<br></code></pre></td></tr></table></figure><p>因此 cursor 在這邊的用途真的只是計算 offset。<br>bpf_dext_pkt 在 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/src/cc/export/helpers.h">helper.h</a> 有所定義</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c">u64 <span class="hljs-title function_">bpf_dext_pkt</span><span class="hljs-params">(<span class="hljs-type">void</span> *pkt, u64 off, u64 bofs, u64 bsz)</span> &#123;<br>  <span class="hljs-keyword">if</span> (bofs == <span class="hljs-number">0</span> &amp;&amp; bsz == <span class="hljs-number">8</span>) &#123;<br>    <span class="hljs-keyword">return</span> load_byte (pkt, off);<br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (bofs + bsz &lt;= <span class="hljs-number">8</span>) &#123;<br>    <span class="hljs-keyword">return</span> load_byte (pkt, off) &gt;&gt; (<span class="hljs-number">8</span> - (bofs + bsz))  &amp;  MASK (bsz);<br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (bofs == <span class="hljs-number">0</span> &amp;&amp; bsz == <span class="hljs-number">16</span>) &#123;<br>     <span class="hljs-keyword">return</span> load_half (pkt, off);<br>  ... <br></code></pre></td></tr></table></figure><p>可以看到他是根據參數大小和類型去正確呼叫 load_byte, load_half, load_dword 系列函數，所以其實他做的事情與我們感興趣的第二段 code <code>load_byte (skb, payload_offset + i);</code> 是一致的。</p><p>接著我們使用 <code>BPF (src=&#39;simple-http-parse.c&#39;, debug=DEBUG_SOURCE)</code> 查看編譯出來的 eBPF binary code。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c">; <span class="hljs-type">int</span> <span class="hljs-title function_">http_filter</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> __sk_buff *skb)</span> &#123; <span class="hljs-comment">// Line  27</span><br>   <span class="hljs-number">0</span>:bf <span class="hljs-number">16</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span>r6 = r1<br>   <span class="hljs-number">1</span>:<span class="hljs-number">28</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">0</span>c <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span>r0 = *(u16 *) skb [<span class="hljs-number">12</span>]<br>; <span class="hljs-keyword">if</span> (!(bpf_dext_pkt (skb, (u64) ethernet+<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, <span class="hljs-number">16</span>) == <span class="hljs-number">0x0800</span>)) &#123; <span class="hljs-comment">// Line  34</span><br>   <span class="hljs-number">2</span>:<span class="hljs-number">55</span> <span class="hljs-number">00</span> <span class="hljs-number">5</span>c <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">08</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span><span class="hljs-keyword">if</span> r0 != <span class="hljs-number">2048</span> <span class="hljs-keyword">goto</span> +<span class="hljs-number">92</span><br></code></pre></td></tr></table></figure><p>其中 r0, r1, r6 是 eBPF 的 register，我們這邊只關注第 1 行 <code>r0 = *(u16 *) skb [12]</code>，這行是從 skb 的第 12 個 byte 拿取資料出來，剛好對應到 bpf_dext_pkt。</p><p>根據 ebpf instruction set 的定義，第一個 byte 28 (0010 1000) 是 op code。<br>最後 3 個 bit 000 是 op code 的種類。這邊的 0x00 對應到 <code>BPF_LD</code> (non-standard load operations)</p><p>|3 bits (MSB) | 2 bits|3 bits (LSB)|<br>|———— |—–‐——|—–‐——|<br>| mode | size | instruction class|</p><p>在 <code>BPF_LD</code> 這個分類內，size bits 01 剛好對應到 <code>BPF_H</code> (half word (2 bytes))<br>最前面的 3 個 bit 000 代表 <code>BPF_ABS</code>(legacy BPF packet access)。</p><p>到這邊我們就理解它是怎麼運作了了，eBPF 定義了 <code>BPF_ABS</code> 來代表對封包的存取操作，LLVM 在編譯的時會將對 skb 的 load_byte 轉譯成對應的 instruction。</p><h3 id="了解-eBPF-BPF-ABS-instruction"><a href="#了解-eBPF-BPF-ABS-instruction" class="headerlink" title="了解 eBPF BPF_ABS instruction"></a>了解 eBPF BPF_ABS instruction</h3><p>我們可以更深入的了解一下 eBPF 對 <code>BPF_ABS</code> 做了什麼事情，在 verifier 這個神奇的地方搜尋 <code>BPF_ABS</code> 這個 instruction，會找到下面這段內容 (簡化版)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">/* Implement LD_ABS and LD_IND with a rewrite, if supported by the program type. */</span><br><span class="hljs-keyword">if</span> (BPF_CLASS (insn-&gt;code) == BPF_LD &amp;&amp;<br>(BPF_MODE (insn-&gt;code) == BPF_ABS ||<br> BPF_MODE (insn-&gt;code) == BPF_IND)) &#123;<br><br>cnt = env-&gt;ops-&gt;gen_ld_abs (insn, insn_buf);<br>new_prog = bpf_patch_insn_data (env, i + delta, insn_buf, cnt);<br></code></pre></td></tr></table></figure><p>首先執行條件是 <code>BPF_LD</code> 及 <code>BPF_ABS</code>，我們的 code 剛好符合這個條件，接著會呼叫 <code>env-&gt;ops-&gt;gen_ld_abs</code>，根據原本的 instrunction <code>insn</code>，生成新的 instruction 寫入 <code>insn_buf</code>，接著呼叫 <code>bpf_patch_insn_data</code> 將原本的指令取代為新的指令。</p><p>接著我們要找一下 <code>gen_ld_abs</code>，跟 day 11 介紹 map 的情況類似，verifier 定義了 bpf_verifier_ops 結構，讓不同的 program type 根據需要，實作 bpf_verifier_ops 定義的 function 來提供不同的功能和行為。</p><p>socket filter 的定義如下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">const</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_verifier_ops</span> <span class="hljs-title">sk_filter_verifier_ops</span> =</span> &#123;<br>.get_func_proto= sk_filter_func_proto,<br>.is_valid_access= sk_filter_is_valid_access,<br>.convert_ctx_access= bpf_convert_ctx_access,<br>.gen_ld_abs= bpf_gen_ld_abs,<br>&#125;;<br></code></pre></td></tr></table></figure><p>所以讓我們看到 <code>bpf_gen_ld_abs</code> (一樣經過簡化只看我們需要的部分)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title function_">bpf_gen_ld_abs</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> bpf_insn *insn,</span><br><span class="hljs-params">  <span class="hljs-keyword">struct</span> bpf_insn *insn_buf)</span><br>&#123;<br>*insn++ = BPF_MOV64_REG (BPF_REG_2, orig-&gt;src_reg);<br><br><span class="hljs-comment">/* We&#x27;re guaranteed here that CTX is in R6. */</span><br>*insn++ = BPF_MOV64_REG (BPF_REG_1, BPF_REG_CTX);<br><br>*insn++ = BPF_EMIT_CALL (bpf_skb_load_helper_16_no_cache);<br>&#125;<br></code></pre></td></tr></table></figure><p>看到最後一行就很清晰了，最後其實等於調用了內部使用的 helper function 來存取資料。eBPF 也提提供了類似的 helper function <code>bpf_skb_load_bytes</code>，來提供存取封包內容的功能。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c">BPF_CALL_2 (bpf_skb_load_helper_16_no_cache, <span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> sk_buff *, skb,<br>   <span class="hljs-type">int</span>, offset)<br>&#123;<br><span class="hljs-keyword">return</span> ____bpf_skb_load_helper_16 (skb, skb-&gt;data, skb-&gt;len - skb-&gt;data_len,<br>  offset);<br>&#125;<br></code></pre></td></tr></table></figure><p>而 bpf_skb_load_helper_16_no_cache 其實就是直接從 <code>sk_buff-&gt;data</code> 的位置取得資料，data 是 sk_buff 用來指到封包開頭的指標。</p><h3 id="sk-buff-的限制"><a href="#sk-buff-的限制" class="headerlink" title="__sk_buff 的限制"></a>__sk_buff 的限制</h3><p>既然整個指令的本質是從 <code>sk_buff-&gt;data</code> 拿取資料，那我們是不是能夠直接從 <code>__sk_buff</code> 裡面拿到資料呢？</p><p>在 socket program type 下 program context 是 <code>__sk_buff</code>，他其實本質是對 sk_buff 的多一層封裝 (原因 <a href="https://lwn.net/Articles/636647">參見</a>)，在執行的時候，verifier 換將其取代回 sk_buff，因此__sk_buff 等於是 sk_buff 暴露出來的介面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">struct __sk_buff &#123;<br>...<br>__u32 data;<br>__u32 data_end;<br>__u32 napi_id;<br>...<br></code></pre></td></tr></table></figure><p>參考__sk_buff 的定義，<code>__sk_buff</code> 是有定義將 <code>data</code> 和 <code>data_end</code>，那我們原始的 eBPF 程式是不是可以改成</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">void *cursor = (void*)(long)(__sk_buff-&gt;data);<br>struct ethernet_t *ethernet = cursor_advance (cursor, sizeof (*ethernet));<br>if (!(ethernet-&gt;type == 0x0800)) &#123;<br>goto DROP;<br>&#125;<br></code></pre></td></tr></table></figure><p>如果完成這樣的修改，重新跑一遍 <code>http-parse-simple.py</code>，你會得到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">python3 http-parse-simple.py -i eno0<br>binding socket to &#x27;enp0s3&#x27;<br>bpf: Failed to load program: Permission denied<br>; int http_filter (struct __sk_buff *skb) &#123;<br>0: (bf) r6 = r1<br>; void *cursor = (void*)(long) skb-&gt;data;<br>1: (61) r7 = *(u32 *)(r6 +76)<br>invalid bpf_context access off=76 size=4<br>processed 2 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0<br><br>Traceback (most recent call last):<br>  File &quot;http-parse-simple.py&quot;, line 69, in &lt;module&gt;<br>    function_http_filter = bpf.load_func (&quot;http_filter&quot;, BPF.SOCKET_FILTER)<br>  File &quot;/usr/lib/python3/dist-packages/bcc/__init__.py&quot;, line 526, in load_func<br>    raise Exception (&quot;Failed to load BPF program % s: % s&quot; %<br>Exception: Failed to load BPF program b&#x27;http_filter&#x27;: Permission denied<br></code></pre></td></tr></table></figure><p>可以看到程式碼被 verifier 拒絕，並拿到了一個 <code>invalid bpf_context access off=76 size=4</code> 的錯誤，表示存取 <code>__sk_buff-&gt;data</code> 是非法的。</p><p>回去追蹤程式碼的話，會看到在 verifier 裡面會用 <code>env-&gt;ops-&gt;is_valid_access</code> 來檢查該存取是否有效，這同樣定義在 <code>bpf_verifier_ops</code> 結構內。</p><p>其中 socket filter program 的實作是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">static bool sk_filter_is_valid_access (int off, int size,<br>      enum bpf_access_type type,<br>      const struct bpf_prog *prog,<br>      struct bpf_insn_access_aux *info)<br>&#123;<br>switch (off) &#123;<br>case bpf_ctx_range (struct __sk_buff, tc_classid):<br>case bpf_ctx_range (struct __sk_buff, data):<br>case bpf_ctx_range (struct __sk_buff, data_meta):<br>case bpf_ctx_range (struct __sk_buff, data_end):<br>case bpf_ctx_range_till (struct __sk_buff, family, local_port):<br>case bpf_ctx_range (struct __sk_buff, tstamp):<br>case bpf_ctx_range (struct __sk_buff, wire_len):<br>case bpf_ctx_range (struct __sk_buff, hwtstamp):<br>return false;<br>&#125;<br>...<br></code></pre></td></tr></table></figure><p>可以很直接看到拒絕了 data 的存取。</p><p>從 linux kernel 的 <a href="https://github.com/torvalds/linux/commit/db58ba45920255e967cc1d62a430cebd634b5046">變更紀錄</a> 來推測，data 欄位好像本來就不是給 socket filter 使用的，只是單純因為 cls_bpf 和 socker filter 可能共用了這部分的程式碼，因此要額外阻擋這部分的 code 不讓使用。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>最後還有一個沒解決的問題，<code>u8 *cursor = 0;</code>，為甚麼空指標經過 LLVM 編譯後會編譯成對 skb 的存取還是未知的，看起來像是 BCC 特別的機制，但是找不太到相關資料，只好保留這個問題。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://www.kernel.org/doc/html/v5.17/bpf/instruction-set.html">eBPF Instruction Set</a></li><li><a href="https://stackoverflow.com/questions/61702223/bpf-verifier-rejects-code-invalid-bpf-context-access">Stackoverflow: invalid bpf_context access”</a></li><li><a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">bpf-helpers man page</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;這篇文章會介紹 eBPF socket filter 的概念，並使用 bcc 的 &lt;a href=&quot;https://github.com/iovisor/bcc/tree/master/examples/networking/http_filter&quot;&gt;http-parse-simple.py&lt;/a&gt; 作為範例，來說明如何使用 eBPF socket filter 來過濾 HTTP 請求，並且會深入底下 eBPF 的 socket filter 底層部分的實作。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 4 - BCC tcpconnect</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-4-BCC-tcpconnect/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-4-BCC-tcpconnect/</id>
    <published>2022-10-31T08:10:37.000Z</published>
    <updated>2022-10-31T10:33:00.555Z</updated>
    
    <content type="html"><![CDATA[<p>我們要來看 BCC 的 <code>tools/tcpconnect.py</code> 這支程式。原始碼在 <a href="https://github.com/iovisor/bcc/blob/master/tools/tcpconnect.py">這邊</a>。</p><h2 id="tcpconnect-介紹"><a href="#tcpconnect-介紹" class="headerlink" title="tcpconnect 介紹"></a>tcpconnect 介紹</h2><p>這隻程式會追蹤紀錄 kernel 發起的 TCP 連線，可以看到發起連線的 pid, 指令名稱，ip version, IP 地址和目標 port 等資訊。</p><span id="more"></span><p>執行結果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">python3 tools/tcpconnect <br>Tracing connect ... Hit Ctrl-C to end<br>PID     COMM         IP SADDR            DADDR            DPORT <br>2553    ssh          4  10.0.2.15        10.0.2.1         22     <br>2555    wget         4  10.0.2.15        172.217.160.100  80 <br></code></pre></td></tr></table></figure><h2 id="tcpconnect-實作"><a href="#tcpconnect-實作" class="headerlink" title="tcpconnect 實作"></a>tcpconnect 實作</h2><p>首先透過 <code>argparse</code> 定義了指令的參數輸入，主要是提供 filter 的選項，讓使用者可以透過 pid, uid, namespace 等參數去 filter 連線紀錄。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">parser = argparse.ArgumentParser (<br>    description=<span class="hljs-string">&quot;Trace TCP connects&quot;</span>,<br>    formatter_class=argparse.RawDescriptionHelpFormatter,<br>    epilog=examples)<br>parser.add_argument (<span class="hljs-string">&quot;-p&quot;</span>, <span class="hljs-string">&quot;--pid&quot;</span>,<br>    <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;trace this PID only&quot;</span>)<br>...<br>args = parser.parse_args ()<br></code></pre></td></tr></table></figure><p>接著就來到主要的 eBPF 程式碼的定義</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">bpf_text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">#include &lt;uapi/linux/ptrace.h&gt;</span><br><span class="hljs-string">#include &lt;net/sock.h&gt;</span><br><span class="hljs-string">#include &lt;bcc/proto.h&gt;</span><br><span class="hljs-string"></span><br><span class="hljs-string">BPF_HASH (currsock, u32, struct sock *);</span><br><span class="hljs-string">...</span><br></code></pre></td></tr></table></figure><p>首先可以看到 <code>BPF_HASH</code>，這是 BCC 提供的一個巨集，用來定一個 hash type 的 map，對於不同 map type BCC 都定義了對應的巨集來建立 map。具體列表可以參考 <a href="https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#maps">這邊</a>。<br>第一個參數是 map 的名稱，這邊叫做 currsock，同時這個變數也用於後續程式碼中對 map 的參考和 API 呼叫，例如 <code>currsock.lookup (&amp;tid);</code> 就是對 currsock 這個 map 進行 lookup 操作。<br>接著兩個欄位分別對應 key 和 value 的 type，key 是一個 32 位元整數，value 則對應到 sock struct 指標。sock 結構在 <a href="https://elixir.bootlin.com/linux/latest/source/include/net/sock.h#L352">net&#x2F;sock.h</a> 內定義，是 linux kernel 用來維護 socket 的資料結構。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipv4_data_t</span> &#123;</span><br>    u64 ts_us;<br>    u32 pid;<br>    u32 uid;<br>    u32 saddr;<br>    u32 daddr;<br>    u64 ip;<br>    u16 lport;<br>    u16 dport;<br>    <span class="hljs-type">char</span> task [TASK_COMM_LEN];<br>&#125;;<br>BPF_PERF_OUTPUT (ipv4_events);<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipv6_data_t</span> &#123;</span><br>...<br></code></pre></td></tr></table></figure><p>接著分別針對 ipv4 和 ipv6 定義了一個 data_t 的資料結構，用於 bpf 和 userspace client 之間傳輸 tcp connect 的資訊用。</p><p>這邊可以看到另外一個特別的巨集 <code>BPF_PERF_OUTPUT</code>。這邊用到了 eBPF 提供的 perf event 機制，定義了一個 per-CPU 的 event ring buffer，並提供了對應的 bpf_perf_event_output helper function 來把資料推進 ring buffer 給 userspace 存取。<br>在 bcc 這邊則使用 <code>ipv4_events.perf_submit (ctx, &amp;data, sizeof (data));</code> 的 API 來傳輸資料。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//separate flow keys per address family</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipv4_flow_key_t</span> &#123;</span><br>    u32 saddr;<br>    u32 daddr;<br>    u16 dport;<br>&#125;;<br>BPF_HASH (ipv4_count, <span class="hljs-keyword">struct</span> <span class="hljs-type">ipv4_flow_key_t</span>);<br></code></pre></td></tr></table></figure><p>接著又是一個 HASH map，tcpdconnect 提供一個功能選項是統計各種 connection 的次數，所以這邊定義了一個 ipv4_flow_key_t 當作 key 來作為統計依據，<code>BPF_HASH</code> 在預設情況下 value 的 type 是 <code>u64</code>，一個 64 位元無號整數，因此可以直接拿來統計。</p><p>接著就來到了 bpf 函數主體，這個函數會被 attach 到 tcp_v4_connect 和 tcp_v6_connect 的 kprobe 上，當呼叫 tcp_v4_connect 和 tcp_v6_connect 時被觸發。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">trace_connect_entry</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> pt_regs *ctx, <span class="hljs-keyword">struct</span> sock *sk)</span><br>&#123;<br>    <span class="hljs-keyword">if</span> (container_should_be_filtered ()) &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    &#125;<br>    u64 pid_tgid = bpf_get_current_pid_tgid ();<br>    u32 pid = pid_tgid &gt;&gt; <span class="hljs-number">32</span>;<br>    u32 tid = pid_tgid;<br>    FILTER_PID<br>    u32 uid = bpf_get_current_uid_gid ();<br>    FILTER_UID<br>    <span class="hljs-comment">//stash the sock ptr for lookup on return</span><br>    currsock.update (&amp;tid, &amp;sk);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;;<br></code></pre></td></tr></table></figure><p>首先它接收的參數是 pt_regs 結構和 tcp_v4_connect 的參數，pt_regs 包含了 CPU 佔存器的數值資訊，作為 eBPF 的上下文。後面 tcp_v4_connect 的第一個參數 sock 結構對應到當次連線的 socket 資訊，由於後面幾個參數不會使用到所以可以省略掉。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">./tcpconnect --cgroupmap mappath  # only trace cgroups in this BPF map<br>./tcpconnect --mntnsmap mappath   # only trace mount namespaces in the map<br></code></pre></td></tr></table></figure><p>首先呼叫的是 <code>container_should_be_filtered</code>。在 argparser 中定義了兩個參數 cgroupmap 和 mntnsmap 用來針對特定的 cgroups 或 mount namespace。<code>container_should_be_filtered</code> 則會負責這兩項的檢查。</p><p>一開始看可能會發現在 eBPF 程式裡面找不到這個函數定的定義，由於這兩個 filter 非常常用因此 bcc 定義了 <code>bcc.containers.filter_by_containers</code><a href="https://github.com/iovisor/bcc/blob/master/src/python/bcc/containers.py">函數</a>，在 python 程式碼裡面會看到，<code>bpf_text = filter_by_containers (args) + bpf_text</code>。<br>以 cgroup 來說，如果使用者有提供 <code>cgroupmap</code> 這個參數，<code>filter_by_containers</code> 會在 mappath 透過 <code>BPF_TABLE_PINNED</code> 在 BPFFS 建立一個 hash type 的 map，根據這個 map 的 key 來 filter cgroup id，透過 <code>bpf_get_current_cgroup_id ()</code> 取得當前上下文的 cgroup_id 並只保留有在 map 內的上下文。</p><p>接著 <code>FILTER_PID</code> 和 <code>FILTER_UID</code> 分別是針對 pid 和 uid 去 filter，在後面的 python 程式碼中會根據是否有啟用這個選項來把字串替代成對應的程式碼或空字串</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">if args.pid:<br>    bpf_text = bpf_text.replace (&#x27;FILTER_PID&#x27;,<br>        &#x27;if (pid != % s) &#123; return 0; &#125;&#x27; % args.pid)<br>bpf_text = bpf_text.replace (&#x27;FILTER_PID&#x27;, &#x27;&#x27;)<br></code></pre></td></tr></table></figure><p>如果一切都滿足，就會使用 tid 當 key，將 sock 結構更新到 <code>currsock</code> map 當中。</p><p>後半部分的 eBPG 程式碼定義了 <code>trace_connect_return</code>，這個函數會被 attach 到 tcp_v4_connect 和 tcp_v6_connect 的 kretprobe 上。kprobe 是在函數被呼叫時被觸發，kretprobe 則是在函數回傳時被觸發，因此可以取得函數的回傳值和執行結果。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">trace_connect_v4_return</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> pt_regs *ctx)</span><br>&#123;<br>    <span class="hljs-keyword">return</span> trace_connect_return (ctx, <span class="hljs-number">4</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>真正的進入點分成 ip v4 和 v6 的版本來傳入 ipver 變數。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-type">int</span> <span class="hljs-title function_">trace_connect_return</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> pt_regs *ctx, <span class="hljs-type">short</span> ipver)</span><br>&#123;<br>    <span class="hljs-type">int</span> ret = PT_REGS_RC (ctx);<br>    u64 pid_tgid = bpf_get_current_pid_tgid ();<br>    u32 pid = pid_tgid &gt;&gt; <span class="hljs-number">32</span>;<br>    u32 tid = pid_tgid;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sock</span> **<span class="hljs-title">skpp</span>;</span><br>    skpp = currsock.lookup (&amp;tid);<br>    <span class="hljs-keyword">if</span> (skpp == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;   <span class="hljs-comment">//missed entry</span><br>    &#125;<br>    <span class="hljs-keyword">if</span> (ret != <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-comment">//failed to send SYNC packet, may not have populated</span><br>        <span class="hljs-comment">//socket __sk_common.&#123;skc_rcv_saddr, ...&#125;</span><br>        currsock.delete (&amp;tid);<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-comment">//pull in details</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sock</span> *<span class="hljs-title">skp</span> =</span> *skpp;<br>    u16 lport = skp-&gt;__sk_common.skc_num;<br>    u16 dport = skp-&gt;__sk_common.skc_dport;<br>    FILTER_PORT<br>    FILTER_FAMILY<br>    <span class="hljs-title function_">if</span> <span class="hljs-params">(ipver == <span class="hljs-number">4</span>)</span> &#123;<br>        IPV4_CODE<br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-comment">/* 6 */</span> &#123;<br>        IPV6_CODE<br>    &#125;<br>    currsock.delete (&amp;tid);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>透過 <code>PT_REGS_RC</code> 可以取得函數的回傳值，根據函數的定義，如果執行成功應該要回傳 0 所以如果 <code>ret</code> 不為零，表示執行錯誤，直接忽略。<br>透過 <code>currsock.lookup</code> 我們可以取回對應 tid 的 sock 指標，然後取得 dst port 和 src port (lport)，由於這時候 tcp_connect 已經執行完成，所以 src port 已經被 kernel 分配。</p><blockquote><p>這邊可以看到 eBPF 程式設計上比較複雜的地方，sock 結構體要在 kprobe 取得，但是我們又需要 kretprobe 後的一些資訊，因此整個架構要被拆成兩個部分，然後透過 map 來進行傳輸。</p></blockquote><p>接著 <code>FILTER_PORT</code> 和 <code>FILTER_FAMILY</code> 一樣會被替換，然後根據 dst port 和 family 來 filter。</p><p>由於 tcpconnect 有紀錄和統計連線次數兩種模式，因此最後一段的 code 一樣先被標記成 <code>IPV4_CODE</code>。然後根據模式的不同來取代成不同的 code。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.count:<br>    bpf_text = bpf_text.replace (<span class="hljs-string">&quot;IPV4_CODE&quot;</span>, struct_init [<span class="hljs-string">&#x27;ipv4&#x27;</span>][<span class="hljs-string">&#x27;count&#x27;</span>])<br>    bpf_text = bpf_text.replace (<span class="hljs-string">&quot;IPV6_CODE&quot;</span>, struct_init [<span class="hljs-string">&#x27;ipv6&#x27;</span>][<span class="hljs-string">&#x27;count&#x27;</span>])<br><span class="hljs-keyword">else</span>:<br>    bpf_text = bpf_text.replace (<span class="hljs-string">&quot;IPV4_CODE&quot;</span>, struct_init [<span class="hljs-string">&#x27;ipv4&#x27;</span>][<span class="hljs-string">&#x27;trace&#x27;</span>])<br>    bpf_text = bpf_text.replace (<span class="hljs-string">&quot;IPV6_CODE&quot;</span>, struct_init [<span class="hljs-string">&#x27;ipv6&#x27;</span>][<span class="hljs-string">&#x27;trace&#x27;</span>])<br><br></code></pre></td></tr></table></figure><p>我們這邊就只看 ipv4 trace 的版本。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ipv4_data_t</span> <span class="hljs-title">data4</span> =</span> &#123;.pid = pid, .ip = ipver&#125;;<br>data4.uid = bpf_get_current_uid_gid ();<br>data4.ts_us = bpf_ktime_get_ns () / <span class="hljs-number">1000</span>;<br>data4.saddr = skp-&gt;__sk_common.skc_rcv_saddr;<br>data4.daddr = skp-&gt;__sk_common.skc_daddr;<br>data4.lport = lport;<br>data4.dport = ntohs (dport);<br>bpf_get_current_comm (&amp;data4.task, <span class="hljs-keyword">sizeof</span>(data4.task));<br>ipv4_events.perf_submit (ctx, &amp;data4, <span class="hljs-keyword">sizeof</span>(data4));<br></code></pre></td></tr></table></figure><p>這邊其實就是去填充 ipv4_data_t 結構、透過 bpf_get_current_comm 取得當前程式的名稱，最後透過前面透過 BPP_PERF_OUT 定義的 ipv4_events，呼叫 <code>perf_submit (ctx, &amp;data4, sizeof (data4))</code> 將資料送到 user space。</p><p>到這邊就完成了整個的 eBPF 程式碼 <code>bpf_text</code> 的定義，後面就會先經過前面講的，將 IPV4_CODE 等字段，根據 tcpconnect 的參數進行取代。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">b = BPF (text=bpf_text)<br>b.attach_kprobe (event=<span class="hljs-string">&quot;tcp_v4_connect&quot;</span>, fn_name=<span class="hljs-string">&quot;trace_connect_entry&quot;</span>)<br>b.attach_kprobe (event=<span class="hljs-string">&quot;tcp_v6_connect&quot;</span>, fn_name=<span class="hljs-string">&quot;trace_connect_entry&quot;</span>)<br>b.attach_kretprobe (event=<span class="hljs-string">&quot;tcp_v4_connect&quot;</span>, fn_name=<span class="hljs-string">&quot;trace_connect_v4_return&quot;</span>)<br>b.attach_kretprobe (event=<span class="hljs-string">&quot;tcp_v6_connect&quot;</span>, fn_name=<span class="hljs-string">&quot;trace_connect_v6_return&quot;</span>)<br></code></pre></td></tr></table></figure><p>接著透過 BCC 的 library 完成 eBPF 程式碼的編譯、載入和 attach。</p><p>最後是輸出的部分，前面會先輸出一些下列的欄位資訊，但是由於這不是很重要所以就省略掉。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Tracing connect ... Hit Ctrl-C to end<br>PID     COMM         IP SADDR            DADDR            DPORT <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">b = BPF (text=bpf_text)<br>...<br><span class="hljs-comment"># read events</span><br>b [<span class="hljs-string">&quot;ipv4_events&quot;</span>].open_perf_buffer (print_ipv4_event)<br>b [<span class="hljs-string">&quot;ipv6_events&quot;</span>].open_perf_buffer (print_ipv6_event)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br><span class="hljs-keyword">try</span>:<br>b.perf_buffer_poll ()<br><span class="hljs-keyword">except</span> KeyboardInterrupt:<br>exit ()<br></code></pre></td></tr></table></figure><p>完成載入後，我們可以拿到一個對應的 BPF 物件，透過 b[MAP_NAME]，我們可以調用 map 對應的 <code>open_perf_buffer</code>API，透過 <code>open_perf_buffer</code>，我們可以定義一個 callback function 當有資料從 kernel 透過 perf_submit 被傳輸的時候被呼叫來處理 eBPF 程式送過來的資料。</p><p>最後會呼叫 <code>b.perf_buffer_poll</code> 來持續檢查 perf map 是不是有新的 perf event，以及呼叫對應的 callback function。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_ipv4_event</span>(<span class="hljs-params">cpu, data, size</span>):<br>    event = b [<span class="hljs-string">&quot;ipv4_events&quot;</span>].event (data)<br>    <span class="hljs-keyword">global</span> start_ts<br>    <span class="hljs-keyword">if</span> args.timestamp:<br>        <span class="hljs-keyword">if</span> start_ts == <span class="hljs-number">0</span>:<br>            start_ts = event.ts_us<br>        printb (<span class="hljs-string">b&quot;%-9.3f&quot;</span> % ((<span class="hljs-built_in">float</span>(event.ts_us) - start_ts) / <span class="hljs-number">1000000</span>), nl=<span class="hljs-string">&quot;&quot;</span>)<br>    <span class="hljs-keyword">if</span> args.print_uid:<br>        printb (<span class="hljs-string">b&quot;%-6d&quot;</span> % event.uid, nl=<span class="hljs-string">&quot;&quot;</span>)<br>    dest_ip = inet_ntop (AF_INET, pack (<span class="hljs-string">&quot;I&quot;</span>, event.daddr)).encode ()<br>    <span class="hljs-keyword">if</span> args.lport:<br>        printb (<span class="hljs-string">b&quot;%-7d %-12.12s %-2d %-16s %-6d %-16s %-6d % s&quot;</span> % (event.pid,<br>            event.task, event.ip,<br>            inet_ntop (AF_INET, pack (<span class="hljs-string">&quot;I&quot;</span>, event.saddr)).encode (), event.lport,<br>            dest_ip, event.dport, print_dns (dest_ip)))<br>    <span class="hljs-keyword">else</span>:<br>        printb (<span class="hljs-string">b&quot;%-7d %-12.12s %-2d %-16s %-16s %-6d % s&quot;</span> % (event.pid,<br>            event.task, event.ip,<br>            inet_ntop (AF_INET, pack (<span class="hljs-string">&quot;I&quot;</span>, event.saddr)).encode (),<br>            dest_ip, event.dport, print_dns (dest_ip))) x<br></code></pre></td></tr></table></figure><p>透過 <code>b [&quot;ipv4_events&quot;].event</code> 可以直接將 data 數據轉換成 BPF 程式內定義的資料結構，方便存取。取得的資料再經過一些清洗和轉譯就能夠直接輸出了。</p><p>雖然我們跳過了 count 功能還有一個紀錄 dst ip 的 DNS 查詢，但到此我們大致上看完了整個 tcpconnect 的主要的實作內容。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我們要來看 BCC 的 &lt;code&gt;tools/tcpconnect.py&lt;/code&gt; 這支程式。原始碼在 &lt;a href=&quot;https://github.com/iovisor/bcc/blob/master/tools/tcpconnect.py&quot;&gt;這邊&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;tcpconnect-介紹&quot;&gt;&lt;a href=&quot;#tcpconnect-介紹&quot; class=&quot;headerlink&quot; title=&quot;tcpconnect 介紹&quot;&gt;&lt;/a&gt;tcpconnect 介紹&lt;/h2&gt;&lt;p&gt;這隻程式會追蹤紀錄 kernel 發起的 TCP 連線，可以看到發起連線的 pid, 指令名稱，ip version, IP 地址和目標 port 等資訊。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 3 - BCC 介紹</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-3-Introduction-to-BCC/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-3-Introduction-to-BCC/</id>
    <published>2022-10-31T08:00:31.000Z</published>
    <updated>2022-10-31T10:32:34.210Z</updated>
    
    <content type="html"><![CDATA[<p>BPF Compiler Collection (<a href="https://github.com/iovisor/bcc">BCC</a>) 是一套用於 eBPF，用來有效開發 kernel 追蹤修改程式的工具集。</p><span id="more"></span><h3 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h3><p>BCC 我覺得可以看成兩個部分:</p><ul><li>eBPF 的 python 和 lua 的前端，透過 BCC 我們可以使用 python 和 lua 比較簡單的開發 eBPF 的應用程式，BCC 將 bpf system call 還有 eBPC 程式編譯封裝成了 API，並提供一系列預先定義好的巨集和語法來簡化 eBPF 程式。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from bcc import BPF<br>b = BPF (text = &quot;&quot;&quot;<br>#include &lt;uapi/linux/bpf.h&gt;<br>int xdp_prog1 (struct xdp_md *ctx)<br>&#123;<br>    return XDP_DROP;<br>&#125;<br>&quot;&quot;&quot;<br>fn = b.load_func (&quot;xdp_prog1&quot;, BPF.XDP)<br>b.attach_xdp (&quot;eth0&quot;, fn, 0)<br></code></pre></td></tr></table></figure><p>以上面的範例來說，透過 BPF 物件實立化時會完成 eBPC bytecode 的編譯，然後透過 load_func 和 attach_xdp 就可以很簡單的將上面的 eBPF 程式碼編譯載入到 kernel 然後 attach 到 xdp 的 hook point 上。</p><ul><li>一系列使用自身框架開發的工具<ul><li>BCC 使用自己的 API 開發了一系列可以直接使用的現成 bcc eBPF 程式，本身就幾乎涵蓋了 eBPF 的所有 program type，可以開箱即用，直接跳過 eBPF 的開發。</li><li>下圖包含了 BCC 對 linux kernel 各個模組實現的工具名稱<br>  <img src="/eBPF/Learn-eBPF-Serial-1-Abstract-and-Background/bcc_tracing_tools.png" alt="bcc tracing tools"></li></ul></li><li>eBPC 本身和 bcc 相關的開發文件以及範例程式<ul><li>可以看到前面很多天有參考到 BCC 的文件，資料非常地豐富</li></ul></li></ul><h3 id="安裝"><a href="#安裝" class="headerlink" title="安裝"></a>安裝</h3><p>首先 bcc 的安裝大概有幾種方式</p><ul><li>透過各大發行板的套件管理工具安裝</li><li>直接使用原始碼編譯安裝</li><li>透過 docker image 執行<br>對於前兩著，bcc 官方的文件列舉了需多發行版的 <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">安裝方式</a>，所以可以很容易地照著官方文件安裝。以 ubuntu 來說，可以透過 Universe 或 iovisor 的 repo 安裝。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># use Universe<br># add-apt-repository universe <br><br># iovisor<br>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD<br>echo &quot;deb [trusted=yes] https://repo.iovisor.org/apt/xenial xenial-nightly main&quot; | sudo tee /etc/apt/sources.list.d/iovisor.list<br><br>sudo apt-get update<br>sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)<br></code></pre></td></tr></table></figure>然而必須要注意的是，目前 iovisor 和 universe 上面的 bcc 套件本的都比較陳舊，甚至沒有 20.04 和 22.04 對應的安裝源，因此透過 apt 安裝可能會出現版本不支援或安裝後連範例都跑不起來的問題。</li></ul><p>因此特別建議透過原始碼來安裝會是比較穩妥的方式。一樣在 bcc 的的 <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">安裝文檔</a> 詳細列舉了在各個發行版本的各個版本下，要怎麼去安裝相依套件，然後編譯安裝 bcc。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">sudo apt install -y bison build-essential cmake flex git libedit-dev \<br>  libllvm12 llvm-12-dev libclang-12-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils<br><br>git clone https://github.com/iovisor/bcc.git<br>mkdir bcc/build; cd bcc/build<br>cmake ..<br>make<br>sudo make install<br>cmake -DPYTHON_CMD=python3 .. # build python3 binding<br>pushd src/python/<br>make<br>sudo make install<br>popd<br></code></pre></td></tr></table></figure><p>這邊同樣以 ubuntu 舉例，首先因為 BCC 後端還是使用 LLVM，因此需要先安裝 llvm 以及 bcc 編譯需要的 cmake 等工具，然後後過 cmake 編譯安裝。</p><p>安裝完成後，昨天提到的 bcc 自己寫好的 kernel trace tools 會被安裝到 <code>/usr/share/bcc/tools</code>，因此可以直接 cd 到該目錄來玩，由於這些 tools 其實就是 python script，所以其實也可以直接透過 python3 執行 bcc repo 下 tools 目錄內的 python 檔，其結果其實是一樣的。</p><p>同樣的還有 examples 這個資料夾下的範例也會被安裝到 <code>/usr/share/bcc/examples</code> 目錄下。</p><p>最後是透過 docker 的方式執行 bcc。同樣參考 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/QUICKSTART.md">quickstart</a> 文件，不過加上 <code>--pid=host</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">docker run -it --rm \<br>  --pid=host \<br>  --privileged \<br>  -v /lib/modules:/lib/modules:ro \<br>  -v /usr/src:/usr/src:ro \<br>  -v /etc/localtime:/etc/localtime:ro \<br>  --workdir /usr/share/bcc/tools \<br>  zlim/bcc<br></code></pre></td></tr></table></figure><p>但是不論是直接使用 <code>zlim/bcc</code> 還是透過 bcc repo 內的 dockerfile 自行編譯，目前測試起來還是有許多問題，使用 zlim&#x2F;bcc 在執行部分的 eBPF 程式時會編譯失敗，直接透過 dockerfile 編譯初步測試也沒辦法 build 成功，因此目前自行編譯使用可能還是相對比較穩定簡單快速的方式。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;BPF Compiler Collection (&lt;a href=&quot;https://github.com/iovisor/bcc&quot;&gt;BCC&lt;/a&gt;) 是一套用於 eBPF，用來有效開發 kernel 追蹤修改程式的工具集。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
    <category term="bcc" scheme="https://blog.louisif.me/tags/bcc/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 2 - 基本概念</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-2-Basic-concept/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-2-Basic-concept/</id>
    <published>2022-10-31T06:14:28.000Z</published>
    <updated>2022-10-31T10:32:24.307Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章將會介紹包含 program type, map 等重要概念還有 eBPF 的載入流程</p><span id="more"></span><h2 id="Program-Type"><a href="#Program-Type" class="headerlink" title="Program Type"></a>Program Type</h2><p>我們可以把 eBPF 程式區分成不同的 BPF program type，不同的 program type 代表實現不同功能的 eBPF 程式。通常不同的 program type 也會有不同 hook point，eBPF 程式的輸入和輸出格式也不同，也就影響到不同的 kernal 組件。</p><p>到目前 Linux kernal 5.19 版，linux 總共定義了 32 種的 program type。在 linux kernal source code 的 <a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/bpf.h">include&#x2F;uapi&#x2F;linux&#x2F;bpf.h</a> 中定義了 <code>bpf_prog_type</code> 列舉，列舉了所有的 program type。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">enum bpf_prog_type &#123;<br>BPF_PROG_TYPE_UNSPEC,<br>BPF_PROG_TYPE_SOCKET_FILTER,<br>BPF_PROG_TYPE_KPROBE,<br>BPF_PROG_TYPE_SCHED_CLS,<br>BPF_PROG_TYPE_SCHED_ACT,<br>BPF_PROG_TYPE_TRACEPOINT,<br>BPF_PROG_TYPE_XDP,<br>BPF_PROG_TYPE_PERF_EVENT,<br>BPF_PROG_TYPE_CGROUP_SKB,<br>BPF_PROG_TYPE_CGROUP_SOCK,<br>...<br>&#125;;<br></code></pre></td></tr></table></figure><p>以 <code>BPF_PROG_TYPE_XDP</code> 為例，XDP 是 <code>Express Data Path</code> 的縮寫，XDP 程式會在封包從網路卡進入到 kernal 的最早期被觸發。</p><p>一個 eBPF 程式大致上可以看成一個 c 的 function，在 XDP program type 下，kernal 會帶入 xdp_md 資料結構作為 eBPF 程式的輸入，包含了封包的內容、封包的來源介面等資訊。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">//include/uapi/linux/bpf.h<br><br>/* user accessible metadata for XDP packet hook */<br>struct xdp_md &#123;<br>    __u32 data;<br>    __u32 data_end;<br>    __u32 data_meta;<br><br>    /* Below access go through struct xdp_rxq_info */<br>    __u32 ingress_ifindex; /* rxq-&gt;dev-&gt;ifindex */<br>    __u32 rx_queue_index;  /* rxq-&gt;queue_index  */<br>    __u32 egress_ifindex;  /* txq-&gt;dev-&gt;ifindex */<br>&#125;;<br></code></pre></td></tr></table></figure><p>eBPF 程式必須回傳一個 <code>xdp_action</code> 的 enum，其中 <code>XDP_PASS</code> 表示封包可以繼續通過到 kernal network stack，<code>XDP_DROP</code> 表示直接丟棄該封包。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//include/uapi/linux/bpf.h</span><br><br><span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">xdp_action</span> &#123;</span><br>    XDP_ABORTED = <span class="hljs-number">0</span>,<br>    XDP_DROP,<br>    XDP_PASS,<br>    XDP_TX,<br>    XDP_REDIRECT,<br>&#125;;<br></code></pre></td></tr></table></figure><p>透過這樣的 eBPF 程式，我們就可以在封包剛進入 kernal 的時候直接丟棄非法封包，能夠比較高效的處理 DDos 攻擊等問題。</p><p>以此可以寫出一個極簡單的 eBPF 程式範例 (只包含最主要的部份，完整的程式寫法會在後面提到)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">int xdp_prog_simple (struct xdp_md *ctx)<br>&#123;<br>    return XDP_DROP;<br>&#125;<br></code></pre></td></tr></table></figure><p>這個 eBPF 程式可以被 attach 到某一個 interface 上，當封包進來時會被呼叫。由於無條件回傳 XDP_DROP，因此會丟棄所有的封包。</p><h2 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h2><p>eBPF 程式碼要被編譯成 eBPF 虛擬機的 bytecode 才能夠執行。<br>以 XDP 為例，最底層的做法是直接使用 LLVM 編譯這段 eBPF 程式碼。<br>首先需要補齊使用 LLVM 編譯時，需要的 header file 和資訊。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;uapi/linux/bpf.h&gt;</span></span><br><br>SEC (<span class="hljs-string">&quot;xdp_prog&quot;</span>)<br><span class="hljs-type">int</span>  <span class="hljs-title function_">xdp_program</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> xdp_md *ctx)</span><br>&#123;<br><span class="hljs-keyword">return</span> XDP_DROP;<br>&#125;<br><br><span class="hljs-type">char</span> _license [] SEC (<span class="hljs-string">&quot;license&quot;</span>) = <span class="hljs-string">&quot;GPL&quot;</span>;<br></code></pre></td></tr></table></figure><p>接著使用 LLVM 編譯成 ELF 格式文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">clang -c -target bpf xdp.c -o xdp.o<br></code></pre></td></tr></table></figure><p>然後使用 <code>bpf</code> system call 將 bytecode 載入到 kernal 的 eBPF 虛擬機內，並取得對應的 file descriptor。最後透過 netlink socket 發送一個 <code>NLA_F_NESTED | 43</code> 訊息來把 interface index 與 ebpf 程式的 file descriptor 綁定。就能夠讓 eBPF 程式在對應的 interface 封包處理過程中被呼叫。</p><p>iproute2 有實作載入 eBPF 的功能，因此可以透過下指令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ip link set dev eth1 xdp xdp.o<br></code></pre></td></tr></table></figure><p>可能會注意在程式碼的最後一行。特別標註了 GPL licence。由於 eBPF 程式會嵌入到 kernel，與 kernel 緊密的一起執行 (共用 address space、權限等)，在法律判斷獨立程式的邊界時，eBPF 程式和相關的 kernel 組件會被視為一體，因此 eBPF 程式會受到相關的 licence 限制。</p><p>而這邊提到的內核組件指的是 eBPF helper function。helper function 是 eBPF 程式與 kernel 溝通的橋梁，由於 eBPF 程式是在 eBPF 虛擬機內執行，因此如果要取得 kernel 的額外資訊或改變 kernel 的行為，必須透過虛擬機提供的 helper function 接口。</p><p>一部份的 helper function 基於 GPL 授權，因此當 eBPF 程式使用了 GPL 授權的 helper function 就必須標示為 GPL 授權，否則將 eBPF 程式載入到 kernel 時，會直接被 kernel 拒絕。</p><p>直接使用最底層的方法開發相對來說是不方便和困難的，不同 program type 的載入方式可能還完全不一樣，因此許多抽象的框架和 SDK 被發出來。雖然還是需要編寫 eBPF 的 c code，但是編譯、載入、溝通等工作被包在 SDK 裡面，可以方便的直接使用。</p><p>這邊舉例 BPF Compiler Collection (BCC) 這套工具，BCC 將 eBPF 的編譯和載入動作包裝成了 python 的 API，因此能夠簡單的完成 eBPF 的編譯和執行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> bcc <span class="hljs-keyword">import</span> BPF<br><span class="hljs-keyword">import</span> time<br><br>b = BPF (text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">#include &lt;uapi/linux/bpf.h&gt;</span><br><span class="hljs-string">int xdp_prog1 (struct xdp_md *ctx)</span><br><span class="hljs-string">&#123;</span><br><span class="hljs-string">    return XDP_DROP;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span>)<br>fn = b.load_func (<span class="hljs-string">&quot;xdp_prog1&quot;</span>, BPF.XDP)<br>b.attach_xdp (<span class="hljs-string">&quot;wlp2s0&quot;</span>, fn, <span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">try</span>:<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>time.sleep (<span class="hljs-number">1</span>)<br><span class="hljs-keyword">except</span> KeyboardInterrupt:<br><span class="hljs-keyword">pass</span><br><br>b.remove_xdp (<span class="hljs-string">&quot;wlp2s0&quot;</span>, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="使用條件"><a href="#使用條件" class="headerlink" title="使用條件"></a>使用條件</h2><p>在開始玩 eBPF 之前，我們要先確定一下我們的環境能夠使用 eBPF。最早在 kernal 3.15 版加入了 eBPF 功能。後續在 3.15 到現在的 5.19 版間，eBPF 陸陸續續加入了許多新的功能，因此開發的時候，如果不是使用最新版的作業系統，就可能會需要確認一下版本是否支援，各個功能支援的版本可以在 <a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md">這邊</a> 參考</p><p>另外就是 eBPF 的功能需要在編譯 kernel 的時候啟用，大部分的發行版應該都直接啟用了，不過如果使用時出現問題可能還是到 <code>/proc/config.gz</code> 或 <code>/boot/config-&lt;kernel-version&gt;</code> 檢查內核編譯的設定，是否有開啟 <code>CONFIG_BPF</code>, <code>CONFIG_BPF_SYSCALL</code>, <code>CONFIG_BPF_JIT</code> 還有其他 BPF 相關 Kernal 選項。<br>設定可以參考 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md#kernel-configuration">安裝需求</a></p><h2 id="載入流程"><a href="#載入流程" class="headerlink" title="載入流程"></a>載入流程</h2><p><img src="https://ebpf.io/static/loader-dff8db7daed55496f43076808c62be8f.png"><br>當 eBPF 程式編譯完成後，就需要透過 <code>bpf</code> system call (<a href="https://elixir.bootlin.com/linux/v5.13/source/kernel/bpf/syscall.c#L4369">原始碼</a>)，將編譯後的 bytecode 載入 kernel 內執行。</p><p>為了安全性考量，要掛載 eBPF 程式需要 root 權限或 <code>CAP_BPF</code> capability，不過目前也有在設計讓非 root 權限帳號能載入 eBPF 程式，因此將 <code>kernel.unprivileged_bpf_disabled</code> sysctl 設置為 false 的情況下，非 root 帳號是有能力能夠使用 <code>BPF_PROG_TYPE_SOCKET_FILTER</code> 的 eBPF 程式。</p><p>eBPF 程式需要嵌入到 kernel 執行，因此 eBPF 程式的安全性是極為重要的，也要避免 eBPF 程式的錯誤有可能會導致 kernel 崩潰或卡死，因此每個載入 kernel 的 eBPF 程式都要先經過接著 verifier 檢查。</p><p>首先 eBPF 程式必須要在有限的時間內執行完成，不然就會造成 kernel 卡死，因此在早期的版本中 verifier 是拒絕任何 loop 的存在的，整個程式碼必須是一張 DAG (有向無環圖)。不過在 kernel 5.3 版本開始，verifier 允許了有限次數的循環，verifier 會透過模擬執行檢查 eBPF 是不是會在有限次數內在所有可能的分支上走到 <code>bpf_exit</code>。</p><p>接著 eBPF 程式的大小也存在限制，早期只一個 eBPF 程式只允許 4096 個 ebpf vm 的 instructionｓ，在設計比較複雜的 eBPF 程式上有些捉襟見肘，因此後來在 <a href="https://github.com/torvalds/linux/commit/c04c0d2b968ac45d6ef020316808ef6c82325a82">5.2 版</a> 這個限制被放寬成 1 million 個指令，基本上是十分夠用了，也還是能確保 ebpf 程式在 1&#x2F;10 秒內執行完成。</p><p>然後程式的 stack 也存在大小限制，目前限制是 512。</p><p>當然 verifier 檢查的項目不只如此，前面提到 non-GPL licence eBPF 程式使用 GPL licence 的 helper function，也會被 verifier 拒絕，並收到一個 <code>cannot call GPL-restricted function from non-GPL compatible program</code> 的錯誤。</p><p>此外 verifier 也會針對 helper function 的函數呼叫參數合法性，暫存器數值合法性，或其他無效的使用方式、無效的回傳數值、特定必須的資料結構是否定義、是否非法存取修改數據、無效的 instruction 參數等等做出檢查以及拒絕存在無法執行到的程式碼。</p><p>具體的 verifier 可以參考 1 萬 5 千行的 <a href="https://github.com/torvalds/linux/blob/master/kernel/bpf/verifier.c">原始碼</a> </p><h2 id="JIT"><a href="#JIT" class="headerlink" title="JIT"></a>JIT</h2><p>當 eBPF 程式通過 verifier 的驗證之後會進行 JIT (Just In Time) 的二次編譯。之前一直提到 eBPF 是一個執行在 kernel 內的虛擬機，因此編譯出來的 bytecode 需要再執行的過程中在轉換成 machine code，才能夠真正在 CPU 上面執行，然而這樣的虛擬化和轉換過程，會造成 eBPF 程式的執行效率，比直接執行 machine code 要低上很多。</p><p>因此 eBPF 加入了 JIT 的功能，簡單來說就是把 eBPF 的 bytecode 預先在載入的時候，直接編譯成 CPU 可執行的 machine code，在執行 eBPF 程式的時候就可以直接執行，而不用再經過 eBPF 虛擬機的轉換，使 eBPF 可以達到原生程式的執行效率。</p><p>由於 JIT 需要編譯出 machine code，因此針對不同的 CPU 平台他的支援是分開的，不過當然到了現在，基本上大部分主流的 CPU 架構 (x86, ARM, RISC, MIPS…) 都已經支援了，具體的支援情況可以參考這張 <a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#jit-compiling">表</a>。</p><p>同樣 JIT 是 eBPF 的一個可開關的獨立功能，透過設置 bpf_jit_enable 來啟用 JIT 的功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">systcl -w net.core.bpf_jit_enable=1<br></code></pre></td></tr></table></figure><p>(設置為 2 的話，可以在 kernel log 看到相關日誌)</p><p>到此 eBPF 程式就就完成載入了，雖然在 eBPF 程式的載入過程中還會完成一些資料結構的建立和維護，但是這個部分就不再本文的範圍內了。</p><p>當然到此 eBPF 程式只是載入到了內核之中，並未連接到任何的 hook point，因此到此為 eBPF 程式還未能真正被執行，不過這就是後面的故事了。</p><blockquote><p>從 kernel source code 來看，在 eBPF 程式載入的過程中會呼叫 <a href="https://elixir.bootlin.com/linux/v5.13/source/kernel/bpf/core.c#L1840">bpf_prog_select_runtime</a> 來判斷是否要呼叫 JIT compiler 去編譯，有興趣可以去 trace 這部分的 code。</p></blockquote><h2 id="生命週期"><a href="#生命週期" class="headerlink" title="生命週期"></a>生命週期</h2><p>在透過 <code>bpf (BPF_PROG_LOAD, ...)</code> system call 將 eBPF 程式載入內核的過程 (可以參考 <a href="https://elixir.bootlin.com/linux/v5.13/source/kernel/bpf/syscall.c#L2079">原始碼</a>)，會替該 eBPF 程式建立 <code>struct bpf_prog</code> <a href="https://elixir.bootlin.com/linux/v5.13/source/include/linux/filter.h#L550">結構</a>，其中 <code>prog-&gt;aux-&gt;refcnt</code> 計數器記錄了該 eBPF 程式的參考數量，載入的時候會透過 <code>atomic64_set (&amp;prog-&gt;aux-&gt;refcnt, 1);</code> 將 refcnt 設置為一，並返為對應的 file descriptor。</p><p>當 refcnt 降為 0 的時候，就會觸發 unload，將 eBPF 程式資源給釋放掉。(<a href="https://elixir.bootlin.com/linux/v5.13/source/kernel/bpf/syscall.c#L1714">原始碼</a>)<br>因此如果呼叫 <code>BPF_PROG_LOAD</code> 的程式沒有進一步操作，並直接結束的話，當 file descriptor 被 release，就會觸發 refcnt–，而變成 0 並移除 eBPF 程式。</p><p>要增加 eBPF 程式 refcnf 大致上有幾種方式</p><ul><li>透過 bpf systemcall 的 BPF_BTF_GET_FD_BY_ID 等方式取得 eBPF 程式對應的 file descriptor</li><li>將 eBPF 程式 attach 到事件、Link 上，使 eBPF 程式能真的開始工作。<ul><li>因此當 eBPF 被 attach 到 hook points 上之後，即便原始載入程式結束也不會導致 eBPF 程式被回收，而可以正常繼續工作。</li><li>Link 是 eBPF 後來提供的新特性，因此暫時超出了本文的討論範圍</li></ul></li><li>透過 bpf systemcall 的 BPF_OBJ_PIN，將 eBPF 程式釘到 BPFFS 上。<ul><li>BPFFS 是 BPF file system，本質上是一個虛擬的檔案系統，一樣透過 bpf system call 的 BPF_OBJ_PIN，我們可以把 eBPF 程式放到 <code>/sys/fs/bpf/</code> 路徑下的指定位置，並透過 <code>open</code> 的方式直接取得 file descriptor。PIN 同樣會增加 refcnt，因此 PIN 住的程式不會被回收</li><li>要釋放 PIN 住的程式，可以使用 unlink 指令移除虛擬檔案，即可取消 PIN。</li></ul></li></ul><p>透過以上的操作都會增加 refcnt，相反的，對應的資源釋放則會減少 refcnt。因此只要確保有任何一個 eBPF 程式的參考存在，即可保證 eBPF 程式一直存在 kernel 內。</p><h2 id="Helper-Funtions"><a href="#Helper-Funtions" class="headerlink" title="Helper Funtions"></a>Helper Funtions</h2><p>之前在介紹 eBPF 的 GPL 授權的時候，有提到 eBPF helper function 這個東西，接下來我們來比較仔細的介紹一下。</p><p>之前提到 eBPF 程式是在 eBPF 虛擬機內執行，由於 eBPF 程式會嵌入 kernel 內，在 kernel space 執行，所以為了安全性考量我們不能讓 eBPF 程式任意的存取和修改 kernel 記憶體和呼叫 kernel 函數，因此 eBPF 的解決方案是提供了一系列的 API，讓 eBPF 程式只能夠過限定的 API 去與 kernel 溝通，因此可以讓 eBPF 程式對 kernel 的操作限制在一個可控的範圍，也可以透過 verifier 和 API 後面的實作去確保 API 呼叫的有效和安全性。</p><p>在 eBPF 裡這一系列的 API 就稱之為 eBPF helper funtions。</p><p>另外不同的對於 eBPF program type 的 eBPF 程式，由於他們執行的時機點和在 kernel 的位置不同，因此他們能夠取得的 kernel 資訊也就不同，他們可以呼叫執行的 helper funtions 也就不同。具體每個不同 program type 可以執行的 helper function 可以參考 bcc 的 <a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#program-types">文件</a></p><p>下面列幾舉個所有 program type 都可以呼叫的 helper function</p><ul><li>u64 bpf_ktime_get_ns (void)<ul><li>取得從開機開始到現在的時間，單位是奈秒</li></ul></li><li>u32 bpf_get_prandom_u32 (void)<ul><li>取得一個 random number</li></ul></li></ul><p>接著我們舉例在 BPF_PROG_TYPE_SOCKET_FILTER 下才能使用的 helper function</p><ul><li>long bpf_skb_load_bytes (const void *skb, u32 offset, void *to, u32 len)<ul><li>由於 socket filter 的功能就是對 socket 的流量做過濾，因此我們可以透過 skb_load_bytes 來取得 socket 傳輸的封包內容</li></ul></li></ul><p>完整的 helper function 列表還有每個函數具體的定義以及使用說明描述可以在 <a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/bpf.h">bpf.h</a> 查找到。</p><p>另外特別要注意的是受限於 eBPF 虛擬機的限制，eBPF helper function 的參數數量最多只可以有五個，在使用不定參數長度的參數時，最多也只能有 5 個參數 (如之後會提到的 trace_printk)。</p><p>因此雖然 eBPF 非常強大能夠非常方便的動態對 kernel 做修改，但為了安全，他可以執行的操作是訂定在一個非常嚴格的框架上的，在開發時需要熟習整個框架的限制和可利用的 API 資源。</p><h2 id="Debug-Tracing"><a href="#Debug-Tracing" class="headerlink" title="Debug Tracing"></a>Debug Tracing</h2><p>在將 eBPF 程式載入 kernel 工作後，我們勢必需要一些手段來與 eBPF 程式做溝通，一方面我們需要輸出偵錯訊息，來對 eBPF 程式 debug，一方面我們可能會希望能夠實時透過 eBPF 程式取得 kernel 的某些資訊又或著動態調整 eBPF 程式的執行規則。</p><p>如果只是需要 eBPF 程式單方面的輸出訊息，讓我們可以偵錯，可以使用比較簡單的手段。eBPF 有提供一個 helper function <code>long bpf_trace_printk (const char *fmt, u32 fmt_size, ...)</code>，可以輸入一個格式化字串 <code>fmt</code>，及最多三個變數 (參數個數的限制)。輸出結果會被輸出到 <code>/sys/kernel/debug/tracing/trace_pipe</code> 中。.</p><p>可以透過指令查看輸出結果:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">sudo cat /sys/kernel/debug/tracing/trace_pipe<br></code></pre></td></tr></table></figure><p>輸出的格式如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">telnet-470   [001] .N.. 419421.045894: 0x00000001: &lt;formatted msg&gt;<br></code></pre></td></tr></table></figure><ul><li>首先是 process name <code>telnet</code>，然後是 PID 470。 </li><li>接續的 001 是指當前執行的 CPU 編號。</li><li>.N.. 的每個字元對應到一個參數<ul><li>irqs 中斷是否啟用</li><li><code>TIF_NEED_RESCHED</code> 和 <code>PREEMPT_NEED_RESCHED</code> 是否設置 (用於 kernel process scheduling)</li><li>硬中断 &#x2F; 軟中断是否發生中</li><li>level of preempt_disabled</li></ul></li><li>419421.045894 時間</li><li>0x00000001: eBPF 內部的指令暫存器數值</li></ul><p>雖然 trace_printk 可以接收格式化字串，但是支援的格式字元比較少，只支援 <code>% d, % i, % u, % x, % ld, % li, % lu, % lx, % lld, % lli, % llu, % llx, % p, % s</code>。</p><p>另外有一個 bpf_printk 巨集，會使用 sizeof (fmt) 幫忙填上第二個 fmt_size。因此使用 bpf_printk 可以省略 fmt_size。</p><p>在比較新的版本提供了 bpf_snprintf 和 bpf_seq_printf 兩個新的 print 函數，前者是把資料寫入預先建立好的 buffer 內，後者可以寫入在特定 program type 下可以取得的 seg_file，兩者皆用陣列存放後面的參數列，因此可以打破 helper funtion 5 個參數的限制。</p><p>最後要特別注意的是使用 trace_printk 會大幅拖慢 eBPF 程式的執行效率，所以 trace_printk 只適用於開發時用來 debug 使用，不適用於正式環境當中。</p><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p>接著介紹 eBPF 的另外一個重要組件 <code>map</code>，前面提到 trace_printk 只適合用在除錯階段，輸出 eBPF 的執行資訊到 user space，然而我們需要一個可以在正式環境內，提供 user space 程式和 eBPF 程式之間雙向數據交換的能力，另外每次觸發 eBPF 程式都可看作獨立執行 eBPF 程式，所以也需要在多次呼叫 eBPF 程式時共享資料的功能。因此 eBPF 程式引入了 <code>map</code>。</p><p>eBPF map 定義了一系列不同的不同的資料結構類型，包含了 hash, array, LRU hash, ring buffer, queue 等等，另外也提供 per-cpu hash, per-cpu array 等資料結構，由於每顆 CPU 可以獲得獨立的 map，因此可以減少 lock 的需求，提高執行效能。所有的 map type 一樣可以參考 <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/uapi/linux/bpf.h#n880">bpf.h</a> 的 <code>enum bpf_map_type</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">struct bpf_map_def SEC (&quot;maps&quot;) map = &#123;  <br>.type = BPF_MAP_TYPE_ARRAY,  <br>.key_size = sizeof (int),  <br>.value_size = sizeof (__u32),  <br>.max_entries = 4096,  <br>&#125;;<br></code></pre></td></tr></table></figure><p>首先要先在 eBPF 程式內定義 map 的資料結構，在 eBPF 程式內定義一個 map 時，基本需要定義四個東西分別是該資料結構的 map type, key 和 value 的大小以及資料結構內最多有含多少 entry，如果超出 max_entries 上限則會發生錯誤回傳 (-E2BIG)。</p><p>eBPF 提供了 <code>bpf_map_lookup_elem</code>, <code>bpf_map_update_elem</code>, <code>bpf_map_delete_elem</code> 等 helper functions 來對 map 資料做操作。lookup 的完整定義是 <code>void *bpf_map_lookup_elem (struct bpf_map *map, const void *key)</code>，透過 key 去尋找 map 裡面對應的 value，並返回其指標，由於返回的是指標，所以會指向 map 真實儲存的記憶體，可以直接對其值進行更新。</p><p>當然除了幾個基本的 helper function 外，不同的 map type 可能會支援更多的操作或功能，例如 bpf_skb_under_cgroup 是給 BPF_MAP_TYPE_CGROUP_ARRAY 專用的。</p><h3 id="原始碼解析"><a href="#原始碼解析" class="headerlink" title="原始碼解析"></a>原始碼解析</h3><p>linux kernel 定義了 <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/bpf.h#L64">struct bpf_map_ops</a>，來描述 map 可能會支援的所有功能。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_map_ops</span> &#123;</span><br><span class="hljs-comment">/* funcs callable from userspace (via syscall) */</span><br><span class="hljs-type">int</span> (*map_alloc_check)(<span class="hljs-keyword">union</span> bpf_attr *attr);<br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_map</span> *(*<span class="hljs-title">map_alloc</span>)(<span class="hljs-title">union</span> <span class="hljs-title">bpf_attr</span> *<span class="hljs-title">attr</span>);</span><br><span class="hljs-type">void</span> (*map_release)(<span class="hljs-keyword">struct</span> bpf_map *<span class="hljs-built_in">map</span>, <span class="hljs-keyword">struct</span> file *map_file);<br><span class="hljs-type">void</span> (*map_free)(<span class="hljs-keyword">struct</span> bpf_map *<span class="hljs-built_in">map</span>);<br><span class="hljs-type">int</span> (*map_get_next_key)(<span class="hljs-keyword">struct</span> bpf_map *<span class="hljs-built_in">map</span>, <span class="hljs-type">void</span> *key, <span class="hljs-type">void</span> *next_key);<br><span class="hljs-type">void</span> (*map_release_uref)(<span class="hljs-keyword">struct</span> bpf_map *<span class="hljs-built_in">map</span>);<br><span class="hljs-type">void</span> *(*map_lookup_elem_sys_only)(<span class="hljs-keyword">struct</span> bpf_map *<span class="hljs-built_in">map</span>, <span class="hljs-type">void</span> *key);<br>...<br>&#125;<br></code></pre></td></tr></table></figure><p>不同的 map 再根據需要去實作對應的操作，在 <a href="https://github.com/torvalds/linux/blob/master/include/linux/bpf_types.h">include&#x2F;linux&#x2F;bpf_types.h</a> 定義。以 <code>BPF_MAP_TYPE_QUEUE</code> 這個 map type 來說對應到 queue_map_ops。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//kernel/bpf/queue_stack_maps.c</span><br><br><span class="hljs-type">const</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_map_ops</span> <span class="hljs-title">queue_map_ops</span> =</span> &#123;<br>.map_meta_equal = bpf_map_meta_equal,<br>.map_alloc_check = queue_stack_map_alloc_check,<br>.map_alloc = queue_stack_map_alloc,<br>.map_free = queue_stack_map_free,<br>.map_lookup_elem = queue_stack_map_lookup_elem,<br>.map_update_elem = queue_stack_map_update_elem,<br>.map_delete_elem = queue_stack_map_delete_elem,<br>.map_push_elem = queue_stack_map_push_elem,<br>.map_pop_elem = queue_map_pop_elem,<br>.map_peek_elem = queue_map_peek_elem,<br>.map_get_next_key = queue_stack_map_get_next_key,<br>.map_btf_name = <span class="hljs-string">&quot;bpf_queue_stack&quot;</span>,<br>.map_btf_id = &amp;queue_map_btf_id,<br>&#125;;<br></code></pre></td></tr></table></figure><p>當呼叫 bpf_map_push_elem 時，就會呼叫 bpf_map_ops.map_push_elem 來調用 queue 的 queue_stack_map_push_elem 完成。</p><p>而具體每個 map 支援什麼 help function 可能就要參考 <a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">helper function 文件描述</a></p><h3 id="使用範例"><a href="#使用範例" class="headerlink" title="使用範例"></a>使用範例</h3><p>這邊我們一個特別的使用實例來看</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">elem</span> &#123;</span><br>    <span class="hljs-type">int</span> cnt;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">bpf_spin_lock</span> <span class="hljs-title">lock</span>;</span><br>&#125;;<br><br><span class="hljs-keyword">struct</span> bpf_map_def <span class="hljs-title function_">SEC</span><span class="hljs-params">(<span class="hljs-string">&quot;maps&quot;</span>)</span> counter = &#123;  <br>.type = BPF_MAP_TYPE_ARRAY,  <br>.key_size = <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">int</span>),  <br>.value_size = <span class="hljs-keyword">sizeof</span>(elem),  <br>.max_entries = <span class="hljs-number">1</span>,  <br>&#125;;<br></code></pre></td></tr></table></figure><p>首先我們定義了一個特別的 ARRAY map，它的 array size 只有 1，然後 value 是一個包含 u32 整數和一個 lock 的資料結構。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c">SEC (<span class="hljs-string">&quot;kprobe/sys_clone&quot;</span>)<br><span class="hljs-type">int</span> <span class="hljs-title function_">hello_world</span><span class="hljs-params">(<span class="hljs-type">void</span> *ctx)</span> &#123;<br>  u32 key = <span class="hljs-number">0</span>;<br>  elem *val;<br>  val = bpf_map_lookup_elem (&amp;counter, &amp;key);<br>  <br>  bpf_spin_lock (&amp;val-&gt;lock);<br>  val-&gt;cnt++;<br>  bpf_spin_unlock (&amp;val-&gt;lock);<br><br>  bpf_trace_printk (<span class="hljs-string">&quot;sys_clone count: % d&quot;</span>, val-&gt;cnt);<br>  <br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>由於 key 我們固定是 0，透過 bpf_map_lookup_elem 我們永遠會取得同一筆資料，因此可以簡單看成我們把 <code>counter</code> 當作一個單一的容器來存放 cnt 變數，並使用 lock 避免 cnt 更新時的 race condition。</p><p>我們將這個程式附加到 kprobe&#x2F;sys_clone，就可以用來統計 sys_clone 呼叫的次數。</p><p>和其他 eBPF 的操作一樣，我們透過 <code>bpf</code> 的 system call 去與 kernel 進行溝通。跟 helper fuction 類似，bpf systemcall 提供了 <code>BPF_MAP_LOOKUP_ELEM</code>, <code>BPF_MAP_UPDATE_ELEM</code>, <code>BPF_MAP_DELETE_ELEM</code> 等參數來提供搜尋、更新、刪除 map 數值的方法。另外為了減少 system call 的開銷，也提供 <code>BPF_MAP_LOOKUP_BATCH</code>, <code>BPF_MAP_LOOKUP_AND_DELETE_BATCH</code>, <code>BPF_MAP_UPDATE_BATCH</code>, <code>BPF_MAP_DELETE_BATCH</code> 等方法來在單次 system call 內完成多次 map 操作。</p><p>必要要注意的是 map 並不是 eBPF program 的附屬品，在 eBPF 虛擬機內，map 和 program 一樣是獨立的物件，每個 map 有自己的 refcnt 和生命週期，eBPF 程式的生命週期和 map 不一定是統一的。</p><h3 id="map-載入流程"><a href="#map-載入流程" class="headerlink" title="map 載入流程"></a>map 載入流程</h3><p>在透過函式庫將 eBPF 程式載入 kernel 時，先做的其實是建立 map，對每張 map 會呼叫 <code>bpf system call</code> 的 BPF_MAP_CREATE，並帶入 map type, key size, value size, max entries, flags 等資訊來建立 map，建立完成後會返回 map 對應的 fire descripter。</p><p>接著函數庫會修改編譯過的 ebpf bytecode 裡面參考到 map 變數的地方 (例如 lookup 等 helper function 的參數部分)，將原先流空的 map 地址修改成 map 對應的 file descripter。</p><p>接著一樣呼叫 <code>bpf</code> BPF_PROG_LOAD 來載入 eBPF bytecode，在載入過程中，verifier 會呼叫到 replace_map_fd_with_map_ptr 函數，將 bytecode 裡面 map 的 file descripter 在替換成 map 的實際地址。</p><h3 id="Map-持久化"><a href="#Map-持久化" class="headerlink" title="Map 持久化"></a>Map 持久化</h3><p>如前面所述，map 在 eBPF 虛擬機內和 prog 同等是獨立的存在，並且具有自己的 refcnt，因此和 prog 一樣，我們可以透過 <code>bpf</code> BPF_OBJ_PIN 將 map 釘到 BPFFS 的 <code>/sys/fs/bpf/</code> 路徑下，其他程式就一樣能透過 open file 的方式取得 map 的 file descripter，將 map 載入到其他的 eBPF 程式內，達成了多個 eBPF 程式 share 同一個 map 的效果。</p><h2 id="Tail-call"><a href="#Tail-call" class="headerlink" title="Tail call"></a>Tail call</h2><p>最後我們要來聊的是 tail call 的功能。</p><p>tail call 簡單來說就是在 eBPF 程式內執行另外一個 eBPF 程式，不過和一般的函數呼叫不一樣，eBPF 虛擬機在跳轉到另外一個 eBPF 程式後就不會再回到前一個程式了，所以他是一個單向的呼叫。</p><p>另外雖然他會直接複用前一個 eBPF 程式的 stack frame，但是被呼叫的 eBPF 程式不能夠存取前呼叫者的暫存器和 stack，只能透取得在呼叫 tail call 時，透過參數傳遞的 <code>ctx</code>。</p><p>使用 tail call 可以透過拆解簡化一個 eBPF 程式，打破單個 eBPF 程式只能有 512bytes 的 stack、1 million 個指令的限制。</p><p>一個使用範例是先使用一個 eBPF 程式作為 packet dispatcher，然後根據不同的 packet ether type 之類的欄位，將 packet 轉發給對應處理的 eBPF 程式。</p><p>另外一個就是將 eBPF 程式視為多個模組，透過 map 和 tail call 去動態的任意重整排序執行結構。</p><p>為了避免 eBPF 程式交替呼叫彼此導致卡死的狀況，kernel 定義了 <code>MAX_TAIL_CALL_CNT</code> 表示在單個 context 下最多可呼叫的 tail call 次數，目前是 32。如果 tail call 因為任何原因而執行失敗，則會繼續執行原本的 eBPF 程式。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>tail call 的 helper function 定義如下 <code>long bpf_tail_call (void *ctx, struct bpf_map *prog_array_map, u32 index)</code>。在使用的時候我們要一個 <code>BPF_MAP_TYPE_PROG_ARRAY</code> type 的 map，用來保存一個 eBPF program file descriptor 的陣列。在呼叫 tail call 的時候傳遞進去執行。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://blogs.oracle.com/linux/post/bpf-a-tour-of-program-types">BPF: A Tour of Program Types</a></li><li><a href="https://arthurchiao.art/blog/bpf-advanced-notes-1-zh/">BPF 程序（BPF Prog）类型详解</a></li><li><a href="https://facebookmicrosites.github.io/bpf/blog/2018/08/31/object-lifetime.html">Lifetime of BPF objects</a></li><li><a href="https://stackoverflow.com/questions/68278120/ebpf-difference-between-loading-attaching-and-linking">difference between loading, attaching, and linking?</a></li><li><a href="https://vvl.me/2021/02/eBPF-3-eBPF-map/">eBPF map</a></li><li><a href="https://arthurchiao.art/blog/bpf-advanced-notes-3-zh/">BPF Map 内核实现</a></li><li><a href="https://www.ebpf.top/post/bpf_ring_buffer/">BPF 环形缓冲区</a></li><li><a href="https://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/108612744">BPF 技术介绍及学习分享</a></li><li><a href="https://www.ebpf.top/post/map_internal/">揭秘 BPF map 前生今世</a></li><li><a href="https://davidlovezoe.club/wordpress/archives/1044">BPF 数据传递的桥梁 ——BPF MAP（一）</a></li><li><a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">bpf-helpers man page</a></li><li><a href="https://lwn.net/Articles/645169/">introduce bpf_tail_call () helper</a></li><li><a href="https://www.readfog.com/a/1663618518017478656">從 BPF to BPF Calls 到 Tail Calls</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇文章將會介紹包含 program type, map 等重要概念還有 eBPF 的載入流程&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>學習 eBPF 系列 1 - 摘要與簡介</title>
    <link href="https://blog.louisif.me/eBPF/Learn-eBPF-Serial-1-Abstract-and-Background/"/>
    <id>https://blog.louisif.me/eBPF/Learn-eBPF-Serial-1-Abstract-and-Background/</id>
    <published>2022-10-31T05:54:04.000Z</published>
    <updated>2022-10-31T10:32:09.492Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>這個系列是 2022 年參加 ithome 鐵人 30w 競賽的產物，參賽主題是 “<a href="https://ithelp.ithome.com.tw/users/20152703/ironman/5911">教練我想玩 eBPF</a>“，因為近年來 eBPF 成為 cloud native 的一個熱門話題，在 COSCUP2022 還有一些 meetup 的活動都在討論這個議題，因此希望能夠找個機會來學習這項技術，借助這個比賽的機會，來學習和整理 eBPF 的知識，比賽結束後將這三十天的產出重新整理後重新發表，成為本 “學習 eBPF 系列”。</p><span id="more"></span><p>本系列文章主要是兩個部分，首先是針對 eBPF 的一些基本語法、架構和用法的基本介紹。接著透過 trace bcc 這個 eBPF 的開發框架的許多範例原始碼，來了解 eBPF 的用途和實例，最後簡單介紹一下所有的 eBPF 裡面很重要的 helper functions。</p><h2 id="eBPF-的前身"><a href="#eBPF-的前身" class="headerlink" title="eBPF 的前身"></a>eBPF 的前身</h2><p>要介紹 eBPF 勢必得先聊聊 eBPF 的前身 Berkeley Packet Filter (BPF)，BPF 最早是在 1993 年 USENIX 上發表的一個在類 Unix 系統上封包擷取的架構。</p><p>由於封包會持續不斷的產生，因此在擷取封包時，單一個封包的處理時間可能只有幾豪秒的時間，又封包擷取工具的使用者通常只關注某部分特定的封包，而不會是所有的封包，因此將每個封包都丟到 User space 來處理是極為低效的行為，因此 BPF 提出了一種在 kernal 內完成的封包過濾的方法。</p><p>簡單來說，BPF 在 kernal 內加入了一個簡易的虛擬機環境，可以執行 BPF 定義的指令集，當封包從網卡內進入的時候，就會進到 BPF 的虛擬機，根據虛擬機的執行結果來決定是否要解取該封包，要的話再送到 user space，因此可以直接在 kernal 過濾掉所有不必要的封包。</p><p>大家最常使用的封包過濾工具應該是 <code>tcpdump</code> ，tcpdump 底下就是基於 BPF 來完成封包過濾的，tcpdmp 使用了 libpcap 這個 library 來與 kernal 的 BPF 溝通，當我們下達 <code>tcpdump tcp port 23 host 127.0.0.1</code> 這樣的過濾規則時，過濾規則會被 libpcap 編譯成 BPC 虛擬機可以執行的 bpf program，然後載入到 kernal 的 BPF 虛擬機，BPF 擷取出來的封包也會被 libpcap 給接收，然後回傳給 tcpdump 顯示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tcpdump -d ip<br></code></pre></td></tr></table></figure><p>透過 <code>-d</code> 這個參數，我們可以看到 <code>ip</code> 這個過濾規則會被編譯成怎樣的 BPF program</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">(000) ldh      [12]<br>(001) jeq      #0x800           jt 2    jf 3<br>(002) ret      #262144<br>(003) ret      #0<br></code></pre></td></tr></table></figure><ul><li>Line 0 <code>ldh</code> 指令複製從位移 12 字節開始的 half word (16 bits) 到暫存器，對應到 ethernet header 的 ether type 欄位。</li></ul><p><img src="/eBPF/Learn-eBPF-Serial-1-Abstract-and-Background/ethernet_header.png" alt="Ethernet header"></p><ul><li>Line 1 <code>jeq</code> 檢查暫存器數值是否為 <code>0x0800</code> (對應到 IP 的 ether type)<ul><li>是的話，走到 Line 2 <code>return 262144</code></li><li>不是的話，跳到 Line 3 <code>return 0</code></li></ul></li><li><code>ret</code> 指令結束 BPF 並根據回傳值決定是不是要擷取該封包，回傳值為 0 的話表示不要，非 0 的話則帶表要擷取的封包長度，tcpdump 預設指定的擷取長度是 262144 bytes。</li></ul><p>BPF 提供了一個高效、可動態修改的 kernal 執行環境的概念，這個功能不僅只能用在封包過濾還能夠用在更多地方，因此在 Linux kernal 3.18 加入了 eBPF 的功能，提供了一個 “通用的” in-kernal 虛擬機。承接了 BPF 的概念，改進了虛擬機的功能與架構，支援了更多的虛擬機啟動位置，使 eBPF 可以用在更多功能上。</p><p>也因為 eBPF 做為一個現行更通用更強大的技術，因此現在提及 BPF 常常指的是 eBPF，而傳統的 BPF 則用 classic BPF (cBPF) 來代指。</p><h2 id="eBPF-的應用"><a href="#eBPF-的應用" class="headerlink" title="eBPF 的應用"></a>eBPF 的應用</h2><p>在介紹 BPF 的時候，有提到 BPF 本身就是一個在 kernal 內的虛擬機。eBPF 在 kernal 的許多功能內埋入了虛擬機的啟動點 (hook point)。例如當 kernal 執行 clone 這個 system call 的時候，就會去檢查有沒有 eBPF 程式的啟動條件是等待 clone 這個 system call，如果有的話就會調用 BPF 虛擬機執行 eBPF 程式，同時把 clone 相關的資訊帶入到虛擬機。同時虛擬機的執行結果可以控制 kernal 的後續行為，因此可以透過 eBPF 做到改變 kernal 程式進程、數據，擷取 kernal 執行狀態等功能。</p><p>使用 eBPF 我們可以在不用修改 kernal 或開發 kernal module 的情況下，增加 kernal 的功能，大大了降低了 kernal 功能開發的難度還有降低對 kernal 環境版本的依賴。</p><p>這邊舉立一些 eBPF 的用途</p><ul><li><p>in kernal 的網路處理：以往在 linux 上要實作網路封包的處理，通常都會經過整個 kernal 的 network stack，通過 iptables (netfilter), ip route 等組件的處理。透過 eBPF，我們可以在封包進入 kernal 的早期去直接丟棄非法封包，這樣就不用讓每個封包都要跑完整個 network stack，達到提高效能的作用</p><ul><li>最知名的應該是 Cilium 這個 CNI 專案，基於 eBFP 提供了整套完整網路、安全、監控的 Kubernetees CNI 方案。</li></ul></li><li><p>kernal tracing: 前面提到 eBPF 在 kernal 內的許多地方都埋入了啟動點，因此透過 eBPF 可以再不用對 kernal 做任何修改的情況下，很有彈性的監聽分析 kernal 的執行狀況</p><ul><li>下圖是 bcc 專案使用 eBPF 開發的一系列 Linux 監看工具，基本涵蓋了 kernal 的各個面向。<br>  <img src="/eBPF/Learn-eBPF-Serial-1-Abstract-and-Background/bcc_tracing_tools.png" alt="bcc tracing tools"></li></ul></li><li><p>另外一個專安 <code>bpftrace</code> 也提供了一個非常簡單的語法，來產生對應的 eBFP tracing code。</p></li><li><p>user level tracing: 透過 eBFP，我們可以做 user level 的 dynamic tracing，來監看 user space 應用程式的行為。</p><ul><li>一個很有趣的案例是我們可以使用 eBPF 來做 ssl 加密連線的監看。SSL&#x2F;TSL 的連線加密通常是在 user space 應用程式內完成加密的，因此即便我們監看應用程式送入 kernal socket 的內容，內容也已經是被加密的了。但是要拆解應用程式來查看又相對比較複雜困難，使用 eBPF 就可以用一個相對簡單的方法來監看加密訊息。</li><li>在 Linux 上，應用程式的加密經常會使用 libssl 這個 library 來完成，並使用 libssl 提供的 <code>SSL_read</code> 和 <code>SSL_write</code> 取代 socket 的 <code>read</code> 和 <code>write</code>，透過 eBPF 的功能，我們可以比較簡單的直接監聽應用程式對這兩個函數的呼叫，並直接提取出未加密的連線內容。</li></ul></li><li><p>Security: 前面有講到透過 eBFP，我們可以監控 system call 的呼叫、kernal 的執行、user space 程式的函數呼叫等等，因此我們也就可以透過 eBFP 來監控這些事件，並以此檢測程式的安全，拒絕非法的 system call 呼叫，或異常行為等等。<br>  + 詳細可以參考 <code>Tetragon</code> 和 <code>tracee</code> 之類的專案。</p></li></ul><p>上面大概介紹了一些 eBFP 的應用場景，BPF 經過擴展之後，不再侷限於封包過濾這個場景，而在網路處理、内核追蹤、安全監控，等各個方面有了更多可以開發的潛能。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><p><a href="https://www.usenix.org/legacy/publications/library/proceedings/sd93/mccanne.pdf">The BSD Packet filter paper</a></p></li><li><p><a href="https://www.tcpdump.org/manpages/pcap_compile.3pcap.html">tcpdump man page</a></p></li><li><p><a href="https://blog.csdn.net/dillanzhou/article/details/96913981">tcpdump 与 libpcap 原理分析</a></p></li><li><p><a href="https://blog.px.dev/ebpf-openssl-tracing/">Debugging with eBPF Part 3: Tracing SSL&#x2F;TLS connections</a></p></li><li><p><a href="https://ebpf.io/applications/">eBPF applications</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;這個系列是 2022 年參加 ithome 鐵人 30w 競賽的產物，參賽主題是 “&lt;a href=&quot;https://ithelp.ithome.com.tw/users/20152703/ironman/5911&quot;&gt;教練我想玩 eBPF&lt;/a&gt;“，因為近年來 eBPF 成為 cloud native 的一個熱門話題，在 COSCUP2022 還有一些 meetup 的活動都在討論這個議題，因此希望能夠找個機會來學習這項技術，借助這個比賽的機會，來學習和整理 eBPF 的知識，比賽結束後將這三十天的產出重新整理後重新發表，成為本 “學習 eBPF 系列”。&lt;/p&gt;</summary>
    
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/categories/eBPF/"/>
    
    
    <category term="eBPF" scheme="https://blog.louisif.me/tags/eBPF/"/>
    
    <category term="linux" scheme="https://blog.louisif.me/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>使用 Terraform 部屬 Proxmox 虛擬機</title>
    <link href="https://blog.louisif.me/Terraform/Deploy-proxmox-vm-with-terraform/"/>
    <id>https://blog.louisif.me/Terraform/Deploy-proxmox-vm-with-terraform/</id>
    <published>2022-10-28T16:00:00.000Z</published>
    <updated>2022-10-28T16:26:16.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Proxmox 提供了 web GUI 來方便的建立和管理 LXC 和虛擬機，但是如果有大量的虛擬機需要建立，那麼使用 GUI 就會變得非常繁瑣，而且不利於版本控制。因此我們可以使用 Terraform 來完成自動化建立的過程。然而在 Proxmox 上使用 Terraform，我覺得相對 openstack 來說概念會比較複雜一點，因此花了一點點時間來釐清。這邊記錄下使用 terrform 管理 Proxmox 的基本操作，希望對大家有幫助。</p><span id="more"></span> <h2 id="Cloud-init-基本觀念"><a href="#Cloud-init-基本觀念" class="headerlink" title="Cloud-init 基本觀念"></a>Cloud-init 基本觀念</h2><p>在使用 Terraform 建立 Proxmox VM 的過程中，我們會使用到 cloud-init 這個技術。<br>在使用 Promox GUI 設置虛擬機的過程中會有兩大麻煩的地方，第一個是需要在 web GUI 介面上一台一台的建立出來，第二個是需要在每台虛擬機上完成 OS 的安裝，設置硬碟、網路、SSH 等。<br>前者我們透過 terraform 來解決，後者我們則會搭配利用 cloud-init。cloud-init 是一個業界標準，在許多 Linux 發行版還有公 &#x2F; 私有雲上都有相對應的支援。<br>各 Linux 發行版會發行特製的 cloud image 來支持 cloud-init。<br>支援 cloud-init 的作業系統會在開機執行的時候執行透過特定方式去讀取使用者設定檔，自動完成前面提到的網路、帳號等設置，來達到自動化的目的。</p><h3 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h3><p>在 cloud image 中，cloud-init 會根據設定檔來完成設置，而設定檔的來源 (Data source) 有很多種，不同的 cloud (AWS, Azure, GCP, Openstack, Proxmox) 在 cloud-init 標準下制定了不同的設定檔來源。(可參考 <a href="https://cloudinit.readthedocs.io/en/latest/topics/datasources.html">文件</a>)</p><p>在 Proxmox 上支援 NoCloud 和 ConfigDrive 兩種資料源，兩種的執行方式相似，將使用者設定檔轉成一個特製的印象檔掛載到虛擬機上，當 VM 開機時 cloud-init 可以自動搜索到該印象檔，並讀取裡面的設定檔來完成設置。</p><h2 id="前置作業"><a href="#前置作業" class="headerlink" title="前置作業"></a>前置作業</h2><p>首先我們要先安裝 Terraform 和在 proxmox 上安裝 cloud-init 的工具，這邊簡單直接把 Terraform 也裝在 promox host 上面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># cloud-init<br>apt-get install cloud-init<br><br># Terraform<br>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg<br>echo &quot;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main&quot; | sudo tee /etc/apt/sources.list.d/hashicorp.list<br>sudo apt update &amp;&amp; sudo apt install terraform<br></code></pre></td></tr></table></figure><h2 id="Proxmox-amp-Cloud-init"><a href="#Proxmox-amp-Cloud-init" class="headerlink" title="Proxmox &amp; Cloud-init"></a>Proxmox &amp; Cloud-init</h2><p>再透過 Terraform 自動部屬之前，我們要先看看怎麼在 Proxmox 上搭配 cloud-init 手動部屬 VM。</p><p>這邊我們透過 promox 的 CLI 工具來完成設置，不過操作也都可以透過 GUI 完成。</p><h3 id="建立-VM"><a href="#建立-VM" class="headerlink" title="建立 VM"></a>建立 VM</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">export VM_ID=&quot;9001&quot;<br><br>cd /var/lib/vz/template/iso/<br>wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img<br><br>qm create $VM_ID \<br>    --name ubuntu-2004-focal-fossa \<br>    --ostype l26 \<br>    --memory 8192 \<br>    --balloon 2048 \<br>    --sockets 1 \<br>    --cores 1 \<br>    --vcpu 1  \<br>    --net0 virtio,bridge=vmbr11 \<br>    --onboot 1<br><br>qm importdisk $VM_ID focal-server-cloudimg-amd64.img local-lvm --format qcow2<br><br>qm set $VM_ID --scsihw virtio-scsi-pci<br>qm set $VM_ID --scsi0 local-lvm:vm-$VM_ID-disk-0<br>qm set $VM_ID --boot c --bootdisk scsi0<br><br>qm set $VM_ID --ide2 local-lvm:cloudinit<br>qm set $VM_ID --serial0 socket<br></code></pre></td></tr></table></figure><p>前面提到 cloud-init 要使用特製的印象檔，這邊我們透過 wget 抓取印象檔，放到 <code>/var/lib/vz/template/iso/</code> 路徑下，這是 proxmox 預設放置 ISO 檔的路徑，因此可以透過 GUI 到 storage&#x2F;local&#x2F;ISO image 的頁面看到我們剛剛下載的印象檔。</p><p>接著透過 <code>qm create</code> 指令建立 VM，這邊 balloon 參數對應到 <code>Minimum memory</code> 的設定。</p><p>這邊提供的 cloud image 提供的印象檔並不是通常我們用來安裝作業系統的 iso 安裝檔，而是 qcow2 文件，qcow2 是一種虛擬機硬碟快照格式，因此這邊我們透過 importdisk 指令，直接將 img 轉換成硬碟。</p><p>接著我們要將建立好的硬碟掛載到 VM 上，這邊我們指定 scsi0 介面，將硬碟掛載上去，同時由於我們要從 cloud image 開機，因此這邊直接將 bootdisk 設定為 scsi0。</p><blockquote><p>Proxmox 官方文件有提到，ubuntu 的 cloud-init 映像，如果使用 scsi 接口掛載的話，需要將控制器設置為 <code>virtio-scsi-pci</code>。</p></blockquote><p>接著我們需要添加兩個特殊的設備，首先是 cloudinit (GUI 顯示為 cloud driver)，這個是前面提到用於傳輸 cloud-init 設定檔的設備，當在 proxmox 上完成 cloud-init 設定後，proxmox 會生成對應的印象檔掛到 cloud driver 上，</p><p>另外由於 cloud image 的特殊性，我們需要添加一個 srial 設備。</p><p>到這邊設址結果如下圖：<br><img src="/Terraform/Deploy-proxmox-vm-with-terraform/proxmox_hardware.png" alt="Proxmox hardware 結果"></p><h3 id="設定-cloud-init"><a href="#設定-cloud-init" class="headerlink" title="設定 cloud-init"></a>設定 cloud-init</h3><p>接著我們要設定 cloud-init，這邊我們透過 GUI 的方式來完成設定。<br><img src="/Terraform/Deploy-proxmox-vm-with-terraform/proxmox_cloudinit.png" alt="Proxmox cloudinit 設定"><br>在 proxmox 上我們可以簡單的在 GUI 完成 cloudinit 的設定 (包含帳號、密碼、SSH key 等)，接著按下 <code>Regenerage image</code> 按鈕，proxmox 會生成設定檔，並掛載到前面建立的 cloud driver 上。</p><h3 id="啟動-VM"><a href="#啟動-VM" class="headerlink" title="啟動 VM"></a>啟動 VM</h3><p>接著我們只要按下 <code>Start</code> 按鈕，VM 就會開機，並自動完成前面的 cloud-init 設定。</p><blockquote><p>Cloud image 不太建議使用密碼登入，因此預設 VM 通常都會把 SSH 密碼登入關閉，因此需要透過 SSH key 登入，或著使用最後後提到的 cicustom 來修改 SSH 設定。</p></blockquote><h2 id="使用-Terraform"><a href="#使用-Terraform" class="headerlink" title="使用 Terraform"></a>使用 Terraform</h2><p>接著我們就要搭配 Terraform 來將完成自動化部屬了。(<a href="https://registry.terraform.io/providers/Telmate/proxmox/2.9.11">Proxmox provider</a>)</p><p>首先前面的指令不能丟，我們在最後加上一行 <code>qm template $VM_ID</code>，將 VM 轉成模板用於後續的 Terraform 部屬。<br>這邊使用模板，目前研究起來有兩個原因，首先硬碟、cloud driver、serial 這些固定虛擬硬體和 cloud image 可以直接複製，而不用在 Terraform 上重新設定。<br>另外 proxmox 的 terraform provider 好像不支援 importdisk 這樣導入 qcow2 印象檔的方式。</p><p>首先是 provider 的基礎設定</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs hcl">terraform &#123;<br>  required_providers &#123;<br>    proxmox = &#123;<br>      source = &quot;Telmate/proxmox&quot;<br>      version = &quot;2.9.11&quot;<br>    &#125;<br>  &#125;<br>&#125;<br><br>provider &quot;proxmox&quot; &#123;<br>  # Configuration options<br>  pm_api_url = &quot;https://127.0.0.1:8006/api2/json&quot;<br>  pm_user    = &quot;root@pam&quot;<br>  pm_password = &quot;password&quot;<br>  pm_tls_insecure = true<br>&#125;<br></code></pre></td></tr></table></figure><p>這邊我們直接使用 root 的帳號密碼登入 proxmox web，不過為了安全和控管的話，建議還是建立額外的使用者給 terraform 使用，以及使用 token 來取代密碼。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 建立 terraform 使用者 </span><br>pveum role add TerraformProv -privs <span class="hljs-string">&quot;VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit&quot;</span><br>pveum user add terraform-prov@pve --password &lt;password&gt;<br>pveum aclmod /-user terraform-prov@pve -role TerraformProv<br><br><span class="hljs-comment"># 建立 token</span><br>pveum user token add terraform-prov@pve terraform-token<br></code></pre></td></tr></table></figure><p>接著我們透過 proxmox_vm_qemu 資源來建立 VM</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs hcl">resource &quot;proxmox_vm_qemu&quot; &quot;resource-name&quot; &#123;<br>  name        = &quot;VM-name&quot;<br>  target_node = &quot;Node to create the VM on&quot;<br><br>  clone = &quot;ubuntu-2004-focal-fossa&quot;<br>  full_clone = true<br>  os_type = &quot;cloud-init&quot;<br><br>  onboot  = true<br>  cores    = 8<br>  sockets  = 1<br>  cpu      = &quot;host&quot;<br>  memory   = 8192<br>  balloon  = 2048<br>  scsihw   = &quot;virtio-scsi-pci&quot;<br>  bootdisk = &quot;virtio0<br><br>  disk &#123;<br>    slot     = 0<br>    size     = &quot;65536M&quot;<br>    type     = &quot;scsi&quot;<br>    storage  = &quot;local-lvm&quot;<br>    iothread = 1<br>  &#125;<br><br>  ipconfig0 = &quot;192.168.0.1/24,gw=192.168.0.254&quot;<br>  ciuser=&quot;username&quot;<br>  cipassword=&quot;password&quot;<br>  sshkeys = file (&quot;/root/.ssh/id_rsa.pub&quot;)<br>&#125;<br></code></pre></td></tr></table></figure><p>首先當然是指定我們 VM 的名子還有要長在 proxmox cluster 的哪台機器上 (name, target_node)。<br>接著我們指定我們要 clone 的我們剛剛做的 VM 模板 (clone) 並指定為完整複製 (full_clone)，以及指定 OS type 為 <code>cloud-init</code>。</p><p>接著是設定 VM 的 CPU、memory 等硬體規格，這邊要特別注意的是這先參數的規格，如果不指定，並不會套用模板的規格，而是 provider 預設的規格，因此我們需要指定這些參數。</p><p>接著比較特別的是我們要重新定義我們的硬碟，前面雖然我們已經將 cloud image 轉成硬碟掛載到 VM 上了，但是這樣掛載上去硬碟大小是絕對不夠用的 (以 ubuntu 的 image 來說只有 2G 多的硬碟大小)，因此我們這邊複寫修改 <code>scsi0</code> 的硬碟大小，cloud-init 在第一次開機的時候能夠自我察覺並修改分割區的大小來匹配新的硬碟容量。</p><p>最後就是 cloud-init 的設定，這邊我們指定 VM 的 IP、帳號密碼、以及 ssh key。</p><p>最後就一樣透過指令完成自動部屬</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">terraform init<br>terraform apply<br></code></pre></td></tr></table></figure><p>到這邊我們就完成 terraform 與 proxmox 搭配的自動部屬了。</p><h2 id="其他雜紀"><a href="#其他雜紀" class="headerlink" title="其他雜紀"></a>其他雜紀</h2><h3 id="cicustom"><a href="#cicustom" class="headerlink" title="cicustom"></a>cicustom</h3><p>前面我們都是透過 proxmox 本身的功能來生成 cloud-init 的設定檔，但是 proxmox 提供的設置選項有限，因此有時候我們會需要直接修改 cloud-init 的設定檔，<br>在 proxmox 上提供兩種方式來直接設定 cloud-init 設定檔的參數，一個是直接在指令上提供參數值，另外一個是直接提供 cloud-init 的 yaml 設定檔</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">qm set 9000 --cicustom &quot;user=&lt;volume&gt;,network=&lt;volume&gt;,meta=&lt;volume&gt;&quot;<br>qm set 9000 --cicustom &quot;user=local:snippets/userconfig.yaml&quot;<br></code></pre></td></tr></table></figure><p>在 terraform 上面，我們一樣可以透過 <code>cicustom</code> 設定來達到相同的事情。</p><h3 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h3><p>在查找資料時，在許多範例會看到指定 <code>agent</code> 參數為 0 或 1，這邊的 agent 指的是 <code>Qemu-guest-agent</code>，簡單來說就是在虛擬機內部安裝一個 agent 來當作 proxmox 直接操作虛擬機內部的後門，不過具體的功能就不在本篇的範圍內了，且預設情況下這個功能是關閉的。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>這邊簡單紀錄了一下 terraform 和 proxmox 的搭配使用，在一開始研究的時候，cloud-init 還有使用 VM template 這兩件事，是之前在使用 terraform 或 proxmox 不會特別注意到的東西，因此會有點混亂和不知道功能，希望這篇文章能夠幫助到有需要的人。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://registry.terraform.io/providers/Telmate/proxmox/latest/docs">Terraform Provider for Proxmox</a></li><li><a href="https://pve.proxmox.com/wiki/Cloud-Init_Support">Proxmox Cloud-Init</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Proxmox 提供了 web GUI 來方便的建立和管理 LXC 和虛擬機，但是如果有大量的虛擬機需要建立，那麼使用 GUI 就會變得非常繁瑣，而且不利於版本控制。因此我們可以使用 Terraform 來完成自動化建立的過程。然而在 Proxmox 上使用 Terraform，我覺得相對 openstack 來說概念會比較複雜一點，因此花了一點點時間來釐清。這邊記錄下使用 terrform 管理 Proxmox 的基本操作，希望對大家有幫助。&lt;/p&gt;</summary>
    
    
    
    <category term="Terraform" scheme="https://blog.louisif.me/categories/Terraform/"/>
    
    
    <category term="IaC" scheme="https://blog.louisif.me/tags/IaC/"/>
    
    <category term="Terraform" scheme="https://blog.louisif.me/tags/Terraform/"/>
    
    <category term="Proxmox" scheme="https://blog.louisif.me/tags/Proxmox/"/>
    
  </entry>
  
  <entry>
    <title>Openstack 架設系列文章 (1) - 網路架構解析及設置</title>
    <link href="https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/"/>
    <id>https://blog.louisif.me/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/</id>
    <published>2022-10-04T19:10:57.000Z</published>
    <updated>2022-10-31T11:58:16.994Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在嘗試布建 openstack 的過程中，卡關最久的應該是網路的部分，由於在設置 openstack 的過程中，需要先把網路介面和線路連接設定好，所以必須要對 openstack 的網路架構，有清晰的認知，不然在設置的過程中會遇到許多障礙…</p><p>特別是 openstack 有 <code>flat, vlan, vxlan</code> 以及 <code>provider, self-service</code> 兩種不同維度的概念分類，因此在釐清的過程中花費許多時間，因此這邊整理一下 openstack 的各種網路架構。</p><p>這篇文章不會介紹具體的 openstack 搭建，不過之後會有一篇介紹如何使用 openstack-ansible 來部屬 openstack。</p><span id="more"></span><p>首先為了後續討論方便我們這邊要先幫 openstack 裡面的幾個網路層級訂個名子。</p><ul><li>物理網路：由 openstack 節點上面的實體網卡以及結點外的網路構成</li><li>節點覆蓋 (Overlay) 網路：節點上面的特定一個 linux bridge 透過物理網路與其他節點上的 bridge 共同構成的一個 L2 網路</li><li>虛擬網路：以虛擬機的視角看到的 L2 網路<br>後續會持續出現這三個名詞，在介紹的過程中來解釋和分析這三者的差別。</li></ul><h2 id="Openstack-網路類型概念"><a href="#Openstack-網路類型概念" class="headerlink" title="Openstack 網路類型概念"></a>Openstack 網路類型概念</h2><p>首先我們先撇開 openstack，我們看一個最簡單在多台實體設備上面裝設虛擬機構建出來的一個網路結構。</p><p><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221002233913.png" alt="基礎網路架構"><br>在這個架構裡，所有的 VM 和實體網卡都接在同一個 linux bridge 上，由於實體網卡也橋接在 bridge 上了，所以節點上的虛擬網路是物理網路的延伸，可以看成所有實體共同構成了一個 L2 網路。<br>由於 bridge 本身可以給予 IP，當成是接在節點上的網路介面，因此 VM 和節點本身也都同在一個 L2 網路內彼此互相可見。<br>在這個網路架構中，物理網路、節點覆蓋網路和虛擬網路三者是完全等價通透的。</p><h3 id="Flat-network"><a href="#Flat-network" class="headerlink" title="Flat network"></a>Flat network</h3><p><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221002235709.png" alt="基礎 Openstack 網路架構"><br>接著我們加入 openstack 的組件。<br>首先是 controller 節點，我們在 controller 節點上部屬了 openstack 的 API 服務。<br>接著我們在 compute 節點上增加一張實體網卡 <code>eth1</code> 讓節點上的 nova agent 和 neutron agent 可以與 contrroller node 溝通。<br>在這個架構中，所有的 service 和 agent 是直接安裝在節點上的程式，因此可以透過 eth1 實體網卡的 ip 直接進行訪問。<br>在圖中的 linux bridge 換了顏色，因為這個 bridge 並不是由我們手動建立出來的而是由 neutron agent 自行建立出來的。同時當 nova 建立 VM 後，neutron 也會建立 veth pair 連接 bridge 和 VM。<br>到此我們就建立了最簡單的 openstack 網路，在這個網路架構下雖然節點本身的網路從 bridge 分離出來成為了獨立的網卡，但是節點本身和所有的 VM 還是同處在一個 L2 網路內，因此在這個架構下物理網路、節點覆蓋網路和虛擬網路三者還是完全等價通透的。</p><p>到此我們就建構了 neutron 的第一個種網路模式 <code>flat</code>。Neutron 的不同模式差異在於說 neuitron 管理的 bridge 是如何接入節點覆蓋網路的 (在這邊可以先直接看成物理網路，關於節點覆蓋網路我們會在後面解釋)。在 <code>flat</code> 模式下對外網卡直接橋接到 bridge 上，不對封包做處理。</p><p>在 openstack 中有 tenant networks 租戶網路的概念，我們可以在 openstack 上切出若干個獨立的 L2 網路，分給不同的 project 和 vms 使用。在使用單一個 <code>flat</code> 網路的時候，這件事是做不到了，因為所有 VM 都在同一個 L2 網路內。<br>![flat 網路運算節點架構](Openstack&#x2F;Openstack-Deployment-Serial-1-Network-Architecure-and-Config&#x2F;Pasted image20221003133348.png)<br>直觀的第一個方法當然是切出多個 <code>flat</code> network 給不同的 tenat 使用，然而這樣使用的限制非常大，首先我們需要替每個 flat 預先切出獨立的 L2 網路，在這個範例中我們使用獨立的網卡和交換機來分割，非常麻煩，雖然後面會提到可以使用節點覆蓋網路的方法來切割，只用一張網路卡達成，然而每次增加網路需要修改每個節點上 neutron agent 的設定檔，不適合隨時依據需求改動。</p><p>因此比較可行的方法是使用 vlan 或 vxlan 來切割租戶網路，並將 vlan、vxlan 的建立管理交給 neutron agent 來處理。</p><h3 id="Vlan-network"><a href="#Vlan-network" class="headerlink" title="Vlan network"></a>Vlan network</h3><p><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221003122839.png" alt="vlan 網路運算節點架構"></p><p>首先是 <code>vlan</code> 模式，在 vlan 模式下我們一樣提供一個網路介面 <code>eth0</code> 給 neutron 使用，並一樣假設所有節點上的 <code>eth0</code> 介面都在 L2 互通。</p><p>接著當我們透過 openstack 建立網路並指定為 <code>vlan</code> type 時，neutron 會替該網路分配一個 vlan id，並建立對應的 vlan 網路卡，vlan 網路卡是 linux 本身的功能，經過 vlan 網路卡的封包會從該網路卡榜定的原始網卡送出去 (eth0)，但是封包會加上 vlan 網卡綁定的 vlan id。</p><p>如上圖所示，我們切割出了 vlan 356 和 vlan 357 兩個租戶網路，並統一透過 eth0 物理網路與其他節點溝通。</p><h3 id="Vxlan-network"><a href="#Vxlan-network" class="headerlink" title="Vxlan network"></a>Vxlan network</h3><p>首先我們先介紹一下 vxlan。和 vlan 的功能類似，vxlan 的功能也是在一個網路內切割出多個虛擬網路，不過教於 vlan，vxlan 有以下特點:</p><ul><li>vlan id 只能夠切出 4096 個網路，但是 vxlan id 可以切出 2 的 24 次方個網路。</li><li>vlan 只是在 ethernet header 和 IP header 中間插了一個 vlan header，因此 vlan 只能在 L2 的物理網路內傳輸。但是 vxlan 的封裝方式是將整個 L2 封包加上一個 vxlan header 後，封裝在一個 UDP 封包內，因此透過外面的 UDP 封包，vxlan 能夠跨越 L3 網路建立 L2 的通道，連結兩個不相連的 L2 網路。</li></ul><p>在 Linux 上建立 vxlan 介面時要指定兩個東西，一個是 vni (vxlan id), 另一個是 local ip，當封包通共 vxlan 介面時，會被加上包含 vni 的 vxlan id，外部的 UDP 封包則會利用 local ip 當作 src ip，同時利用該 local ip 對應的 interface 來收發 vxlan 封包。</p><p>在 vlan 網路裡面，由於 vlan 一樣是 L2 封包，因此封包是透過 L2 的廣播學習機制來傳遞的。然而如前面所示，vxlan 封包會封裝成 UDP 封包，因此 vxlan 靠的是 L3 的路由機制來傳遞。</p><p>在 Linux 上這個路由機制有幾種作法</p><ul><li>在建立 vxlan interface 時直接指定 remote ip，建立成點對點的 vxlan tunnel</li><li>透過 linux bridge forwarding database，下達 forwarding rule，指定當 L2 封包的 mac address 是多少的時候要送到哪個 remote ip</li><li>最後也是 openstack 使用的方式，將封包發送到一個 L3 廣播地址 (例如 239.1.1.1)，linux 會自動在 L2 加上 multicast mac address，所有在同一個 L2 物理網路上的設備就能夠同時收到 vxlan 的封包。<br><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221003140037.png" alt="vxlan 網路運算節點架構"><br>回到 openstack 上，在設置 <code>vxlan</code> 時和 <code>vlan</code>、<code>flat</code> 不太一樣，我們不用指定綁定的網路介面 (eth0)，而是要指定 <code>local_ip</code> 跟 <code>vxlan_group</code>，前者對應到綁定解面 (eth0) 的 IP，後者則是 vxlan 廣播域的 IP (239.1.1.1)。<br>後面就和 vlan 一樣，在 openstack 上建立的網路時，netruon 為該網路建立對應的 vxlan 介面和 linux bridge，來提供給該網路的虛擬機使用。</li></ul><p>雖然在一台節點上面不太可能開到 4096 個 tenant，但是在整個 openstack 叢集內可以將不同 tenant 分配在不同的節點上，因此 vxlan 是有意義的。</p><h2 id="Openstack-網路設置"><a href="#Openstack-網路設置" class="headerlink" title="Openstack 網路設置"></a>Openstack 網路設置</h2><p>在 openstack neutron 中，我們要設定每個節點上的 ml2 設定檔來提供 neutron 網路的基本資訊。路徑在 <code>/etc/neutron/plugins/ml2/</code>。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="hljs-section">[ml2]</span><br><span class="hljs-attr">type_drivers</span> = flat,vlan,vxlan,local<br><span class="hljs-attr">tenant_network_types</span> = vxlan,vlan,flat<br><span class="hljs-attr">mechanism_drivers</span> = linuxbridge<br><span class="hljs-attr">extension_drivers</span> = port_security<br></code></pre></td></tr></table></figure><ul><li>首先對於上述三種不同的網路類型，如果要在 openstack 內使用需要指定啟用對應的 <code>type_drivers</code>。(除了前面介紹的三種網路類型，還有 local 跟 gre 這兩種，前者用於單機情況 neutron 的 linux bridge 不連接到任何對外網路，後者使用 gre tunnel 來取代 vxlan tunnel，這邊不多作介紹)</li><li>接著 <code>tenant_network_types</code> 表示在 openstack 設可以用於建立 project 租戶網路的網路類型，同時順序也代表了在建立網路時預設使用的網路類型優先順序。</li><li><code>mechanism_drivers</code> 這邊指定是 linuxbridge，前面提到 neutron 會建立 bridge 來連接 VM 跟外部網路介面，其實這邊還有很多其他選項，例如 open vSwitch，來提供更多網路管理功能，但是這邊就只以 linux bridge 為主要介紹對象。</li><li><code>extension_drivers</code> 則可以載入其他 plugin 來提供 QoS、安全管理的功能。</li></ul><p>接著我們要設定物理網路和網路介面的對應，在 flat 和 vlan 網路模式下，我們都需要將網路與一張可以連接到物理網路的網路介面做綁定 (前面的 eth0 或 eth1)，但是在實際情況中，每個節點上網路介面卡的名稱可能不相通，因此需要在每個節點上指定物理網路和網路介面的對應。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/linuxbridge_agent.ini</span><br><span class="hljs-section">[linux_bridge]</span><br><span class="hljs-attr">physical_interface_mappings</span> = vlan1:eth0,flat1:eth1,flat2:eth2<br></code></pre></td></tr></table></figure><p>接著對不同的網路類型有各自的網路設置，首先是 flat，要指定的東西很簡單，就是可以用於建立 flat network 的物理網路，如果所有網路都可以，則指定為 *</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="hljs-section">[ml2_type_flat]</span><br><span class="hljs-attr">flat_networks</span> = flat1, flat2<br><span class="hljs-comment"># Example:flat_networks = *</span><br></code></pre></td></tr></table></figure><p>在 vlan 部分則要指定在每個物理網路上允許的租戶網路 vlan id，格式是<br><code>&lt;physical_network&gt;[:&lt;vlan_min&gt;:&lt;vlan_max&gt;]</code></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="hljs-section">[ml2_type_vlan]</span><br><span class="hljs-attr">network_vlan_ranges</span> = vlan1:<span class="hljs-number">101</span>:<span class="hljs-number">200</span>,vlan1:<span class="hljs-number">301</span>:<span class="hljs-number">400</span><br></code></pre></td></tr></table></figure><p>最後 vxlan 分成兩個部分，首先和 vlan 類似，我們要指定可使用的 vxlan vni 和廣播域 ip</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="hljs-section">[ml2_type_vxlan]</span><br><span class="hljs-attr">vxlan_group</span> = <span class="hljs-number">239.1</span>.<span class="hljs-number">1.1</span><br><span class="hljs-attr">vni_ranges</span> = <span class="hljs-number">1</span>:<span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure><p>接著我們要設置綁定的網卡 ip</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/linuxbridge_agent.ini</span><br><span class="hljs-section">[vxlan]</span><br><span class="hljs-attr">enable_vxlan</span> = <span class="hljs-literal">True</span><br><span class="hljs-attr">vxlan_group</span> = <span class="hljs-number">239.1</span>.<span class="hljs-number">1.1</span><br><span class="hljs-comment"># VXLAN local tunnel endpoint</span><br><span class="hljs-attr">local_ip</span> = <span class="hljs-number">192.168</span>.<span class="hljs-number">56.101</span><br><span class="hljs-attr">l2_population</span> = <span class="hljs-literal">False</span><br><span class="hljs-attr">ttl</span> = <span class="hljs-number">32</span><br></code></pre></td></tr></table></figure><p>這邊有一個特別的東西是 l2 population，l2 population 是一種降低廣播封包造成網路負載的方式，在傳統網路內 ARP 封包需要廣播到所有的節點上，大大增加的網路的負擔，透過 l2 population 提供的 proxy ARP 機制，ARP 會在節點上直接由 neutron 回應，而不用透過物理網路廣播到所有節點上，大大降低網路負擔，由於 openstack 內所有的 VM IP、網卡 mac address 都會受到 openstack 的管理，因此 openstack 可以做到 proxy ARP。如果需要啟用 l2 population 則需要額外的 driver，這邊一樣不做詳細介紹。</p><p>最後是網路安全的部分，在 openstack 我們可以使用 iptables 來建立 VM 的網路管理，iptables 會阻擋所有進出虛擬機網路的流量，並透過 security group 來下達 iptables 的規則允許特定流量進出。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/linuxbridge_agent.ini</span><br><span class="hljs-section">[securitygroup]</span><br><span class="hljs-attr">firewall_driver</span> = iptables<br><span class="hljs-attr">enable_security_group</span> = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="hljs-section">[securitygroup]</span><br><span class="hljs-attr">enable_security_group</span> = <span class="hljs-literal">True</span><br><span class="hljs-attr">enable_ipset</span> = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h2 id="Provider-vs-Self-service-networks"><a href="#Provider-vs-Self-service-networks" class="headerlink" title="Provider vs Self-service networks"></a>Provider vs Self-service networks</h2><p>接著我們要介紹 openstack 裡面另外一個重要的網路概念，provider network 跟 self-service network 的差別。</p><p>首先要特別注意的是，這邊提到的分類方式和前面的網路類型是兩種不同的角度和分類方式。</p><p>雖然 flat network 通常會搭配 provider networks，vlan 和 vxlan 通常會搭配 self service network 但是這並不是一定而且必要的。</p><p>前面的網路類型我們探討的是要如何在跨節點的網路架構下提供一個或多個 L2 網路讓 VMs 之間可以彼此連接。</p><p>這邊 provider 和 self-service 的差別則是要探討，如何提供前面切割出來的網路 L3 的網路功能 (gateway, routing…)</p><p>在 provider 網路模式下，我們假設 L3 的網路功能可以直接由物理網路提供，因此 openstack 只負責提供 DHCP，以及將虛擬機接到物理網路上，其餘的 gateway, routing 功能，openstack 只假設存在，不多做處理。在最簡單的 <code>flat</code> 網路模式下這麼做當然是非常簡單可行的，最簡單的方法就是將 <code>flat</code> 網路直接接入節點間 openstack 管理用的網段，並與節點共用 ip subnet，直接由物理網路提供功能。</p><p>然而在 <code>vlan</code> 和 <code>vxlan</code> 模式下使用 proider 網路就不是這麼好用了，前面提到相較於 <code>flat</code>，<code>vlan</code> 和 <code>vxlan</code> 的特點就是能夠動態建立獨立的租戶網路，如果要使用 provider 機制，變成說我們要導入額外的自動化方式，替每個 vlan 或 vxlan 提供 gateway 的功能，因此通常會使用 self-service network。</p><p>self-service network 指的是 openstack self service，也就是由 openstack 本身來提供 L3 網路的 gateway, router 的功能。</p><p>為了要提供 router 的功能，openstack 的作法是使用 network namespace。</p><p>首先我們需要在一個節點上部署 neutron 的 L3 agent。<br>當我們透過 openstack 的 API 建立一個 virtual router 的時候，netron L3 agent 會在節點上建立一個獨立的 network namespace 當作這台 virtual router 的主體。</p><p><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221005021155.png"><br>接著我們就可以將不同的租戶網路接入 router，透過 linux 本身的 iptables, routing 的功能來提供租戶網路間 routing 的功能。<br><img src="/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pastedimage20221005021447.png"><br>我們也可以透過把其中一個租戶網路或 <code>flat</code> 網路接入物理網路，使用 provider network 的方式，其他的租戶網路可以透過 virtual router routing 到 provider network 的租戶網路來上網。</p><h3 id="Openstack-self-service-指令"><a href="#Openstack-self-service-指令" class="headerlink" title="Openstack self-service 指令"></a>Openstack self-service 指令</h3><p>這邊補充一下如何透過 cli 在 openstack 上建立 router 還有把租戶網路加入 router</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">openstack router create router1<br></code></pre></td></tr></table></figure><p>首先建立 router1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">openstack port create --network net1 net1-router-port<br></code></pre></td></tr></table></figure><p>接著為了讓租戶網路能夠 “接線” 到 router，我們需要幫網路建立一個 port</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">openstack router add port router1 net1-router-port<br></code></pre></td></tr></table></figure><p>最後把 port 接到 router 上</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在嘗試布建 openstack 的過程中，卡關最久的應該是網路的部分，由於在設置 openstack 的過程中，需要先把網路介面和線路連接設定好，所以必須要對 openstack 的網路架構，有清晰的認知，不然在設置的過程中會遇到許多障礙…&lt;/p&gt;
&lt;p&gt;特別是 openstack 有 &lt;code&gt;flat, vlan, vxlan&lt;/code&gt; 以及 &lt;code&gt;provider, self-service&lt;/code&gt; 兩種不同維度的概念分類，因此在釐清的過程中花費許多時間，因此這邊整理一下 openstack 的各種網路架構。&lt;/p&gt;
&lt;p&gt;這篇文章不會介紹具體的 openstack 搭建，不過之後會有一篇介紹如何使用 openstack-ansible 來部屬 openstack。&lt;/p&gt;</summary>
    
    
    
    <category term="Openstack" scheme="https://blog.louisif.me/categories/Openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>CNI Spec 導讀</title>
    <link href="https://blog.louisif.me/Kubernetes/CNI-Spec-Guiding/"/>
    <id>https://blog.louisif.me/Kubernetes/CNI-Spec-Guiding/</id>
    <published>2022-09-04T16:57:00.000Z</published>
    <updated>2022-09-04T18:01:07.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>這次來嘗試寫寫看 spec 導讀，今天要講的是 <a href="https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md">Container Network Interface (CNI) Specification</a>。CNI 是 <a href="https://cncf.io/">CNCF</a> 的一個專案，這個專案包含了今天要講的 CNI SPEC 以及基於這個 SPEC 開發出來 libraries 還有一系列的 CNI plugins。</p><span id="more"></span> <p>CNI 定義了一套 plugin-based 的網路解決方案，包含了設定還有呼叫執行的標準，使用 CNI 最知名的專案應該就是 kubernetes 了，在 k8s 環境中，容器的網路並不是直接由 container runtime (ex. Docker) 處理的，container runtime 建立好容器後，kubelet 就會依照 CNI 的設定和通訊標準去呼叫 CNI plugin，由 CNI plugin 來完成容器的網路設置。</p><p>Container runtime 在執行容器建立時，會依據設定檔 (後續稱為 network configuration) 的指示，依序呼叫一個或多個的 CNI plugin 來完成網路功能的設置，一個網路功能的設置可能會需要多個 CNI plugins 之間的相互合作，或著由多個 CNI plugin 提供多個不同的網路功能。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>目前 CNI spec 的版本是 1.0.0，CNI 的 repository 裡面除了 spec 文件外也包含了基於 CNI spec 開發的 go library，用於 CNI plugin 的開發。不過 CNI spec 和 library 的版本號是獨立的，當前的 library 版本是 1.1.2。</p><p>首先要對幾個基本名詞定義</p><ul><li>container 是一個網路獨立的環境，在 linux 上通常透過 network namespace 的機制來切割，不過也可能是一個獨立的 VM。</li><li>network 是 endpoints 的集合，每個 endpoint 都有著一個唯一是別的的地址 (通常是 ip address) 可用於互相通訊。一個端點可能是一個容器、VM 或著路由器之類的網路設備。</li><li>runtime 指的是呼叫執行 CNI plugin 的程式，以 k8s 來說，kubelet 作為這個角色</li><li>plugin 指的是一個用來完成套用特定網路設定的程式。</li></ul><p>CNI Spec 包含五個部分</p><ol><li>Network configuration format: CNI 網路設定的格式</li><li>Execution Protocol: runtime 和 CNI plugin 之間溝通的 protocol</li><li>Execution of Network Configurations: 描述 runtime 如何解析 network configuration 和操作 CNI plugin。</li><li>Plugin Delegation: 描述 CNI plugin 如何呼叫 CNI plugin。</li><li>Result Types: 描述 CNI plugin 回傳的結果格式。</li></ol><h2 id="Section-1-Network-configuration-format"><a href="#Section-1-Network-configuration-format" class="headerlink" title="Section 1: Network configuration format"></a>Section 1: Network configuration format</h2><p>CNI 定義了一個網路設定的格式，這個格式用於 runtime 讀取的設定檔，也用於 runtime 解析後，plugin 接收的格式，通常來說設定檔是以 ** 靜態 ** 檔案的方式存在主機上，不會任意變更。</p><p>CNI 使用了 JSON 作為設定檔的格式，並包含以下幾個主要欄位</p><ul><li><code>cniVersion</code> (string): 對應的 CNI spec 版本，當前是 1.0.0</li><li><code>name</code> (string): 一個在主機上不重複的網路名稱</li><li><code>disableCheck</code> (boolean): 如果 disableCheck 是 true 的話，runtime 就不能呼叫 <code>CHECK</code>，<code>CHECK</code> 是 spec 定義的一個指令用來檢查網路是否符合 plugin 依據設定的結果，由於一個網路設定可能會呼叫多個 CNI plugins，因此可能會出現網路狀態符合管理員預期，但是 CNI plugin 之間衝突檢查失敗的情況，這時就可以設定 disableCheck</li><li><code>plugin</code> (list): CNI plugin 設定 (Plugin configuration object) 的列表。</li></ul><h3 id="Plugin-configuration-object"><a href="#Plugin-configuration-object" class="headerlink" title="Plugin configuration object"></a><strong>Plugin configuration object</strong></h3><p>plugin configuration object 包含了一些明定的欄位，但 CNI plugin 可能根據需要增加欄位，會由 runtime 在不修改的情況下送給 CNI plugin。</p><ul><li>必填:<ul><li><code>type</code> (string): CNI plugin 的執行檔名稱</li></ul></li><li>可選欄位 (CNI protocol 使用):<ul><li><code>capabilities</code> (dictionary): 定義 CNI plugin 支援的 capabilities，後面在 <a href="#Deriving-execution-configuration-from-plugin-configuration">section 3</a> 會介紹。</li></ul></li><li>保留欄位：這些欄位是在執行過程中，由 runtime 生成出來的，因此不應該在設定檔內被定義。<ul><li><code>runtimeConfig</code></li><li><code>args</code></li><li>任何 <code>cni.dev/</code> 開頭的 key</li></ul></li><li>可選欄位：不是 protocol 定義的欄位，但是由於很多 CNI plugin 都有使用，因此具有特定的意義。<ul><li>ipMasq (boolean): 如果 plugin 支援的話，會在 host 上替該網路設置 IP masquerade，如果 host 要做為該網路的 gateway 的話，可能需要該功能。</li><li>ipam (dictionary): IPAM (IP Address Management) 設置，後面在 <a href="#Section-4-Plugin-Delegation">section 4</a> 會介紹。</li><li>dns (dictionary): DNS 設置相關設置<ul><li>nameservers (list of strings): DNS server 的 IP 列表</li><li>domain (string): DNS search domain</li><li>search (list of strings),: DNS search domain 列表</li><li>options (list of strings): DNS options 列表</li></ul></li></ul></li><li>其他欄位: CNI plugin 自己定義的額外欄位。</li></ul><p>設定檔範例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs java">&#123;<br>  <span class="hljs-string">&quot;cniVersion&quot;</span>: <span class="hljs-string">&quot;1.0.0&quot;</span>,<br>  <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;dbnet&quot;</span>,<br>  <span class="hljs-string">&quot;plugins&quot;</span>: [<br>    &#123;<br>      <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;bridge&quot;</span>,<br>      <span class="hljs-comment">//plugin specific parameters</span><br>      <span class="hljs-string">&quot;bridge&quot;</span>: <span class="hljs-string">&quot;cni0&quot;</span>,<br>      <span class="hljs-string">&quot;keyA&quot;</span>: [<span class="hljs-string">&quot;some more&quot;</span>, <span class="hljs-string">&quot;plugin specific&quot;</span>, <span class="hljs-string">&quot;configuration&quot;</span>],<br>      <br>      <span class="hljs-string">&quot;ipam&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;host-local&quot;</span>,<br>        <span class="hljs-comment">//ipam specific</span><br>        <span class="hljs-string">&quot;subnet&quot;</span>: <span class="hljs-string">&quot;10.1.0.0/16&quot;</span>,<br>        <span class="hljs-string">&quot;gateway&quot;</span>: <span class="hljs-string">&quot;10.1.0.1&quot;</span>,<br>        <span class="hljs-string">&quot;routes&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;dst&quot;</span>: <span class="hljs-string">&quot;0.0.0.0/0&quot;</span>&#125;<br>        ]<br>      &#125;,<br>      <span class="hljs-string">&quot;dns&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;nameservers&quot;</span>: [ <span class="hljs-string">&quot;10.1.0.1&quot;</span> ]<br>      &#125;<br>    &#125;,<br>    &#123;<br>      <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;tuning&quot;</span>,<br>      <span class="hljs-string">&quot;capabilities&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;mac&quot;</span>: <span class="hljs-literal">true</span><br>      &#125;,<br>      <span class="hljs-string">&quot;sysctl&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;net.core.somaxconn&quot;</span>: <span class="hljs-string">&quot;500&quot;</span><br>      &#125;<br>    &#125;,<br>    &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;portmap&quot;</span>,<br>        <span class="hljs-string">&quot;capabilities&quot;</span>: &#123;<span class="hljs-string">&quot;portMappings&quot;</span>: <span class="hljs-literal">true</span>&#125;<br>    &#125;<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Section-2-Execution-Protocol"><a href="#Section-2-Execution-Protocol" class="headerlink" title="Section 2: Execution Protocol"></a>Section 2: Execution Protocol</h2><p>CNI 的工作模式是由 container runtime 去呼叫 CNI plugin 的 binaries，CNI Protocol 定義了 runtime 和 plugin 之間的溝通標準。</p><p>CNI plugin 的工作是完成容器網路介面的某種設置，大致上可以分成兩類</p><ul><li>Interface plugin: 建立容器內的網路介面，並確保其連接可用</li><li>Chained plugin: 調整修改一個已建立的介面 (可能同時會需要建立更多額外的介面)</li></ul><p>Runtime 透過兩種方式傳遞參數，一是透過環境變數，二是透過 stdin 傳遞 Section 1 定義的 configuration。如果成功的話，結果會透過 stdout 傳遞，如果失敗的話就會錯誤資訊會透過 stderr 傳遞。configuration 和結果、錯誤都是使用 JSON 格式。</p><p>Runtime 必須在 Runtime 的網路執行，大多數情況下就是在主機的預設 network namespace&#x2F;dom0</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>Protocol 的參數都是透過環境變數來傳遞的，可能的參數如下</p><ul><li>CNI_COMMAND: 當前執行的 CNI 操作 (可能是 <code>ADD</code>, <code>DEL</code>, <code>CHECK</code>. <code>VERSION</code>)</li><li>CNI_CONTAINERID: 容器的ＩＤ</li><li>CNI_NETNS: 容器網路空間的參考，如果是使用 namespaces 的方式來切割的話，就是 namespce 的路徑（e.g. <code>/run/netns/[nsname]</code> )</li><li>CNI_IFNAME: 要建立在容器內的介面名稱，如果 plugin 無法建立該名稱則回傳錯誤</li><li>CNI_ARGS: 其他參數，Alphanumeric 格式的 key-value pairs，使用分號隔開”e.g. <code>FOO=BAR;ABC=123</code></li><li>CNI_PATH: CNI plugin 的搜尋路徑，因為 CNI 存在 CNI plugin 呼叫 CNI plugin 的情況，所以需要這個路徑。如果包含多個路徑，使用 OS 定義的分隔符號分割。Linux 使用 <code>:</code> ，Windows 使用 <code>;</code></li></ul><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>如果執行成功，CNI Plugin 應該回傳 0。如果失敗則回傳非 0，並從 stderr 回傳 error result type structure。</p><h3 id="CNI-operations"><a href="#CNI-operations" class="headerlink" title="CNI operations"></a>CNI operations</h3><h4 id="ADD"><a href="#ADD" class="headerlink" title="ADD"></a>ADD</h4><p>CNI plugin 會在 <code>CNI_NETNS</code> 內建立 <code>CNI_IFNAME</code> 介面或對該介面套用特定的設置</p><p>如果 CNI plugin 執行成功，應該從 stdout 回傳一個 result structure。如果 plugin 的 stdin 輸入包含 prevResult，他必些直接把 prevResult 包在 result structure 內或對他修改後在包在 result structure 內，runtime 會把前一個 CNI 輸出的 prevResult 包在下一個 CNI 的 stdin 輸入內。</p><p>如果 CNI plugin 嘗試建立介面時，該介面已經存在，應該發出錯誤。</p><p>Runtime 不應該在沒有 DEL 的情況下對一個 <code>CNI_CONTAINERID</code> 容器的一個 <code>CNI_IFNAME</code> 介面多次呼叫 ADD。不過可能對同一個 container 的不同介面呼叫 ADD。</p><p>必有的輸入包含 STDIN JSON configuration object、 <code>CNI_COMMAND</code>、<code>CNI_CONTAINERID</code> 、 <code>CNI_NETNS</code> 和 <code>CNI_IFNAME</code> 。 <code>CNI_ARGS</code> 和 <code>CNI_PATH</code> 為可選項。</p><h4 id="DEL"><a href="#DEL" class="headerlink" title="DEL"></a>DEL</h4><p>移除 <code>CNI_NETNS</code> 內的 <code>CNI_IFNAME</code> 介面，或還原 ADD 套用的網路設定</p><p>通常來說，如果要釋放的資源已經不存在，DEL 也應該視為成功。例如容器網路已經不存在了，一個 IPAM plugin 應該還是要正常的釋放 IP 和回傳成功，除非 IPAM plugin 對容器網路存在與否有嚴格的要求。即便是 DHCP plugin，雖然執行 DEL 操作的時侯，需要透過容器網路發送 DHCP release 訊息，但是由於 DHCP leases 有 lifetime 的機制，超時後會自動回收，因此即便容器網路不存在，DHCP plugin 在執行 DEL 操作時，也應該該回傳成功。</p><p>如果重複對一個 <code>CNI_CONTAINERID</code> 容器的 <code>CNI_IFNAME</code> 介面執行多次 DEL 操作，plguin 都應該回傳，即便介面已經不存在或 ADD 套用的修改已經還原。</p><p>必有的輸入包含 STDIN JSON configuration object、 <code>CNI_COMMAND</code>、<code>CNI_CONTAINERID</code> 和 <code>CNI_IFNAME</code>。 <code>CNI_NETNS</code>、<code>CNI_ARGS</code> 和 <code>CNI_PATH</code> 為可選項。</p><h4 id="CHECK"><a href="#CHECK" class="headerlink" title="CHECK"></a>CHECK</h4><p>Runtime 透過 <code>CHECK</code> 檢查容器的狀態，並確保容器網路符合預期。CNI spec 可以分為 plugin 和 runtime 的兩部分</p><p>Plugin:</p><ul><li><p>plugin 必須根據 <code>prevResult</code> 來判斷介面和地址是否符合預期</p></li><li><p>plugin 必須接受其他 chained plugin 對介面修改的結果</p></li><li><p>如果 plugin 建立並列舉在 <code>prevResult</code> 的 CNI Result type 資源 (介面、地址、路由規則) 不存在或不符合預期狀態，plugin 應該回傳錯誤</p></li><li><p>如果其他不在 Result type 內的資源不存在或不符合預期也應該回垂錯誤。可能的資源如下</p><ul><li>防火牆規則</li><li>流量控管 (Traffic shaping controls)</li><li>IP 保留 (Reservation)</li><li>外部依賴，如連接所需的 daemon</li></ul></li><li><p>如果發現容器網路是無法訪問的應該回傳錯誤</p></li><li><p>plugin 必須在完成 <code>ADD</code> 後能夠立即處理 <code>CHECK</code> 指令，應此 plguin 應該要容忍非同步完成的網路設置在一定的時間內不符合預期。</p></li><li><p>plugin 執行 <code>CHECK</code> 時，應該呼叫所有 delegated plugin 的 <code>CHECK</code>，並把 delegated plugin 的錯誤傳給 plugin 的呼叫者</p></li></ul><p>Runtime:</p><ul><li>Runtime 不應該在未執行 <code>ADD</code> 前或已經 <code>DEL</code> 後沒在執行一次 <code>ADD</code> 的容器執行 <code>CHECK</code></li><li>如果 configuration 的 disableCheck 為真，runtime 不應呼叫 disableCheck</li><li>Runtime 呼叫 <code>CHECK</code> 時，configuration 必須在 <code>prevResult</code> 包含前一次 <code>ADD</code> 操作時最後一個 plugin 的 Result。Runtime 可能會使用 libcni 提供的 Result caching 功能。</li><li>如果其中一個 plugin 回傳錯誤，runtime 可能不會呼叫後面 plugin 的 <code>CHECK</code></li><li>Runtime 可能會在 <code>ADD</code> 完後的下一刻一直到 <code>DEL</code> 執行前的任何時間執行 <code>CHECK</code></li><li>Runtime 可能會假設一個 <code>CHECK</code> 失敗的容器會永久處於配置錯誤的狀態</li></ul><p>必有的輸入包含 STDIN JSON configuration object、 <code>CNI_COMMAND</code>、<code>CNI_CONTAINERID</code>、<code>CNI_NETNS</code> 和 <code>CNI_IFNAME</code>。 <code>CNI_ARGS</code> 和 <code>CNI_PATH</code> 為可選項。</p><p>除了 <code>CNI_PATH</code> 以外的參數必須和 <code>ADD</code> 時一致。</p><h4 id="VERSION"><a href="#VERSION" class="headerlink" title="VERSION"></a>VERSION</h4><p>Plugin 透過 stdout 輸出 JSON 格式的 version result type object，用於查看 CNI plugin 的版本。</p><p>Stdin 輸入的 JSON 物件只包含 <code>cniVersion</code>。<br>環境變數參數只需要 <code>CNI_COMMAND</code>。</p><h2 id="Section-3-Execution-of-Network-Configurations"><a href="#Section-3-Execution-of-Network-Configurations" class="headerlink" title="Section 3: Execution of Network Configurations"></a>Section 3: Execution of Network Configurations</h2><p>這個章節描述了 runtime 如何解析 network configuration，並執行 CNI plugins。Runtime 可能會想要新增、刪除、檢查 configuration，並對應到 CNI plugin 的 <code>ADD</code>, <code>DELETE</code>, <code>CHECK</code> 操作。這個章節也定義 configuration 是如何改變並提供給 plugin 的。</p><p>對容器的 network configuration 操作稱之為 attachment，一個 attachment 通常對應到一個特定 <code>CNI_CONTAINERID</code> 容器的 <code>CNI_IFNAME</code> 介面。</p><h3 id="生命週期"><a href="#生命週期" class="headerlink" title="生命週期"></a>生命週期</h3><ul><li>Runtime 必須在呼叫任何 CNI Plugin 之前，為容器建立新的 network namespace</li><li>Runtime 一定不能同時在一個容器執行多個 plugin 命令，但是同時處理多個容器是可以的。因此 plugin 必須能夠處理多容器 concurrency 的問題，並在共享的資源 (e.g. IPAM DB) 實作 lock 機制</li><li>Runtime 必須確保 <code>ADD</code> 操作後必定執行一次 <code>DEL</code> 操作，即便 <code>ADD</code> 失敗。唯一可能的例外是如結點直接丟失之類的災難性事件。</li><li><code>DEL</code> 操作可能連續多次執行</li><li>network configuration 在 ADD 和 DEL 操作之間，以及不同的為 attachment 之間應該保持一致不變</li><li>runtime 必須負責清除容器的 network namespace</li></ul><h3 id="Attachment-Parameters"><a href="#Attachment-Parameters" class="headerlink" title="Attachment Parameters"></a>Attachment Parameters</h3><p>Network configuration 在不同的 attachments 間應該保持一致。不過 runtime 會傳遞其他每個 attachment 獨立的參數。</p><ul><li>Container ID: 對應到 Section 2 <code>CNI_CONTAINERID</code> 環境變數</li><li>Namespace: 對應到 <code>CNI_NETNS</code> 環境變數</li><li>Container interface name: 對應到 <code>CNI_IFNAME</code> 環境變數</li><li>Generic Arguments: 對應到 <code>CNI_ARGS</code> 環境變數</li><li>Capability Arguments: </li><li>CNI plugins search path: 對應到 <code>CNI_PATH</code> 環境變數</li></ul><h3 id="Adding-an-attachment"><a href="#Adding-an-attachment" class="headerlink" title="Adding an attachment"></a>Adding an attachment</h3><p>對 configuration 的 <code>plugins</code> 欄位的每個 plugin configuration 執行以下步驟</p><ol><li>根據 <code>type</code> 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤</li><li>根據 plugin configuration 生成送給 plugin sdtin 的 configuration</li></ol><ul><li>只有第一個執行的 plugin 不會帶 prevResult 欄位，後續執行的 plugin 都會把前一個的 plugin 的結果放在 prevResult</li></ul><ol start="3"><li>執行 plugin 的執行檔。設置 <code>CNI_COMMAND=ADD</code>，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration</li><li>如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller</li></ol><p>Runtime 必須持久保存最後一個 plugin 的結果，用於 check 和 delete 操作。</p><h3 id="Deleting-an-attachment"><a href="#Deleting-an-attachment" class="headerlink" title="Deleting an attachment"></a>Deleting an attachment</h3><p>刪除 attachment 和添加基本上差不多的，差別是</p><ul><li>plugin 的執行順序是反過來的，從最後一個開始</li><li><code>prevResult</code> 欄永遠是上一次 add 操作時，最後一個 plugin 的結果</li></ul><p>對 configuration 的 <code>plugins</code> 欄位反序的每個 plugin configuration 執行以下步驟</p><ol><li>根據 <code>type</code> 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤</li><li>根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration</li><li>執行 plugin 的執行檔。設置 <code>CNI_COMMAND=DEL</code>，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration</li><li>如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller</li></ol><h3 id="Checking-an-attachment"><a href="#Checking-an-attachment" class="headerlink" title="Checking an attachment"></a>Checking an attachment</h3><p>如同 Section 2 所述，Runtime 會透過 CNI plugin 檢查每個 attachment 是否正常運作中。<br>需注意的是 Runtime 必須使用和 add 操作時一致的 attachment parameters</p><p>檢查和添加只有兩個差別</p><ul><li><code>prevResult</code> 欄永遠是上一次 add 操作時，最後一個 plugin 的結果</li><li>如果 network configuation 中 <code>disableCheck</code> 為真，則直接回傳成功</li></ul><p>對 configuration 的 <code>plugins</code> 欄位的每個 plugin configuration 執行以下步驟</p><ol><li>根據 <code>type</code> 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤</li><li>根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration</li><li>執行 plugin 的執行檔。設置 <code>CNI_COMMAND=CHECK</code>，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration</li><li>如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller</li></ol><h3 id="Deriving-execution-configuration-from-plugin-configuration"><a href="#Deriving-execution-configuration-from-plugin-configuration" class="headerlink" title="Deriving execution configuration from plugin configuration"></a>Deriving execution configuration from plugin configuration</h3><p>在 add, delete, check 操作時，runtime 必須根據 network configuration 生成出 plugin 可以存取的 execution configuration (基本對應到 plugin configuration)，並填入內容。</p><p>如同 section 2 所述，execution configuration 使用 JSON 格式並透過 stdin 傳給 CNI plugin。</p><ul><li><p>必要的欄位如下:</p><ul><li>cniVersion: 同 network configuraion 中 cniVersion 的值</li><li>name: 同 network configuraion 中 name 的值</li><li>runtimeConfig: runtime 需要和 plugin 可提供的 capabilities 的聯集 (capability 會在後面討論)</li><li>prevResult: CNI plugin 回傳的 Result type 結果</li></ul></li><li><p><code>capabilities</code> 欄位必須被移除</p></li><li><p>其他 plugin configuration 的欄位應該被放入 execution configuration</p></li></ul><h4 id="Deriving-runtimeConfig"><a href="#Deriving-runtimeConfig" class="headerlink" title="Deriving runtimeConfig"></a>Deriving runtimeConfig</h4><p>相對於靜態的 network configuration 來說，runtime 可能會需要根據每個不同的 attachment 產生動態參數。<br>雖然 runtime 可以透過 <code>CNI_ARGS</code> 傳遞動態參數給 CNI plugin，但是我們沒辦法預期說 CNI plugin 會不會接收這個參數。透過 capabilities 欄位，可以明定 plugin 支援的功能，runtime 根據 capabilities 及需求，動態生成設定並填入 <code>runtimeConfig</code>。CNI spec 沒有定義 capability，但是比較通用的 capability 有列舉在另外一份 <a href="https://github.com/containernetworking/cni/blob/main/CONVENTIONS.md#dynamic-plugin-specific-fields-capabilities--runtime-configuration">文件</a>。</p><p>以 kubernetes 常用的 Node port 功能來說，需要 CNI plugin 支援 portMappings 這個 capability。<br>在 section 1 的定義中，plugin configuration 包含了 <code>capabilities</code> 欄位，在這個欄位填入 <code>portMappings</code>，讓 runtime 知道可以透過該 plugin 處理 port mapping。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;myPlugin&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;capabilities&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;portMappings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><br>  <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>Runtime 執行 CNI plugin 時，會根據 <code>capabilities</code> 生成 <code>runtimeConfig</code>，並填入對應的動態參數。 </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;myPlugin&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;runtimeConfig&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;portMappings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span> <br>      <span class="hljs-punctuation">&#123;</span> <br>        <span class="hljs-attr">&quot;hostPort&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">8080</span><span class="hljs-punctuation">,</span> <br>        <span class="hljs-attr">&quot;containerPort&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">80</span><span class="hljs-punctuation">,</span> <br>        <span class="hljs-attr">&quot;protocol&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;tcp&quot;</span> <br>      <span class="hljs-punctuation">&#125;</span> <br>    <span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">&#125;</span><br>  ...<br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h2 id="Section-4-Plugin-Delegation"><a href="#Section-4-Plugin-Delegation" class="headerlink" title="Section 4: Plugin Delegation"></a>Section 4: Plugin Delegation</h2><p>雖然 CNI 的主要的架構是一系列的 CNI plkugin 依次執行，但這樣的方式有些時候沒辦法滿足 CNI 的需求。CNI plugin 可能會需要將某些功能委託給另外一個 CNI plugin，並存在 plugin 呼叫 plugin 的情況，最常見的案例就是 IP 地址的分配管理。</p><p>通常來說，CNI plugin 應該要指定並維護容器的 IP 地址以及下達必要的 routing rules，雖然由 CNI plugin 自主完成可以讓 CNI plugin 有更大的彈性但是也加重了 CNI plugin 的職責和開發難度，讓不同的 CNI plugin 需要重複開發相同的 IP 地址管理邏輯，因此許多 CNI 將 IP 管理的邏輯委託給另一個獨立的 plugin，讓相關邏輯可以直接被複用。對此除了前面提到的 interface plugin 和 chanined plugin，我們定義了第三類的 plugin - IP Address Management Pligin (IPAM plugin)。</p><p>由主要的 CNI plugin 去呼叫 IPAM plugin，IPAM plugin 判斷網路介面的 IP 地址、gateway、routing rules，並回傳資訊給主要的 CNI plugin 去完成相對應設置。(IPAM plugin 可能會透過 dhcp 之類的 protocl, 儲存在本地的檔案系統資訊或 network configuration 的 ipam section 取得資訊)</p><h3 id="Delegated-Plugin-protocol"><a href="#Delegated-Plugin-protocol" class="headerlink" title="Delegated Plugin protocol"></a>Delegated Plugin protocol</h3><p>和 Runtime 執行 CNI plugin 的方式一樣，delegated plugin 也是透過執行 CNI plugin 可執行程式的方式。主要的 plugin 在 <code>CNI_PATH</code> 路徑下搜尋 CNI plugin。delegated plugin 必須接收和主要 plugin 完全一致的環境變數參數，以及主要 plugin 透過 stdin 接收到的完整 execute configuration。<br>如果執行成功則回傳 0，並透過 stdout 返回 Success result type output。</p><h3 id="Delegated-plugin-execution-procedure"><a href="#Delegated-plugin-execution-procedure" class="headerlink" title="Delegated plugin execution procedure"></a>Delegated plugin execution procedure</h3><ul><li><p>當 CNI plugin 執行 delegated plugin 時:</p><ul><li>在 <code>CNI_PATH</code> 路徑下搜尋 plugin 的可執行程式</li><li>使用 CNI plugin 的環境變數參數和 execute configuration，作為 delegated plugin 的輸入</li><li>確保 delegated plugin 的 stderr 會輸出到 CNI plugin 的 stderr</li></ul></li><li><p>當 plugin 執行 delete 和 check 時，必須執行所有的 delegated plugin，並將 delegated plugin 的錯誤回傳給 runtime</p></li><li><p>當 ADD 失敗時，plugin 應該在回傳錯誤前，先執行 delegated plugin 的 <code>DEL</code></p></li></ul><h2 id="Section-5-Result-Types"><a href="#Section-5-Result-Types" class="headerlink" title="Section 5: Result Types"></a>Section 5: Result Types</h2><ul><li>Plugin 的回傳結果使用 JSON 格式，並有三種<ul><li>Success</li><li>Error</li><li>Version</li></ul></li></ul><h3 id="Success"><a href="#Success" class="headerlink" title="Success"></a>Success</h3><ul><li>如果 plugin 的輸入包含 <code>prevResult</code>，輸出必須包含該欄位的值，並加上該 plugin 對網路修改的資訊，如果該 plugin 沒有任何操作，則該欄位必須保持原輸入內容。</li></ul><p>Success Type Result 的欄位如下</p><ul><li><p>cniVersion: 同輸入的 <code>cniVersion</code> 版本</p></li><li><p>interfaces: 一個該 plugin 建立的 interface 資訊的陣列，包含 host 的 interface。</p><ul><li>name: interface 的名子</li><li>mac: interface 的 mac address</li><li>sandbox: 該介面的網路環境參考，例如 network namespace 的路徑，如果是 host 介面則該欄位為空，容器內的介面該值應為 <code>CNI_NETNS</code></li></ul></li><li><p>ips: 該 plugin 指定的 ip</p><ul><li>address: CIDR 格式的 ip address (e.g. 192.168.1.1&#x2F;24)</li><li>gateway: default gateway (如果存在的話)</li><li>interface: 介面的 index，對應到前述 interfaces 陣列</li></ul></li><li><p>routes: plugin 建立的 routing rules</p><ul><li>dst: route 的目的 (CIDR)</li><li>gw: nexthop 地址</li></ul></li><li><p>dns</p><ul><li>nameservers: DNS server 位置陣列 (ipv4 或 ipv6 格式)</li><li>domain: DNS 搜尋的 local domain</li><li>search (list of strings): 有優先度的 search domain</li><li>options (list of strings): 其他給 dns resolver 的參數</li></ul></li><li><p>Delgated plugin 可能會忽略不需要的欄位</p><ul><li>IPAM 必須回傳一個 abbreviated Success Type result (忽略 interfaces 和 ips 裡面的 interface 欄位)</li></ul></li></ul><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><p>plugin 回傳的錯誤資訊，欄位有 <code>cniVersion</code>, <code>code</code>, <code>msg</code>, <code>details</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#123;<br>  &quot;cniVersion&quot;: &quot;1.0.0&quot;,<br>  &quot;code&quot;: 7,<br>  &quot;msg&quot;: &quot;Invalid Configuration&quot;,<br>  &quot;details&quot;: &quot;Network 192.168.0.0/31 too small to allocate from.&quot;<br>&#125;<br></code></pre></td></tr></table></figure><p>Error code 0-99 被保留給通用的錯誤，100 以上是 plugin 自定義的錯誤。</p><table><thead><tr><th>Error code</th><th>描述</th></tr></thead><tbody><tr><td>1</td><td>不支援的 CNI 版本</td></tr><tr><td>2</td><td>不支援的 network configuration 欄位，error message 會包含不支援的欄位名稱和數值</td></tr><tr><td>3</td><td>未知或不存在的容器，這個錯誤同時代表 runtime 不需要執行 DEL 之類的清理操作</td></tr><tr><td>4</td><td>無效的環境環境變數參數，error message 會包含無效的欄位名稱</td></tr><tr><td>5</td><td>IO 錯誤，例如無法讀取 stdin 的 execute configuration</td></tr><tr><td>6</td><td>解析錯誤，例如無效的 execute configuration JSON 格式</td></tr><tr><td>7</td><td>無效的 network configuration</td></tr><tr><td>11</td><td>稍後在嘗試，存在暫時無法操作的資源，runtime 應該稍後重試</td></tr></tbody></table><ul><li>此外，stderr 也可被用於非 JSON 結構的錯誤訊息，如 log</li></ul><h3 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h3><ul><li>Version Type Result 的欄位如下<ul><li>cniVersion: 同輸入的 <code>cniVersion</code> 版本</li><li>supportedVersions: 一個支援的 CNI 版本陣列</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&#123;<br>    &quot;cniVersion&quot;: &quot;1.0.0&quot;,<br>    &quot;supportedVersions&quot;: [ &quot;0.1.0&quot;, &quot;0.2.0&quot;, &quot;0.3.0&quot;, &quot;0.3.1&quot;, &quot;0.4.0&quot;, &quot;1.0.0&quot; ]<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix:"></a>Appendix:</h2><p>在 CNI SPEC 裏面包含了一些 configuration 和執行過程中 plugin 輸入輸出的範例，可以參考 <a href="https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md#appendix-examples">原始文件</a><br>另外還有一份 CNI 的文件 <a href="https://github.com/containernetworking/cni/blob/main/CONVENTIONS.md">CONVENTIONS</a>，描述許多 spec 裡面沒有定義但是許多 plugin 常用的欄位。</p><h2 id="小節"><a href="#小節" class="headerlink" title="小節"></a>小節</h2><p>以上是對 <a href="https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md">CNI spec 1.0.0</a> 的導讀，希望對大家有幫助。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;這次來嘗試寫寫看 spec 導讀，今天要講的是 &lt;a href=&quot;https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md&quot;&gt;Container Network Interface (CNI) Specification&lt;/a&gt;。CNI 是 &lt;a href=&quot;https://cncf.io/&quot;&gt;CNCF&lt;/a&gt; 的一個專案，這個專案包含了今天要講的 CNI SPEC 以及基於這個 SPEC 開發出來 libraries 還有一系列的 CNI plugins。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://blog.louisif.me/categories/Kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>ONOS P4 Switch Pipeconf 開發</title>
    <link href="https://blog.louisif.me/ONOS/ONOS-P4-Switch-Pipeconf-Development/"/>
    <id>https://blog.louisif.me/ONOS/ONOS-P4-Switch-Pipeconf-Development/</id>
    <published>2022-08-27T16:25:46.000Z</published>
    <updated>2022-08-30T07:44:07.557Z</updated>
    
    <content type="html"><![CDATA[<p>這篇文章會介紹一下，在 ONOS 上 P4 相關的模組和功能，以及怎麼開發 ONOS APP 與設置 ONOS，讓 ONOS 可以控制 p4 switch 的 pipeline。</p><span id="more"></span> <h2 id="背景介紹"><a href="#背景介紹" class="headerlink" title="背景介紹"></a>背景介紹</h2><h3 id="P4Runtime"><a href="#P4Runtime" class="headerlink" title="P4Runtime"></a>P4Runtime</h3><p>P4Runtime 是 P4 API Working Group 制定的一套基於 Protobuf 以及 gRPC 的傳輸協定，他可以提供不同的 P4 交換機和不同的 SDN 控制器一套統一的 API 標準，提供控制器直接透過 p4runtime 將編譯出來 p4 pipeline 直接上傳到 p4 switch 上、設置 p4 pipeline 的 table entry 以及接收 packet-in 的封包和 counter 的資訊等功能。</p><h3 id="ONOS-架構"><a href="#ONOS-架構" class="headerlink" title="ONOS 架構"></a>ONOS 架構</h3><p><img src="/ONOS/ONOS-P4-Switch-Pipeconf-Development/ONOS_Architecture.png" alt="ONOS 架構圖"></p><p>ONOS 使用分層架構和介面抽象的方式隱藏了底層交換機和和控制協定的具體內容，讓上層的應用可以用統一的 API 來管理網路的行為，因此上層網路可以用完全相同的方式來控制 Openflow switch 及 P4 switch，也可以不用理會各個 switch table 順序和具體規則的下達方式。</p><h3 id="ONOS-Flow-programmable"><a href="#ONOS-Flow-programmable" class="headerlink" title="ONOS Flow programmable"></a>ONOS Flow programmable</h3><p><img src="/ONOS/ONOS-P4-Switch-Pipeconf-Development/ONOS_FlowRule_Abstract.png" alt="ONOS FlowRule 抽象架構"></p><p>即便是在北向提供給使用者的 API，ONOS 也對其做了很多層級的抽象，讓使用者可以自由決定要使用哪一個層級的 API。</p><p>在 flow rule 的部分，ONOS 大致上提供了三個層級的抽象，分別是 <code>Flow Rule</code>, <code>Flow Objective</code> 及 <code>Intent</code> ，不論在哪一個層級，我們主要都是要操作兩個集合 <code>Selector</code> 和 <code>Treatment</code></p><p><code>Selector</code> 決定了哪些封包受這條 flow rule 管理。一個 <code>Selector</code> 包含了若干個 <code>Criterion</code> ，ONOS 透過 <code>Enum</code> 定義了常用的 Criterion Type，來對應封包的不同欄位，例如 <code>IPV4_SRC</code> , <code>ETH_SRC</code> 等。</p><p><code>Treatment</code> 則是 <code>Instruction</code> 的集合，一個 instruction 通常對應到對封包的某個欄位進行修改，或著指定封包在交換機的 output port。</p><p>三個抽象層積的差別在於這兩個集合套用到的對象，在最高層級的 <code>Intent</code> ，我們操作的對象是整個網路流，除了 <code>Selector</code> 和 <code>Treatment</code>，我們還要定義整個網路流在 SDN 網路的入口 (Ingress Point) 和出口 (Egress Point)，ONOS 核心的 <code>Intent Service</code> 會幫我們把一個 <code>Intent</code> 編譯成多個 <code>FlowObjective</code>。</p><p>由於 <code>Intent</code> 操作的是整個網路流，在這個層級定義 Output port 是沒有意義的，但是由於在 ONOS 使用的 JAVA API 是共通的，所以 intent service 會忽略掉這個 instruction，這個在 ONOS 的實作上是很重要的觀念，對 treatment 裡的 instructions，底層的編譯器只會取他可以處理的 instructions 往更底層送，對於不可以處理的 instructions，有些會有 warnning log，有些會直接跳 exception，更有的會直接忽略，因此如果 selector 和 treatment 的執行結果不符合我們的預期，有可能是有些不支援的 instruction 在轉換成交換機可以懂得規則的過程中被忽略的。</p><p><code>FlowObjective</code> 操作的對象是一台網路設備 (通常是一台交換機)，同樣我們定義一個 <code>Selector</code> 和 <code>Treatment</code> ，告訴這台交換機我們要處理哪些封包和怎麼處理。</p><p>最底下到了 Flow Rule 這個層級，Flow rule 這個層級對象是交換機上的一張 table，因此他加入了 table id 這個欄位。一個 <code>FlowObjective</code> 可能會包含多個不同的 instruction，例如我們要修改封包的 mac address，修改 ip 的 ttl 欄位，同時也要決定這個封包的 output port，這些 instruction 在 flow objective 層級可以包含在一個 treatment 內，但是在實際的交換機上這些 instruction 可能分別屬於不同的 table，因此一個 flow objective 會需要對應到一條或多條得 flow rule，這依據底下交換機的不同、傳輸協定的不同而不同，因此 ONOS 引入了 <code>Pipeliner</code> ，Driver 可以實作 <code>Pipeliner</code> 的介面，讓 ONOS 知道如何把 flow objective 轉換成 flow rules。</p><h3 id="P4-in-ONOS"><a href="#P4-in-ONOS" class="headerlink" title="P4 in ONOS"></a>P4 in ONOS</h3><p><img src="/ONOS/ONOS-P4-Switch-Pipeconf-Development/ONOS_P4_Architecture.png" alt="ONOS P4 南向介面架構"></p><p>上圖是 p4 在 ONOS 南向架構上的組件</p><p>在 Protocol layer 由 <code>P4Runtime</code> 組件實作 p4runtime procotol，維護 switch 的連線和具體的 gRPC&#x2F;Protobuf 傳輸內容</p><p>接著是 Driver layer，不同的 p4 switch 在 pipeline 的結構等方面存在差異，因此在 ONOS 設定交換機資訊時要根據不同的 Switch 選擇 driver</p><ul><li>如果是 bmv2 switch 使用 <code>org.onosproject.bmv2</code>，如果是 tofino 交換機使用 <code>org.onosproject.barefoot</code></li></ul><p>最上面是 ONOS 核心，這邊有 translation services 和 pipeconf 架構。p4 交換機的特色是能透過 p4lang 定義出完全不同的 pipeline，在使用 ONOS 控制 p4 switch 的時候，我們就需要針對 pipeline 定義註冊 <code>pipeconf</code>，ONOS 核心可以調用 pipeconf 將 flow objective 或 flow rule 的 flow rule 轉換成真正可以下達到 p4 pipeline table 上的 entry。<br>在 ONOS 核心定義的這套 pipeconf 及轉換架構被稱之為 Pipeline Independent (PI) framework，因此 ONOS 相關的 class 和 interface 都會有一個 PI 的前綴。</p><p><img src="/ONOS/ONOS-P4-Switch-Pipeconf-Development/P4_APP_Compare.png" alt="P4 FlowRule 設置流程"></p><p>另外就要提到 Pipeline-agnostic APP 和 Pipeline-aware APP 的差別，這邊指的都是北向介面上面處理網路邏輯的 APP，差別在於 Pipeline-agnostic app 完全不考慮底下的 pipeline，因此通常操作的是 flow objective，而 pipeline aware app 必須知道底層 pipeline 的架構，直接產出特定的 flow rule。</p><p>開發 Pipeline-agnostic APP 的好處是他足夠抽象因此可以應用在各種不同的交換機和網路，但是我們就需要額外實作 pipeliner 等編譯器來做轉換，因此直接如果只針對單一 pipeline 的情況下，直接開發 pipeline aware app 會比較簡單。</p><h2 id="Pipeconf-開發"><a href="#Pipeconf-開發" class="headerlink" title="Pipeconf 開發"></a>Pipeconf 開發</h2><p>在使用 ONOS 控制 p4 switch 時，最基本要做的就是撰寫 pipeline 對應的 pipeconf。一個完整的 Pipeconf 會包含</p><ul><li><p>從 p4 compiler 拿到 pipeline 資訊檔案，p4info, bmv2 json, Tofino binary….</p></li><li><p>PipeconfLoader: 一個 pipeconf 的進入點，向 PiPipeconfService 註冊一個或多個 pipeconf，定義 pipeconf 的 id, 對應的 interpreter, pipeliner, p4info 檔案路徑等資訊。</p></li><li><p>Interpreter: 主要負責兩件事</p><ul><li>提供 ONOS 核心資訊並協助將 common flow rule 轉換成 protocol independent flow rule，包含了 table id 的 mapping, 欄位名稱和數值的轉換等</li><li>處理 packet-in&#x2F;packet-out 的封包，當封包從 p4 switch packet-in 到 controller 時，會把 metadata (input port 等資訊) 當作 packet 的一個 header 附加在 packet 中，一起送到 controller，pipeconf 需要解析封包，將封包資訊提取出來。當封包 packet-out 時，同樣需要 metadata 轉換成 pipeline 定義的 packet-out header，附加在封包內送至交換機，pipeline parser 才能重新將資訊解析出來處理。</li></ul></li><li><p>Pipeliner: 負責將 flow objective 轉換成 flow rule</p></li></ul><p>但是 Interpreter 和 Pipeliner 的功能是不一定要實作的，如果對應的功能沒有被實作，那北向的 APP 就只能呼叫比較底層的 API 而無法調動 flow objective 等功能。</p><h3 id="開發目標"><a href="#開發目標" class="headerlink" title="開發目標"></a>開發目標</h3><p>下面會用一個非常簡單的 p4 pipeline 作為範例，我們預期要實作一個基本的 Layer 2 switch pipeline，使 ProxyARP APP 和 Reactive Forwarding APP 能夠正常的運作。(使用 bmv2 和 ONOS v2.7.0 開發)</p><h3 id="建立-ONOS-APP"><a href="#建立-ONOS-APP" class="headerlink" title="建立 ONOS APP"></a>建立 ONOS APP</h3><p>跟任何其他 ONOS 模組一樣，pipeconf 可以作為一個獨立的 ONOS APP 開發，再安裝到 ONOS 上，所以首先建立我們的 <code>simepleswitch</code> APP</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">onos-create-app<br>Define value for property &#x27;groupId&#x27;: me.louisif.pipelines<br>Define value for property &#x27;artifactId&#x27;: simpleswitch<br>Define value for property &#x27;version&#x27; 1.0-SNAPSHOT: :<br>Define value for property &#x27;package&#x27; me.louisif: : me.louisif.pipelines.simpleswitch<br></code></pre></td></tr></table></figure><h3 id="pom-xml"><a href="#pom-xml" class="headerlink" title="pom.xml"></a>pom.xml</h3><p>在 pipeconf 的開發中，使用到 p4 相關的 api 並沒有被包含在 onos 標準 api 內，需要額外加入 p4runtime-model 這個 dependency。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.onosproject<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>onos-protocols-p4runtime-model<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;onos.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure><p>另外可以在 onos 的 dependency app 列表內加入 pipeline 對應的 driver，這樣啟動 pipeconf 時就會自動啟用相關的 driver app，而不用事前手動啟動。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">onos.app.requires</span>&gt;</span>org.onosproject.drivers.bmv2<span class="hljs-tag">&lt;/<span class="hljs-name">onos.app.requires</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="P4-撰寫"><a href="#P4-撰寫" class="headerlink" title="P4 撰寫"></a>P4 撰寫</h3><p>接著撰寫我們的 p4 檔案，路徑是 src&#x2F;main&#x2F;resource&#x2F;simpleswitch.p4。resource 資料夾在編譯的時候會被附加到編譯出來的 oar 裡面，所以可以直接在 ONOS 執行的時候存取到編譯出來的 p4info 等檔案。完整的檔案在 <a href="https://github.com/gamerslouis/onos-p4-tutorial">github</a> 上。</p><p>在 simpleswitch 的 ingress pipeline 內只包含一張 table0，用於 L2 的 packet forwarding，使用 send 這個 action 來將封包丟到指定的 output port，在 bmv2 switch 會定義一個 cpu port，當 egress_port 為該 port number 時，封包就會被送至 ONOS，因此 send_to_cpu 這個 action 只單純做 set egress port 這個動作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs c">control <span class="hljs-title function_">table0_control</span><span class="hljs-params">(inout <span class="hljs-type">headers_t</span> hdr,</span><br><span class="hljs-params">                inout <span class="hljs-type">local_metadata_t</span> local_metadata,</span><br><span class="hljs-params">                inout <span class="hljs-type">standard_metadata_t</span> standard_metadata)</span> &#123;<br>    action <span class="hljs-title function_">send</span><span class="hljs-params">(<span class="hljs-type">port_t</span> port)</span> &#123;<br>        standard_metadata.egress_spec = port;<br>    &#125;<br><br>    action <span class="hljs-title function_">send_to_cpu</span><span class="hljs-params">()</span> &#123;<br>        standard_metadata.egress_spec = CPU_PORT;<br>    &#125;<br><br>    action <span class="hljs-title function_">drop</span><span class="hljs-params">()</span> &#123;<br>        mark_to_drop (standard_metadata);<br>    &#125;<br><br>    table table0 &#123;<br>        key = &#123;<br>            standard_metadata.ingress_port : ternary;<br>            hdr.ethernet.src_addr          : ternary;<br>            hdr.ethernet.dst_addr          : ternary;<br>            hdr.ethernet.ether_type        : ternary;<br>        &#125;<br><br>        actions = &#123;<br>            send;<br>            send_to_cpu;<br>            drop;<br>        &#125;<br>        default_action = drop;<br>        size = <span class="hljs-number">512</span>;<br>    &#125;<br><br>    apply &#123;<br>        table0.apply ();<br>    &#125;<br>&#125;<br>control <span class="hljs-title function_">MyIngress</span><span class="hljs-params">(inout <span class="hljs-type">headers_t</span> hdr,</span><br><span class="hljs-params">                  inout <span class="hljs-type">local_metadata_t</span> meta,</span><br><span class="hljs-params">                  inout <span class="hljs-type">standard_metadata_t</span> standard_metadata)</span> &#123;<br>    apply &#123;<br>        <span class="hljs-comment">// 這個後面在介紹 </span><br>        <span class="hljs-comment">//if (standard_metadata.ingress_port == CPU_PORT) &#123;</span><br>        <span class="hljs-comment">//    standard_metadata.egress_spec = hdr.packet_out.egress_port;</span><br>        <span class="hljs-comment">//    hdr.packet_out.setInvalid ();</span><br>        <span class="hljs-comment">//    exit;</span><br>        <span class="hljs-comment">// &#125; else &#123;</span><br>            table0_control.apply (hdr, meta, standard_metadata);<br>        <span class="hljs-comment">// &#125;</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="P4-編譯"><a href="#P4-編譯" class="headerlink" title="P4 編譯"></a>P4 編譯</h3><p>編譯 bmv2 pipeline 可以直接複製 ONOS 內建的 basic pipeline 使用的 <a href="https://github.com/opennetworkinglab/onos/tree/master/pipelines/basic/src/main/resources">編譯腳本</a>，將 Makefile 和 bmv2-compile.sh 這兩個檔案複製到 resources 資料夾下，然後修改 Makefile 把 basic 改成 simpleswitch，並刪除 int pipeline。可以簡單下 <code>make</code> 來完成 pipeline 的編譯。</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs makefile">ROOT_DIR:=<span class="hljs-variable">$(<span class="hljs-built_in">shell</span> dirname $(<span class="hljs-built_in">realpath</span> $(<span class="hljs-built_in">firstword</span> <span class="hljs-variable">$(MAKEFILE_LIST)</span>)</span>))/..<br><br><span class="hljs-section">all: p4 constants</span><br><br><span class="hljs-section">p4: simpleswitch.p4</span><br>        @./bmv2-compile.sh <span class="hljs-string">&quot;simpleswitch&quot;</span> <span class="hljs-string">&quot;&quot;</span><br><br><span class="hljs-section">constants:</span><br>        docker run -v <span class="hljs-variable">$(ONOS_ROOT)</span>:/onos -w /onos/tools/dev/bin \<br>                -v <span class="hljs-variable">$(ROOT_DIR)</span>:/source \<br>                --entrypoint ./onos-gen-p4-constants opennetworking/p4mn:stable \<br>                -o /source/java/me/louisif/pipelines/simpleswitch/SimpleswitchConstants.java \<br>                simpleswitch /source/resources/p4c-out/bmv2/simpleswitch_p4info.txt<br><br><span class="hljs-section">clean:</span><br>        rm -rf p4c-out/bmv2/*<br></code></pre></td></tr></table></figure><p>這個 makefile 主要分為兩個部分。p4 會呼叫 bmv2-compile，編譯出 bmv2 的 p4info 和描述 pipeline 的 json 檔案。constants 則會使用 p4mn 這個 container 生成出一個 SimpleswitchConstants.java 的檔案，這個檔案會把 pipeline 所有 table 名稱、欄位、action 名稱列舉出來，方便 pipeconf 的程式碼直接調用，以 table0 來說，它的完整名稱為 <code>MyIngress.table0_control.table0</code> ，可以使用 MY_INGRESS_TABLE0_CONTROL_TABLE0 變數來代表。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">PiTableId</span> <span class="hljs-variable">MY_INGRESS_TABLE0_CONTROL_TABLE0</span> <span class="hljs-operator">=</span><br>            PiTableId.of (<span class="hljs-string">&quot;MyIngress.table0_control.table0&quot;</span>);<br></code></pre></td></tr></table></figure><h2 id="最小可執行-Pipeconf"><a href="#最小可執行-Pipeconf" class="headerlink" title="最小可執行 Pipeconf"></a>最小可執行 Pipeconf</h2><h3 id="撰寫-PipeconfLoader"><a href="#撰寫-PipeconfLoader" class="headerlink" title="撰寫 PipeconfLoader"></a>撰寫 PipeconfLoader</h3><p>接著我們要先寫一個最小可以動的 pipeconf，只包含 PipeconfLoader.java 這個檔案，路徑是 src&#x2F;main&#x2F;java&#x2F;me&#x2F;louisif&#x2F;simpleswitch&#x2F;PipeconfLoader.java，前文提到 PipeconfLoader 的工作是向 <code>PipeconfService</code> 註冊 pipeconf 的資訊，因此我們幫 PipeconfLoader 加上 <code>Component</code> 的 Annotation，讓 activate function 在 APP 啟動時被呼叫，接著在 activate function 裡面去註冊 pipeconf。</p><p>以我們的例子而言，我們使用的是 bmv2 的 pipeline，所以我們可以這樣寫</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-type">URL</span> <span class="hljs-variable">jsonUrl</span> <span class="hljs-operator">=</span> PipeconfLoader.class.getResource (<span class="hljs-string">&quot;/p4c-out/bmv2/simpleswitch.json&quot;</span>);<br><span class="hljs-keyword">final</span> <span class="hljs-type">URL</span> <span class="hljs-variable">p4InfoUrl</span> <span class="hljs-operator">=</span> PipeconfLoader.class.getResource (<span class="hljs-string">&quot;/p4c-out/bmv2/simpleswitch_p4info.txt&quot;</span>);<br><br><span class="hljs-type">PiPipeconf</span> <span class="hljs-variable">pipeconf</span> <span class="hljs-operator">=</span> DefaultPiPipeconf.builder ()<br>        .withId (PIPECONF_ID)<br>        .withPipelineModel (P4InfoParser.parse (p4InfoUrl))<br>        .addExtension (P4_INFO_TEXT, p4InfoUrl)<br>        .addExtension (BMV2_JSON, jsonUrl)<br>        .build ();<br>piPipeconfService.register (pipeconf);<br></code></pre></td></tr></table></figure><p>檔案路徑要填寫相對於 resource 這個資料夾的路徑，另外 <code>addExtension</code> 的內容會根據 switch 的不同而不同，如果我們今天使用的是 tonifo 的 pipeline 那就要改成</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-type">URL</span> <span class="hljs-variable">binUrl</span> <span class="hljs-operator">=</span> PipeconfLoader.class.getResource (<span class="hljs-string">&quot;/p4c-out/tofino.bin&quot;</span>);<br><span class="hljs-keyword">final</span> <span class="hljs-type">URL</span> <span class="hljs-variable">p4InfoUrl</span> <span class="hljs-operator">=</span> PipeconfLoader.class.getResource (<span class="hljs-string">&quot;/p4c-out/p4info.txt&quot;</span>);<br><span class="hljs-keyword">final</span> <span class="hljs-type">URL</span> <span class="hljs-variable">contextJsonUrl</span> <span class="hljs-operator">=</span> PipeconfLoader.class.getResource (<span class="hljs-string">&quot;/p4c-out/context.json&quot;</span>);<br><br>DefaultPiPipeconf.builder ()<br>                .withId (PIPECONF_ID)<br>                .withPipelineModel (parseP4Info (p4InfoUrl))<br>.addExtension (P4_INFO_TEXT, p4InfoUrl)<br>                .addExtension (TOFINO_BIN, binUrl)<br>                .addExtension (TOFINO_CONTEXT_JSON, contextJsonUrl)<br>                .build ();<br>piPipeconfService.register (pipeconf);<br></code></pre></td></tr></table></figure><h3 id="測試"><a href="#測試" class="headerlink" title="測試"></a>測試</h3><p>完成 PipeconfLoader.java 就構成了一個可以運作的 pipeconf，為了測試我們使用 opennetworking&#x2F;p4mn 這個 container 來實驗，p4mn 是一個 mininet 的 docker image，可以很簡單的啟動一個 mininet 的測試拓譜，並使用 bmv2 switch 取代 mininet 原本使用的 openflow switch。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">docker run -v /tmp/p4mn:/tmp --privileged --rm -it -p <span class="hljs-number">50001</span>:<span class="hljs-number">50001</span> opennetworking/p4mn:stable<br></code></pre></td></tr></table></figure><p>使用這個指令可以啟動一個包含一個叫做 bmv2-s1 的 bmv2 switch 和兩個 host</p><p><img src="/ONOS/ONOS-P4-Switch-Pipeconf-Development/p4mn_topo.png" alt="P4mn 拓譜"></p><p>同時會生成一個 onos 的 netcfg 檔案，路徑 &#x2F;tmp&#x2F;p4mn&#x2F;bmv2-s1-netcfg.json</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs java">&#123;<br>    <span class="hljs-string">&quot;devices&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;device:bmv2:s1&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;ports&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;1&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;s1-eth1&quot;</span>,<br>                    <span class="hljs-string">&quot;speed&quot;</span>: <span class="hljs-number">10000</span>,<br>                    <span class="hljs-string">&quot;enabled&quot;</span>: <span class="hljs-literal">true</span>,<br>                    <span class="hljs-string">&quot;number&quot;</span>: <span class="hljs-number">1</span>,<br>                    <span class="hljs-string">&quot;removed&quot;</span>: <span class="hljs-literal">false</span>,<br>                    <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;copper&quot;</span><br>                &#125;,<br>                <span class="hljs-string">&quot;2&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;s1-eth2&quot;</span>,<br>                    <span class="hljs-string">&quot;speed&quot;</span>: <span class="hljs-number">10000</span>,<br>                    <span class="hljs-string">&quot;enabled&quot;</span>: <span class="hljs-literal">true</span>,<br>                    <span class="hljs-string">&quot;number&quot;</span>: <span class="hljs-number">2</span>,<br>                    <span class="hljs-string">&quot;removed&quot;</span>: <span class="hljs-literal">false</span>,<br>                    <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;copper&quot;</span><br>                &#125;<br>            &#125;,<br>            <span class="hljs-string">&quot;basic&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;managementAddress&quot;</span>: <span class="hljs-string">&quot;grpc://localhost:50001?device_id=1&quot;</span>,<br>                <span class="hljs-string">&quot;driver&quot;</span>: <span class="hljs-string">&quot;bmv2&quot;</span>,<br>                <span class="hljs-string">&quot;pipeconf&quot;</span>: <span class="hljs-string">&quot;me.louisif.pipelines.simpleswitch&quot;</span><br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>這個檔案包含了 ONOS P4 需要的所有資訊，主要分為兩個部分，ports 定義了這個交換機所有的 port 資訊。basic 部分，managementAddress 是 ONOS 用來使用 p4runtime 連線到 switch 的路徑，pipeconf 指定了這個 switch 使用的 pipeconf，預設會是 org.onosproject.pipelines.basic，在我們的範例中需要改為 me.louisif.pipelines.simpleswitch。</p><p>只要將 me.louisif.pipelines.simpleswitch 的 app 啟用，並上傳 bmv2-s1-netcfg.json，ONOS 就可以成功連線到這個 bmv2 switch，並正常提供北向的 APP 服務了。</p><h3 id="如何下-flow-rule"><a href="#如何下-flow-rule" class="headerlink" title="如何下 flow rule"></a>如何下 flow rule</h3><p>完成 pipeconf 後，我們就可以透過下 flow rule 的方式來讓 switch 工作了，為了要讓 h1 和 h2 能夠互相溝通，最簡單的方法就是將所有 port 1 進來的封包送到 port 2、所有從 port 2 進來的封包送到 port 1。</p><p>為此我們需要兩條 table0 的 entry，分別是</p><ul><li>standard_metadata.ingress_port &#x3D;&#x3D; 1 (mask 0x1ff) → send port&#x3D;2</li><li>standard_metadata.ingress_port &#x3D;&#x3D; 2 (mask 0x1ff) → send port&#x3D;1</li></ul><p>由於 standard_metadata.ingress_port 這個 key 是 ternary，因此我們需要包含 mask, port 這個 type 的長度是 9 bits，因為要完全一致，所以 mask 是 0x1ff。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">final</span> PiCriterion.<span class="hljs-type">Builder</span> <span class="hljs-variable">criterionBuilder</span> <span class="hljs-operator">=</span> PiCriterion.builder ()<br>                .matchTernary (PiMatchFieldId.of (<span class="hljs-string">&quot;standard_metadata.ingress_port&quot;</span>), inPortNumber.toLong (), <span class="hljs-number">0x1ff</span>);<br><span class="hljs-keyword">final</span> <span class="hljs-type">PiAction</span> <span class="hljs-variable">piAction</span> <span class="hljs-operator">=</span> PiAction.builder ().withId (PiActionId.of (<span class="hljs-string">&quot;MyIngress.table0_control.send&quot;</span>))<br>        .withParameter (<span class="hljs-keyword">new</span> <span class="hljs-title class_">PiActionParam</span>(PiActionParamId.of (<span class="hljs-string">&quot;port&quot;</span>), outPortNumber.toLong ()))<br>        .build ();<br><span class="hljs-keyword">final</span> <span class="hljs-type">FlowRule</span> <span class="hljs-variable">flowRule</span> <span class="hljs-operator">=</span> DefaultFlowRule.builder ()<br>        .fromApp (coreService.getAppId (APP_NAME))<br>        .forDevice (deviceId)<br>        .forTable (PiTableId.of (<span class="hljs-string">&quot;MyIngress.table0_control.table0&quot;</span>)).makePermanent ().withPriority (<span class="hljs-number">65535</span>)<br>        .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ())<br>        .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();<br>flowRuleService.applyFlowRules (flowRule);<br></code></pre></td></tr></table></figure><p>我們可以透過這樣的方式來下達第一條 table entry，可以發現和平常的 flow entry 不一樣的地方是 table id 使用的是 PiTableId 這個 type，並指定了 table0 的完整 id <code>MyIngress.table0_control.table0</code>，另外 <code>Selector</code> 和 <code>Treatment</code> 分別使用了 matchPi 和 piTableAction 這兩個特別的函數。</p><p>我們這邊將只使用 PiTableId, PiCriterion 和 PiAction 定義的 flow rule 稱之為 PI flow rule，PI flow rule 是 onos 的 p4runtime 可以直接處理的 flow rule，所有的欄位名稱都唯一對應到 p4 pipeline 的某個欄位，前面提到這些 PiTableId, PiMatchFieldId 等都會在 SimpleswitchConstants.java 內被定義，因此可以直接使用來縮短程式碼長度。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> <span class="hljs-keyword">static</span> me.louisif.pipelines.simpleswitch.SimpleswitchConstants.*;<br><br><span class="hljs-keyword">final</span> PiCriterion.<span class="hljs-type">Builder</span> <span class="hljs-variable">criterionBuilder</span> <span class="hljs-operator">=</span> PiCriterion.builder ()<br>                .matchTernary (INGRESS_PORT, inPortNumber.toLong (), <span class="hljs-number">0x1ff</span>);<br><span class="hljs-keyword">final</span> <span class="hljs-type">PiAction</span> <span class="hljs-variable">piAction</span> <span class="hljs-operator">=</span> PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND)<br>        .withParameter (PORT, outPortNumber.toLong ()))<br>        .build ();<br><span class="hljs-keyword">final</span> <span class="hljs-type">FlowRule</span> <span class="hljs-variable">flowRule</span> <span class="hljs-operator">=</span> DefaultFlowRule.builder ()<br>        .fromApp (coreService.getAppId (APP_NAME))<br>        .forDevice (deviceId)<br>        .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0).makePermanent ().withPriority (<span class="hljs-number">65535</span>)<br>        .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ())<br>        .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();<br>flowRuleService.applyFlowRules (flowRule);<br></code></pre></td></tr></table></figure><blockquote><p>在範例程式碼裡面包含了簡單的 cli 指令實作，因此可以在 ONOS CLI 使用 <code>add-pi-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt;</code> 的方式來下達上面的 flow rule。詳情可以 <a href="https://github.com/gamerslouis/onos-p4-tutorial/blob/master/simpleswitch/src/main/java/me/louisif/pipelines/simpleswitch/cli/AddPiFlowRule.java">參考檔案</a>。</p></blockquote><p>當然這樣編寫出來的 flow rule 會產生一個問題，如果相同的 pipeline，我們希望能從 bmv2 移植到 tonifo 上面去使用，由於欄位的名稱會存在差異，因此所有下達 flow rule 的 APP 都需要重新寫，顯然這樣不是一個很好的做法，增加了程式碼維護上的困難，因此 ONOS 加入前面提到的 Interpreter 還有 translator 機制，下一節會介紹如何為 pipeline 編寫 Interpreter 還有用比較通用的方法來下 flow rule，在完成 interpreter 後上述的 flow rule，可以用下面這個我們比較熟悉的方法來下達。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">TABLE0_TABLE_ID</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">final</span> <span class="hljs-type">FlowRule</span> <span class="hljs-variable">flowRule</span> <span class="hljs-operator">=</span> DefaultFlowRule.builder ()<br>  .fromApp (coreService.getAppId (APP_NAME))<br>  .forDevice (deviceId)<br>  .forTable (TABLE0_TABLE_ID).makePermanent ().withPriority (<span class="hljs-number">65535</span>)<br>  .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ())<br>  .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ())<br>  .build ();<br><br>flowRuleService.applyFlowRules (flowRule);<br></code></pre></td></tr></table></figure><h2 id="撰寫-Interpreter"><a href="#撰寫-Interpreter" class="headerlink" title="撰寫 Interpreter"></a>撰寫 Interpreter</h2><p>在前一節我們完成了基本的 pipeconf 註冊，並使用 PI flow rule 的方式來控制交換機，但是直接使用 PI flow rule 會降低 APP 的彈性，使移植到不同 pipeline 的困難度提高。另外 SDN 的一個特色是可以使用 packet-in&#x2F;packet-out 的方式來讓 controller 即時性的處理封包，因此本結會介紹如何實作 Interpreter 來提供 packet-in&#x2F;packet-out，以及 flow rule translation 的功能。</p><p>要提供一個 pipeline interpreter，我們需要實作 PiPipelineInterpreter 這個介面，要注意的是 Interpreter class 需要繼承 AbstractHandlerBehaviour，然後再 PipeconfLoader 去指定這個實作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleSwitchInterpreterImpl</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">AbstractHandlerBehaviour</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">PiPipelineInterpreter</span> &#123;<br>&#125;<br><br><span class="hljs-type">PiPipeconf</span> <span class="hljs-variable">pipeconf</span> <span class="hljs-operator">=</span> DefaultPiPipeconf.builder ()<br>        .withId (PIPECONF_ID)<br>        .withPipelineModel (P4InfoParser.parse (p4InfoUrl))<br>.addBehaviour (PiPipelineInterpreter.class, SimpleSwitchInterpreterImpl.class)<br>        .addExtension (P4_INFO_TEXT, p4InfoUrl)<br>        .addExtension (BMV2_JSON, jsonUrl)<br>        .build ();<br></code></pre></td></tr></table></figure><p>當一條 flow rule 被加入到 ONOS 時，PI framework 會將其翻譯成 PI flow rule，也就是將非 PI * 的欄位轉換成 PI flow rule 的欄位。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-type">FlowRule</span> <span class="hljs-variable">flowRule</span> <span class="hljs-operator">=</span> DefaultFlowRule.builder ()<br>  .forTable (<span class="hljs-number">0</span>)<br>.withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ())<br>  .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ())<br>  .build ();<br><br></code></pre></td></tr></table></figure><h3 id="Table-Id-translation"><a href="#Table-Id-translation" class="headerlink" title="Table Id translation"></a>Table Id translation</h3><p>以前面提到的 port 1 送到 port 2 的 flow rule 來示範，我們需要把 table id 0，轉換成 <code>PiTableId.of (&quot;MyIngress.table0_control.table0&quot;)</code> ，這對應到 Interpreter 的 mapFlowRuleTableId 函數，我們可以定義一個 map 來記錄，table index 跟 Pi table id 之間的關係，然後實作 mapFlowRuleTableId。這邊的 id 0 並不具有特定的意義，只是單純我們 interpreter 定義的 table index，因此當 APP 使用時，需要知道這個 index 對應到的 table 具體是什麼功能，當然通常我們會再加上一層 pipeliner，透過 flow objective 來隱藏 table id 的細節。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> Map&lt;Integer, PiTableId&gt; TABLE_MAP = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ImmutableMap</span>.Builder&lt;Integer, PiTableId&gt;()<br>    .put (<span class="hljs-number">0</span>, MY_INGRESS_TABLE0_CONTROL_TABLE0)<br>    .build ();<br><br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> Optional&lt;PiTableId&gt; <span class="hljs-title function_">mapFlowRuleTableId</span><span class="hljs-params">(<span class="hljs-type">int</span> flowRuleTableId)</span> &#123;<br>    <span class="hljs-keyword">return</span> Optional.ofNullable (TABLE_MAP.get (flowRuleTableId));<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Selector-Translation"><a href="#Selector-Translation" class="headerlink" title="Selector Translation"></a>Selector Translation</h3><p>我們通常使用 <code>DefaultTrafficSelector.builder</code> 來定義 Selector。 <code>matchInPort</code> 會在 selector 內加入一個 <code>PortCriterion</code> ，他的 criterion tpye 是 <code>Criterion.Type.IN_PORT</code> ，為此 Interpreter 需要根據 Criterion type，將其轉換成 p4 table 對應的 key，同樣我們使用 map 的方式來維護其關係。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> Map&lt;Criterion.Type, PiMatchFieldId&gt; CRITERION_MAP =<br>  <span class="hljs-keyword">new</span> <span class="hljs-title class_">ImmutableMap</span>.Builder&lt;Criterion.Type, PiMatchFieldId&gt;()<br>          .put (Criterion.Type.IN_PORT, HDR_STANDARD_METADATA_INGRESS_PORT)<br>          .put (Criterion.Type.ETH_SRC, HDR_HDR_ETHERNET_SRC_ADDR)<br>          .put (Criterion.Type.ETH_DST, HDR_HDR_ETHERNET_DST_ADDR)<br>          .put (Criterion.Type.ETH_TYPE, HDR_HDR_ETHERNET_ETHER_TYPE)<br>          .build ();<br><br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> Optional&lt;PiMatchFieldId&gt; <span class="hljs-title function_">mapCriterionType</span><span class="hljs-params">(Criterion.Type type)</span> &#123;<br>    <span class="hljs-keyword">return</span> Optional.ofNullable (CRITERION_MAP.get (type));<br>&#125;<br></code></pre></td></tr></table></figure><p>可能有些人會有些疑惑說，p4 table key 可能會是 exact 或是 ternary，那 ONOS 要怎麼把 <code>matchInPort</code> 轉換成 ternary mask 0x1ff </p><p>前面提到在註冊 pipeconf 時，我們透過 <code>.withPipelineModel (P4InfoParser.parse (p4InfoUrl))</code> 載入 p4info (在 ONOS 內稱之為 <code>PiPipelineModel</code>)，每個 key 具體的類型和資料長度會包含在 p4info 內，PI framework 在轉換時會根據不同 criterion type 和 key type 的不同和需求自動做轉換。</p><h3 id="Treatment-Translation"><a href="#Treatment-Translation" class="headerlink" title="Treatment Translation"></a>Treatment Translation</h3><p>不像 table id 和 criterion，在 p4 內，每個 action 是一個可帶參數的 function，和 ONOS 定義的 treatment 不存在簡單的對應關係，因此 interpreter 定義了 mapTreatment，輸入是 treatment 和 table id，輸出是 PiAction，讓 interpreter 完整的處理整個 treatment。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> PiAction <span class="hljs-title function_">mapTreatment</span><span class="hljs-params">(TrafficTreatment treatment, PiTableId piTableId)</span> <span class="hljs-keyword">throws</span> PiInterpreterException &#123;<br>    <span class="hljs-comment">// 檢查 table id 是否有效，由於只有 table0 一張 table，因此這邊直接檢查 table id 是不是 table 0</span><br><span class="hljs-keyword">if</span> (!piTableId.equals (MY_INGRESS_TABLE0_CONTROL_TABLE0)) &#123;<br>        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PiInterpreterException</span>(<span class="hljs-string">&quot;Unsupported table id: &quot;</span> + piTableId);<br>    &#125;<br>    <span class="hljs-comment">// 我們的 pipeline 只支援 set output port 一個 instruction</span><br>    <span class="hljs-keyword">if</span> (treatment.allInstructions ().size () != <span class="hljs-number">1</span> ||<br>            !treatment.allInstructions ().get (<span class="hljs-number">0</span>).type ().equals (Instruction.Type.OUTPUT)) &#123;<br>        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PiInterpreterException</span>(<span class="hljs-string">&quot;Only output instruction is supported&quot;</span>);<br>    &#125;<br><br>    <span class="hljs-type">PortNumber</span> <span class="hljs-variable">port</span> <span class="hljs-operator">=</span> ((Instructions.OutputInstruction) treatment.allInstructions ().get (<span class="hljs-number">0</span>)).port ();<br>    <br>    <span class="hljs-comment">// 像 controller、flooding 這些特別的 port，稱之為 Logical port</span><br>    <span class="hljs-keyword">if</span> (port.isLogical ()) &#123;<br>        <span class="hljs-keyword">if</span> (port.exactlyEquals (PortNumber.CONTROLLER)) &#123;<br>            <span class="hljs-comment">// 我們支援使用 send_to_controller 這個 action 將封包送到 ONOS</span><br>            <span class="hljs-keyword">return</span> PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND_TO_CPU)<br>                    .build ();<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PiInterpreterException</span>(<span class="hljs-string">&quot;Unsupported logical port: &quot;</span> + port);<br>        &#125;<br>    &#125;<br>    <br>    一般的使用用 send 這個 action 來傳送封包 < br>    <span class="hljs-keyword">return</span> PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND)<br>            .withParameter (<span class="hljs-keyword">new</span> <span class="hljs-title class_">PiActionParam</span>(PORT, port.toLong ()))<br>            .build ();<br>&#125;<br></code></pre></td></tr></table></figure><p>到此我們已經完成了 flow rule 的轉換，可以使用 ONOS 標準的 flow rule 來操作 pipeline 了。</p><blockquote><p>如果希望對轉換機制有更詳細的瞭解可以查看 <a href="https://github.com/opennetworkinglab/onos/blob/master/core/net/src/main/java/org/onosproject/net/pi/impl/PiFlowRuleTranslatorImpl.java">ONOS 原始碼</a></p></blockquote><h3 id="Packet-in-x2F-Packet-out"><a href="#Packet-in-x2F-Packet-out" class="headerlink" title="Packet-in&#x2F;Packet-out"></a>Packet-in&#x2F;Packet-out</h3><p>Interpreter 另外一個重要的功能是使 pipeline 支援 packet-in&#x2F;packet-out 的功能。為此我們需要先修改我們的 p4 pipeline。</p><p>首先我們會先加入兩個特別的 header，並使用 controller_header anotation 標記，<code>@controller_header (&quot;packet_in&quot;)</code> 來得知這個 packet_in_header_t 對應的是 packet-in 時，附加在這個封包的 meta data，通常會定義 ingress port 來表示封包的 input port。同樣的 packet_out_header_t 是 packet-out 時，ONOS 送來 pipeline 處理的 meta data</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@controller_header (&quot;packet_in&quot;)</span><br>header packet_in_header_t &#123;<br>    bit&lt;<span class="hljs-number">7</span>&gt; _padding;<br>    bit&lt;<span class="hljs-number">9</span>&gt; ingress_port;<br>&#125;<br><br><span class="hljs-meta">@controller_header (&quot;packet_out&quot;)</span><br>header packet_out_header_t &#123;<br>    bit&lt;<span class="hljs-number">7</span>&gt; _padding;<br>    bit&lt;<span class="hljs-number">9</span>&gt; egress_port;<br>&#125;<br></code></pre></td></tr></table></figure><p>接著我們要修改 header、parser，從 ONOS packet out 出來的封包對 p4 交換機來說相當於從特定一個 port 送進來的封包，因此一樣會經過整個 pipeline</p><blockquote><p>以 bmv2 來說，controller 的 port number 可以自由指定，在我們使用的 p4mn container 內這個 port 被定義成 255，為此我們在 pipeline 的 p4 檔案內定義了 CPU_PORT 巨集為 255</p></blockquote><p>我們將 packet-in&#x2F;packet-out header 加入到 headers 內，當 packet-out 時，packet_out 會在封包的開頭，因此在 parser 的 start state，我們先根據 ingress port 是不是 CPU_PORT 來決定是不是要 parse packet_out header。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java">struct headers_t &#123;<br>    packet_in_header_t packet_in;<br>    packet_out_header_t packet_out;<br>    ethernet_h ethernet;<br>&#125;<br><br>parser <span class="hljs-title function_">MyParser</span><span class="hljs-params">(packet_in packet,</span><br><span class="hljs-params">                out headers_t hdr,</span><br><span class="hljs-params">                inout local_metadata_t meta,</span><br><span class="hljs-params">                inout standard_metadata_t standard_metadata)</span> &#123;<br><br>    state start &#123;<br>        transition <span class="hljs-title function_">select</span><span class="hljs-params">(standard_metadata.ingress_port)</span> &#123;<br>            CPU_PORT: parse_packet_out;<br>            <span class="hljs-keyword">default</span>: parse_ethernet;<br>        &#125;<br>    &#125;<br><br>    state parse_packet_out &#123;<br>        packet.extract (hdr.packet_out);<br>        transition parse_ethernet;<br>    &#125;<br><br>    state parse_ethernet &#123;<br>        packet.extract (hdr.ethernet);<br>        transition accept;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>在 Ingress 部分，如果是 packet-out packet，我們沒有必要讓他經過整個 ingress pipeline，為此我們直接將 egress_spec 設置為 packet_out header 內的 egress_port，然後直接呼叫 exit。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java">control <span class="hljs-title function_">MyIngress</span><span class="hljs-params">(inout headers_t hdr,</span><br><span class="hljs-params">                  inout local_metadata_t meta,</span><br><span class="hljs-params">                  inout standard_metadata_t standard_metadata)</span> &#123;<br>    apply &#123;<br>        <span class="hljs-keyword">if</span> (standard_metadata.ingress_port == CPU_PORT) &#123;<br>            standard_metadata.egress_spec = hdr.packet_out.egress_port;<br>            hdr.packet_out.setInvalid ();<br>            exit;<br>        &#125;<br>        table0_control.apply (hdr, meta, standard_metadata);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>我們的 table0 則加入了 send_to_cpu 這個 action，做的事情就是把 egress_spec 設定成 CPU port。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java">control <span class="hljs-title function_">table0_control</span><span class="hljs-params">(inout headers_t hdr,</span><br><span class="hljs-params">                inout local_metadata_t local_metadata,</span><br><span class="hljs-params">                inout standard_metadata_t standard_metadata)</span> &#123;<br>    action <span class="hljs-title function_">send_to_cpu</span><span class="hljs-params">()</span> &#123;<br>        standard_metadata.egress_spec = CPU_PORT;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>當封包要 packet-in 到 ONOS 時，需要將 packet-in header 設為 valid，並填入對應的資料，這個部分會再 Egress pipeline 完成</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java">control <span class="hljs-title function_">MyEgress</span><span class="hljs-params">(inout headers_t hdr,</span><br><span class="hljs-params">                 inout local_metadata_t meta,</span><br><span class="hljs-params">                 inout standard_metadata_t standard_metadata)</span> &#123;<br>    apply &#123;<br>        <span class="hljs-keyword">if</span> (standard_metadata.egress_port == CPU_PORT) &#123;<br>            hdr.packet_in.setValid ();<br>            hdr.packet_in.ingress_port = standard_metadata.ingress_port;<br>            hdr.packet_in._padding = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>最後為了讓 packet-in header 能後被傳到 ONOS，需要再 deparser 加入該 header，注意 packet-out header 是為了讓 pipeline 能夠根據該 header 來處理 pecket-out header 用的，因此他不應該被加入到 deparser。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java">control <span class="hljs-title function_">MyDeparser</span><span class="hljs-params">(packet_out packet, in headers_t hdr)</span> &#123;<br>    apply &#123;<br>        packet.emit (hdr.packet_in);<br>        packet.emit (hdr.ethernet);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>接著回到我們的 interpreter，在 interpreter 內定義了兩個函數 mapInboundPacket 和 mapOutboundPacket，分別對應 packet-in 和 packet-out 封包的處理。先前我們在 p4 pipeline 定義了 packet_in 和 packet_out 的 header，這兩個函數最基本的功能是讀取和寫入這兩個 header 的資訊。由於這兩個函數的功能相對固定，因此可以直接從 ONOS 的 <a href="https://github.com/opennetworkinglab/onos/blob/master/pipelines/basic/src/main/java/org/onosproject/pipelines/basic/BasicInterpreterImpl.java">basic pipeline interpreter</a> 複製過來修改。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br>  <span class="hljs-keyword">public</span> Collection&lt;PiPacketOperation&gt; <span class="hljs-title function_">mapOutboundPacket</span><span class="hljs-params">(OutboundPacket packet)</span> <span class="hljs-keyword">throws</span> PiInterpreterException &#123;<br>      <span class="hljs-type">TrafficTreatment</span> <span class="hljs-variable">treatment</span> <span class="hljs-operator">=</span> packet.treatment ();<br><br>      <span class="hljs-comment">// 由於 outbound packet 的內容通常不用在 switch 上在做修改，因此我們只需要取得 </span><br>      <span class="hljs-comment">//set output port 的 instruction</span><br>      <span class="hljs-comment">// 當然如果有特別的功能需求，可以透過修改 pipeline 來支援更多 instruction</span><br>      List&lt;Instructions.OutputInstruction&gt; outInstructions = ...<br><br>      ImmutableList.Builder&lt;PiPacketOperation&gt; builder = ImmutableList.builder ();<br>      <span class="hljs-keyword">for</span> (Instructions.OutputInstruction outInst : outInstructions) &#123;<br>          ...<br>          <span class="hljs-comment">// 這邊透過呼叫 createPiPacketOperation 來填入 packet_out header 的資訊 </span><br>          builder.add (createPiPacketOperation (packet.data (), outInst.port ().toLong ()));<br>          ...<br>      &#125;<br>      <span class="hljs-keyword">return</span> builder.build ();<br>  &#125;<br><br><span class="hljs-keyword">private</span> PiPacketOperation <span class="hljs-title function_">createPiPacketOperation</span><span class="hljs-params">(ByteBuffer data, <span class="hljs-type">long</span> portNumber)</span><br>            <span class="hljs-keyword">throws</span> PiInterpreterException &#123;<br>        <span class="hljs-type">PiPacketMetadata</span> <span class="hljs-variable">metadata</span> <span class="hljs-operator">=</span> createPacketMetadata (portNumber);<br>        <span class="hljs-keyword">return</span> PiPacketOperation.builder ()<br>                .withType (PACKET_OUT)<br>                .withData (copyFrom (data))<br>                .withMetadatas (ImmutableList.of (metadata))<br>                .build ();<br>  &#125;<br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> InboundPacket <span class="hljs-title function_">mapInboundPacket</span><span class="hljs-params">(PiPacketOperation packetIn, DeviceId deviceId)</span> <span class="hljs-keyword">throws</span> PiInterpreterException &#123;<br>    Ethernet ethPkt;<br>    ...<br>    ethPkt = Ethernet.deserializer ().deserialize (packetIn.data ().asArray (), <span class="hljs-number">0</span>,<br>    ...<br>    <span class="hljs-comment">//packet_in header 的資訊會以 key-value 的方式存在 packetIn.metadatas</span><br>Optional&lt;PiPacketMetadata&gt; packetMetadata = packetIn.metadatas ()<br>                .stream ().filter (m -&gt; m.id ().equals (INGRESS_PORT))<br>                .findFirst ()<br>    <span class="hljs-comment">// 從中提取出 input port number</span><br>    <span class="hljs-type">ImmutableByteSequence</span> <span class="hljs-variable">portByteSequence</span> <span class="hljs-operator">=</span> packetMetadata.get ().value ();<br>    <span class="hljs-type">short</span> <span class="hljs-variable">s</span> <span class="hljs-operator">=</span> portByteSequence.asReadOnlyBuffer ().getShort ();<br>    <span class="hljs-type">ConnectPoint</span> <span class="hljs-variable">receivedFrom</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ConnectPoint</span>(deviceId, PortNumber.portNumber (s));<br>    <span class="hljs-type">ByteBuffer</span> <span class="hljs-variable">rawData</span> <span class="hljs-operator">=</span> ByteBuffer.wrap (packetIn.data ().asArray ());<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">DefaultInboundPacket</span>(receivedFrom, ethPkt, rawData);<br>    ...<br>&#125;<br></code></pre></td></tr></table></figure><p>到此我們已經完成了 interpreter 的實現了，Interpreter 目前還包含 <code>mapLogicalPortNumber</code> 和 <code>getOriginalDefaultAction</code> ，不過基本的 interpreter 不需要實現這兩個功能，所以這邊就不再展開介紹。</p><blockquote><p>同樣使用範例程式碼時，可以在 ONOS CLI 使用 <code>add-common-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt;</code> 的方式來使用下達 ONOS 標準的 flow rule。詳情可以 <a href="https://github.com/gamerslouis/onos-p4-tutorial/blob/master/simpleswitch/src/main/java/me/louisif/pipelines/simpleswitch/cli/AddFlowRule.java">參考檔案</a>。</p></blockquote><h2 id="撰寫-Pipeliner"><a href="#撰寫-Pipeliner" class="headerlink" title="撰寫 Pipeliner"></a>撰寫 Pipeliner</h2><p>到目前為止我們已經完成了 pipeconf 的基本功能，可以下 flow rule 還有使用 packet-in&#x2F;packet-out 的功能，不過到目前我們還是需要直接使用 flow rule，要知道 table id 對應的功能，為了能夠隱藏 table 的細節還有銜接 ONOS 內建的網路功能 APP，我們需要實作 pipeliner 讓我們的交換機支援 flow objective 的功能。</p><p>和 Interpreter 類似，我們需要實作 <code>Pipeliner</code> 這個介面並繼承 <code>AbstractHandlerBehaviour</code> 然後在 PipeconfLoader 透過 <code>addBehaviour (Pipeliner.class, SimpleSwitchPipeliner.class)</code> 的方式加入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleSwitchPipeliner</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">AbstractHandlerBehaviour</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Pipeliner</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">Logger</span> <span class="hljs-variable">log</span> <span class="hljs-operator">=</span> getLogger (getClass ());<br><br>    <span class="hljs-keyword">private</span> FlowRuleService flowRuleService;<br>    <span class="hljs-keyword">private</span> DeviceId deviceId;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">(DeviceId deviceId, PipelinerContext context)</span> &#123;<br>        <span class="hljs-built_in">this</span>.deviceId = deviceId;<br>        <span class="hljs-built_in">this</span>.flowRuleService = context.directory ().get (FlowRuleService.class);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>首先我們實作 init 方法，pipeliner 和交換機之間是 1 對 1 的關係，因此當交換機被初始化的時候，可以透過 init 方法取得 device 的 id。context 最主要的部份是使用 directory ().get 方法。平常我們在 APP 開發時是使用 Reference annotation 來取得 onos 的 service，這邊我們可以直接透過 get 方法來取得 service，由於 pipeliner 需要完成 flow ojbective 到 flow rule 的轉換，並直接送到 flow rule service，因此這邊先取得 flow rule service。</p><p>在 ONOS Flow Objective Service 的架構內，其實總共有三種 objective，分別是 forward、filter 和 next，他們都需要透過 pipeliner 來和 ONOS 核心互動，由於我們只想要實作 forwarding objective 的部分，因此 filter 和 next 可以單純回應不支援的錯誤訊息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">filter</span><span class="hljs-params">(FilteringObjective obj)</span> &#123;<br>    obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));<br>&#125;<br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">next</span><span class="hljs-params">(NextObjective obj)</span> &#123;<br>    obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));<br>&#125;<br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> List&lt;String&gt; <span class="hljs-title function_">getNextMappings</span><span class="hljs-params">(NextGroup nextGroup)</span> &#123;<br>    <span class="hljs-comment">// We do not use nextObjectives or groups.</span><br>    <span class="hljs-keyword">return</span> Collections.emptyList ();<br>&#125;<br></code></pre></td></tr></table></figure><p>接著就到了我們的主角 forward objective，在實作邏輯上其實與 interpreter 對 treatment 的處理方式類似，forward 方法會取得一個 ForwardingObjective 物件，我們根據 treatment 和 selector 生成出一條或多條 flow rule，然後透過 flow rule service 下放到交換機上。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">forward</span><span class="hljs-params">(ForwardingObjective obj)</span> &#123;<br>    <span class="hljs-keyword">if</span> (obj.treatment () == <span class="hljs-literal">null</span>) &#123;<br>        obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));<br>    &#125;<br><br>    <span class="hljs-comment">// Simply create an equivalent FlowRule for table 0.</span><br>    <span class="hljs-keyword">final</span> FlowRule.<span class="hljs-type">Builder</span> <span class="hljs-variable">ruleBuilder</span> <span class="hljs-operator">=</span> DefaultFlowRule.builder ()<br>            .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0)<br>            .forDevice (deviceId)<br>            .withSelector (obj.selector ())<br>            .fromApp (obj.appId ())<br>            .withPriority (obj.priority ())<br>            .withTreatment (obj.treatment ());<br><br>    <span class="hljs-keyword">if</span> (obj.permanent ()) &#123;<br>        ruleBuilder.makePermanent ();<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        ruleBuilder.makeTemporary (obj.timeout ());<br>    &#125;<br><br>    <span class="hljs-keyword">switch</span> (obj.op ()) &#123;<br>        <span class="hljs-keyword">case</span> ADD:<br>            flowRuleService.applyFlowRules (ruleBuilder.build ());<br>            <span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> REMOVE:<br>            flowRuleService.removeFlowRules (ruleBuilder.build ());<br>            <span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">default</span>:<br>            log.warn (<span class="hljs-string">&quot;Unknown operation &#123;&#125;&quot;</span>, obj.op ());<br>    &#125;<br><br>    obj.context ().ifPresent (c -&gt; c.onSuccess (obj));<br>&#125;<br></code></pre></td></tr></table></figure><p>最後我們還需要實作 purgeAll，當刪除所有 flow obejctive 的時候，刪除所有的 flow rule，這邊我們只需要簡單呼叫 flow rule service 的 purgeFlowRules 就好。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">purgeAll</span><span class="hljs-params">(ApplicationId appId)</span> &#123;<br>    flowRuleService.purgeFlowRules (deviceId, appId);<br>&#125;<br></code></pre></td></tr></table></figure><p>到此我們已經完成了整個 pipeconf 的實作，可以透過 flow objective 的方式來管理交換機並與 ONOS 內建的 APP 整合，因此我們可以透過使用 <code>proxyarp</code> 和 <code>fwd</code> 兩個 APP 來讓我們的交換機能夠正常的工作。</p><h2 id="小結"><a href="#小結" class="headerlink" title="小結"></a>小結</h2><p>以上就是 ONOS P4 Pipeconf 的基本開發教學，使用的範例程式碼在 <a href="https://github.com/gamerslouis/onos-p4-tutorial">github</a>，如果有遇到任何問題或有說明不清楚的地方，歡迎留言提問，我會盡力為大家解答。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p><a href="https://hackmd.io/@cnsrl/onos_p4">https:&#x2F;&#x2F;hackmd.io&#x2F;@cnsrl&#x2F;onos_p4</a></p><p><a href="https://wiki.onosproject.org/pages/viewpage.action?pageId=16122675">https://wiki.onosproject.org/pages/viewpage.action?pageId=16122675</a></p><p><a href="https://github.com/p4lang/tutorials/blob/master/exercises/basic/solution/basic.p4">https://github.com/p4lang/tutorials/blob/master/exercises/basic/solution/basic.p4</a></p><p><a href="https://github.com/opennetworkinglab/onos/tree/master/pipelines/basic/src/main/java/org/onosproject/pipelines/basic">ONOS Basic Pipeline</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;這篇文章會介紹一下，在 ONOS 上 P4 相關的模組和功能，以及怎麼開發 ONOS APP 與設置 ONOS，讓 ONOS 可以控制 p4 switch 的 pipeline。&lt;/p&gt;</summary>
    
    
    
    <category term="ONOS" scheme="https://blog.louisif.me/categories/ONOS/"/>
    
    
    <category term="P4" scheme="https://blog.louisif.me/tags/P4/"/>
    
  </entry>
  
  <entry>
    <title>分析 ONOS Packet Processor Treatment 無效之原因</title>
    <link href="https://blog.louisif.me/ONOS/Analyze-why-ONOS-Packet-Processor-Treatment-not-Work/"/>
    <id>https://blog.louisif.me/ONOS/Analyze-why-ONOS-Packet-Processor-Treatment-not-Work/</id>
    <published>2022-08-12T03:26:42.000Z</published>
    <updated>2022-08-27T17:43:33.598Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ONOS 踩坑日記</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近嘗試使用 ONOS 目前最新的 2.7 版來開發 APP，用 OpenFlow 來讓交換機實現 router 的功能。結果踩到 ONOS Packet-in 封包處理實作未完全的坑。</p><span id="more"></span> <p>當封包經過 router 時，會根據 routing table 和封包的目標決定要往哪個 interface 送出，同時將封包的 source mac address 改為交換機的 mac address、封包的 destination mac address 改為 nexthop 的 mac address。因此我們需要在交換機上安裝一條 flow rule，selector 是 destination mac address，treatment 有三個 instructions 分別是：修改 src mac、dst mac 和決定 output port。</p><p>為了減少交換機上的 flow entry 的數量，所以採用 reactive 的方式，也就是當交換機收到第一封包時，先將封包送 (packet-in) 給 SDN controller，controller 根據 routing table，直接修改該封包的 mac address，並從交換機特定的 port 送出 (packet-out)，同時生成對應的 flow rule 並安裝到交換機上，後續的封包就可以直接根據 flow rule 轉送而不用再經過 controller。</p><h2 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h2><p>然而問題就出現在第一個封包上，根據 tcpdump 看到的結果，封包的 source 和 destination mac address 都沒有被修改到。</p><p>由於我是使用 OVS 來模擬 Openflow 交換機，因此首先懷疑是不是 OVS 本身實作限制，不支援同時包含上述三個 instructions 導致。然而，後續經過 flow rule 直接送出的封包，都有成功修改到 mac address。由於只有第一個 packet-in 到 controller，再 packet-out 回 switch 的封包沒有被修改，因此開始懷疑是 ONOS 的問題。</p><h2 id="追蹤"><a href="#追蹤" class="headerlink" title="追蹤"></a>追蹤</h2><p>在 ONOS 裡面，一般使用 PacketProcessor 的方式來處理 packet-in 到 controller 的封包。首先實作 PacketProcessor 介面，然後向 PacketService 註冊，ONOS 就會調用 processor 處理 packet-in 的封包。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-type">PacketProcessor</span> <span class="hljs-variable">processor</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PacketProcessor</span>() &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">process</span><span class="hljs-params">(PacketContext context)</span> &#123;<br>        <span class="hljs-comment">//.... 處理封包的邏輯 </span><br>        <span class="hljs-comment">// 修改設定封包的 mac address 和決定 output port</span><br>        context.treatmentBuilder ()<br>               .setEthSrc (srcMac)<br>               .setEthDst (dstMac)<br>               .setOutput (outPort.port ()); <br>        context.send (); <span class="hljs-comment">// 將封包 packet-out 回交換機 </span><br>    &#125;<br>&#125;;<br><span class="hljs-meta">@Activate</span><br><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">activate</span><span class="hljs-params">()</span> &#123;<br>    packetService.addProcessor (processor,      <br>         PacketProcessor.director (<span class="hljs-number">1</span>));<br>&#125;<br></code></pre></td></tr></table></figure><p>PacketContext 會包含 packet-in 進來的封包內容，並可透過 context.treatmentBuilder 修改封包和決定要往哪個 port 送出去，最後透過 send 指令，packet-out 回交換機。</p><p>搜查一下 ONOS 的原始碼，會在 core&#x2F;api 下面找到 DefaultPacketContext ，這個 class 實作了 PacketContext 這個 Interface，但是這個 class 是一個 abstract class，因此一定有人繼承了它，繼續搜查 PacketContext 這個字會找到兩個跟 Openflow 相關的，DefaultOpenFlowPacketContext 和 OpenFlowCorePacketContext，但是後者才有繼承 DefaultPacketContext 和實作 PacketContext 介面，因此 PacketProcesser 在處理 openflow packet-in 進來的封包時，拿到的 PacketContext 具體應該是 OpenFlowCorePacketContext 這個 class。</p><p>打開 OpenFlowCorePacketContext.java 會看到它實現了 send 這個 function，經過簡單的檢查後呼叫 sendPacket 這個 function，然後你就會看到…</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sendPacket</span><span class="hljs-params">(Ethernet eth)</span> &#123;<br>        List&lt;Instruction&gt; ins = treatmentBuilder ().build ().allInstructions ();<br>        <span class="hljs-type">OFPort</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>        <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> support arbitrary list of treatments must be supported in ofPacketContext</span><br>        <span class="hljs-keyword">for</span> (Instruction i : ins) &#123;<br>            <span class="hljs-keyword">if</span> (i.type () == Type.OUTPUT) &#123;<br>                p = buildPort (((OutputInstruction) i).port ());<br>                <span class="hljs-keyword">break</span>; <span class="hljs-comment">//for now...</span><br>            &#125;<br>        &#125;<br>        .......<br>&#125;<br></code></pre></td></tr></table></figure><p>謎底揭曉，原來 ONOS 只有實作 output 這個 instruction (決定 output port)，因此它直接忽略的 set source mac 和 set destination mac 兩個指令，交換機送出來的封包當然就只有往對的 port 送，而沒有改到 mac address。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>結論就是在當前 ONOS 2.7 環境下，PacketProcesser 在處理 Openflow 交換機封包 packet-out 的時候，只能決定該封包的 output port，其餘對該封包的修改都是無效的。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://github.com/opennetworkinglab/onos/">ONOS Source code</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;ONOS 踩坑日記&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近嘗試使用 ONOS 目前最新的 2.7 版來開發 APP，用 OpenFlow 來讓交換機實現 router 的功能。結果踩到 ONOS Packet-in 封包處理實作未完全的坑。&lt;/p&gt;</summary>
    
    
    
    <category term="ONOS" scheme="https://blog.louisif.me/categories/ONOS/"/>
    
    
  </entry>
  
</feed>
