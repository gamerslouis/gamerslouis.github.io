{"posts":[{"title":"分析 ONOS Packet Processor Treatment 無效之原因","text":"ONOS 踩坑日記 前言最近嘗試使用 ONOS 目前最新的 2.7 版來開發 APP，用 OpenFlow 來讓交換機實現 router 的功能。結果踩到 ONOS Packet-in 封包處理實作未完全的坑。 當封包經過 router 時，會根據 routing table 和封包的目標決定要往哪個 interface 送出，同時將封包的 source mac address 改為交換機的 mac address、封包的 destination mac address 改為 nexthop 的 mac address。因此我們需要在交換機上安裝一條 flow rule，selector 是 destination mac address，treatment 有三個 instructions 分別是：修改 src mac、dst mac 和決定 output port。 為了減少交換機上的 flow entry 的數量，所以採用 reactive 的方式，也就是當交換機收到第一封包時，先將封包送 (packet-in) 給 SDN controller，controller 根據 routing table，直接修改該封包的 mac address，並從交換機特定的 port 送出 (packet-out)，同時生成對應的 flow rule 並安裝到交換機上，後續的封包就可以直接根據 flow rule 轉送而不用再經過 controller。 問題然而問題就出現在第一個封包上，根據 tcpdump 看到的結果，封包的 source 和 destination mac address 都沒有被修改到。 由於我是使用 OVS 來模擬 Openflow 交換機，因此首先懷疑是不是 OVS 本身實作限制，不支援同時包含上述三個 instructions 導致。然而，後續經過 flow rule 直接送出的封包，都有成功修改到 mac address。由於只有第一個 packet-in 到 controller，再 packet-out 回 switch 的封包沒有被修改，因此開始懷疑是 ONOS 的問題。 追蹤在 ONOS 裡面，一般使用 PacketProcessor 的方式來處理 packet-in 到 controller 的封包。首先實作 PacketProcessor 介面，然後向 PacketService 註冊，ONOS 就會調用 processor 處理 packet-in 的封包。 1234567891011121314151617private PacketProcessor processor = new PacketProcessor() { @Override public void process(PacketContext context) { //.... 處理封包的邏輯 // 修改設定封包的 mac address 和決定 output port context.treatmentBuilder () .setEthSrc (srcMac) .setEthDst (dstMac) .setOutput (outPort.port ()); context.send (); // 將封包 packet-out 回交換機 }};@Activateprotected void activate() { packetService.addProcessor (processor, PacketProcessor.director (1));} PacketContext 會包含 packet-in 進來的封包內容，並可透過 context.treatmentBuilder 修改封包和決定要往哪個 port 送出去，最後透過 send 指令，packet-out 回交換機。 搜查一下 ONOS 的原始碼，會在 core/api 下面找到 DefaultPacketContext ，這個 class 實作了 PacketContext 這個 Interface，但是這個 class 是一個 abstract class，因此一定有人繼承了它，繼續搜查 PacketContext 這個字會找到兩個跟 Openflow 相關的，DefaultOpenFlowPacketContext 和 OpenFlowCorePacketContext，但是後者才有繼承 DefaultPacketContext 和實作 PacketContext 介面，因此 PacketProcesser 在處理 openflow packet-in 進來的封包時，拿到的 PacketContext 具體應該是 OpenFlowCorePacketContext 這個 class。 打開 OpenFlowCorePacketContext.java 會看到它實現了 send 這個 function，經過簡單的檢查後呼叫 sendPacket 這個 function，然後你就會看到… 123456789101112private void sendPacket(Ethernet eth) { List&lt;Instruction&gt; ins = treatmentBuilder ().build ().allInstructions (); OFPort p = null; // TODO: support arbitrary list of treatments must be supported in ofPacketContext for (Instruction i : ins) { if (i.type () == Type.OUTPUT) { p = buildPort (((OutputInstruction) i).port ()); break; //for now... } } .......} 謎底揭曉，原來 ONOS 只有實作 output 這個 instruction (決定 output port)，因此它直接忽略的 set source mac 和 set destination mac 兩個指令，交換機送出來的封包當然就只有往對的 port 送，而沒有改到 mac address。 結論結論就是在當前 ONOS 2.7 環境下，PacketProcesser 在處理 Openflow 交換機封包 packet-out 的時候，只能決定該封包的 output port，其餘對該封包的修改都是無效的。 參考資料 ONOS Source code","link":"/ONOS/Analyze-why-ONOS-Packet-Processor-Treatment-not-Work/"},{"title":"Openstack 架設系列文章 (1) - 網路架構解析及設置","text":"前言在嘗試布建 openstack 的過程中，卡關最久的應該是網路的部分，由於在設置 openstack 的過程中，需要先把網路介面和線路連接設定好，所以必須要對 openstack 的網路架構，有清晰的認知，不然在設置的過程中會遇到許多障礙… 特別是 openstack 有 flat, vlan, vxlan 以及 provider, self-service 兩種不同維度的概念分類，因此在釐清的過程中花費許多時間，因此這邊整理一下 openstack 的各種網路架構。 這篇文章不會介紹具體的 openstack 搭建，不過之後會有一篇介紹如何使用 openstack-ansible 來部屬 openstack。 首先為了後續討論方便我們這邊要先幫 openstack 裡面的幾個網路層級訂個名子。 物理網路：由 openstack 節點上面的實體網卡以及結點外的網路構成 節點覆蓋 (Overlay) 網路：節點上面的特定一個 linux bridge 透過物理網路與其他節點上的 bridge 共同構成的一個 L2 網路 虛擬網路：以虛擬機的視角看到的 L2 網路後續會持續出現這三個名詞，在介紹的過程中來解釋和分析這三者的差別。 Openstack 網路類型概念首先我們先撇開 openstack，我們看一個最簡單在多台實體設備上面裝設虛擬機構建出來的一個網路結構。 在這個架構裡，所有的 VM 和實體網卡都接在同一個 linux bridge 上，由於實體網卡也橋接在 bridge 上了，所以節點上的虛擬網路是物理網路的延伸，可以看成所有實體共同構成了一個 L2 網路。由於 bridge 本身可以給予 IP，當成是接在節點上的網路介面，因此 VM 和節點本身也都同在一個 L2 網路內彼此互相可見。在這個網路架構中，物理網路、節點覆蓋網路和虛擬網路三者是完全等價通透的。 Flat network接著我們加入 openstack 的組件。首先是 controller 節點，我們在 controller 節點上部屬了 openstack 的 API 服務。接著我們在 compute 節點上增加一張實體網卡 eth1 讓節點上的 nova agent 和 neutron agent 可以與 contrroller node 溝通。在這個架構中，所有的 service 和 agent 是直接安裝在節點上的程式，因此可以透過 eth1 實體網卡的 ip 直接進行訪問。在圖中的 linux bridge 換了顏色，因為這個 bridge 並不是由我們手動建立出來的而是由 neutron agent 自行建立出來的。同時當 nova 建立 VM 後，neutron 也會建立 veth pair 連接 bridge 和 VM。到此我們就建立了最簡單的 openstack 網路，在這個網路架構下雖然節點本身的網路從 bridge 分離出來成為了獨立的網卡，但是節點本身和所有的 VM 還是同處在一個 L2 網路內，因此在這個架構下物理網路、節點覆蓋網路和虛擬網路三者還是完全等價通透的。 到此我們就建構了 neutron 的第一個種網路模式 flat。Neutron 的不同模式差異在於說 neuitron 管理的 bridge 是如何接入節點覆蓋網路的 (在這邊可以先直接看成物理網路，關於節點覆蓋網路我們會在後面解釋)。在 flat 模式下對外網卡直接橋接到 bridge 上，不對封包做處理。 在 openstack 中有 tenant networks 租戶網路的概念，我們可以在 openstack 上切出若干個獨立的 L2 網路，分給不同的 project 和 vms 使用。在使用單一個 flat 網路的時候，這件事是做不到了，因為所有 VM 都在同一個 L2 網路內。![](Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/Pasted image 20221003133348.png)直觀的第一個方法當然是切出多個 flat network 給不同的 tenat 使用，然而這樣使用的限制非常大，首先我們需要替每個 flat 預先切出獨立的 L2 網路，在這個範例中我們使用獨立的網卡和交換機來分割，非常麻煩，雖然後面會提到可以使用節點覆蓋網路的方法來切割，只用一張網路卡達成，然而每次增加網路需要修改每個節點上 neutron agent 的設定檔，不適合隨時依據需求改動。 因此比較可行的方法是使用 vlan 或 vxlan 來切割租戶網路，並將 vlan、vxlan 的建立管理交給 neutron agent 來處理。 Vlan network 首先是 vlan 模式，在 vlan 模式下我們一樣提供一個網路介面 eth0 給 neutron 使用，並一樣假設所有節點上的 eth0 介面都在 L2 互通。 接著當我們透過 openstack 建立網路並指定為 vlan type 時，neutron 會替該網路分配一個 vlan id，並建立對應的 vlan 網路卡，vlan 網路卡是 linux 本身的功能，經過 vlan 網路卡的封包會從該網路卡榜定的原始網卡送出去 (eth0)，但是封包會加上 vlan 網卡綁定的 vlan id。 如上圖所示，我們切割出了 vlan 356 和 vlan 357 兩個租戶網路，並統一透過 eth0 物理網路與其他節點溝通。 Vxlan network首先我們先介紹一下 vxlan。和 vlan 的功能類似，vxlan 的功能也是在一個網路內切割出多個虛擬網路，不過教於 vlan，vxlan 有以下特點: vlan id 只能夠切出 4096 個網路，但是 vxlan id 可以切出 2 的 24 次方個網路。 vlan 只是在 ethernet header 和 IP header 中間插了一個 vlan header，因此 vlan 只能在 L2 的物理網路內傳輸。但是 vxlan 的封裝方式是將整個 L2 封包加上一個 vxlan header 後，封裝在一個 UDP 封包內，因此透過外面的 UDP 封包，vxlan 能夠跨越 L3 網路建立 L2 的通道，連結兩個不相連的 L2 網路。 在 Linux 上建立 vxlan 介面時要指定兩個東西，一個是 vni (vxlan id), 另一個是 local ip，當封包通共 vxlan 介面時，會被加上包含 vni 的 vxlan id，外部的 UDP 封包則會利用 local ip 當作 src ip，同時利用該 local ip 對應的 interface 來收發 vxlan 封包。 在 vlan 網路裡面，由於 vlan 一樣是 L2 封包，因此封包是透過 L2 的廣播學習機制來傳遞的。然而如前面所示，vxlan 封包會封裝成 UDP 封包，因此 vxlan 靠的是 L3 的路由機制來傳遞。 在 Linux 上這個路由機制有幾種作法 在建立 vxlan interface 時直接指定 remote ip，建立成點對點的 vxlan tunnel 透過 linux bridge forwarding database，下達 forwarding rule，指定當 L2 封包的 mac address 是多少的時候要送到哪個 remote ip 最後也是 openstack 使用的方式，將封包發送到一個 L3 廣播地址 (例如 239.1.1.1)，linux 會自動在 L2 加上 multicast mac address，所有在同一個 L2 物理網路上的設備就能夠同時收到 vxlan 的封包。回到 openstack 上，在設置 vxlan 時和 vlan、flat 不太一樣，我們不用指定綁定的網路介面 (eth0)，而是要指定 local_ip 跟 vxlan_group，前者對應到綁定解面 (eth0) 的 IP，後者則是 vxlan 廣播域的 IP (239.1.1.1)。後面就和 vlan 一樣，在 openstack 上建立的網路時，netruon 為該網路建立對應的 vxlan 介面和 linux bridge，來提供給該網路的虛擬機使用。 雖然在一台節點上面不太可能開到 4096 個 tenant，但是在整個 openstack 叢集內可以將不同 tenant 分配在不同的節點上，因此 vxlan 是有意義的。 Openstack 網路設置在 openstack neutron 中，我們要設定每個節點上的 ml2 設定檔來提供 neutron 網路的基本資訊。路徑在 /etc/neutron/plugins/ml2/。 123456# /etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers = flat,vlan,vxlan,localtenant_network_types = vxlan,vlan,flatmechanism_drivers = linuxbridgeextension_drivers = port_security 首先對於上述三種不同的網路類型，如果要在 openstack 內使用需要指定啟用對應的 type_drivers。(除了前面介紹的三種網路類型，還有 local 跟 gre 這兩種，前者用於單機情況 neutron 的 linux bridge 不連接到任何對外網路，後者使用 gre tunnel 來取代 vxlan tunnel，這邊不多作介紹) 接著 tenant_network_types 表示在 openstack 設可以用於建立 project 租戶網路的網路類型，同時順序也代表了在建立網路時預設使用的網路類型優先順序。 mechanism_drivers 這邊指定是 linuxbridge，前面提到 neutron 會建立 bridge 來連接 VM 跟外部網路介面，其實這邊還有很多其他選項，例如 open vSwitch，來提供更多網路管理功能，但是這邊就只以 linux bridge 為主要介紹對象。 extension_drivers 則可以載入其他 plugin 來提供 QoS、安全管理的功能。 接著我們要設定物理網路和網路介面的對應，在 flat 和 vlan 網路模式下，我們都需要將網路與一張可以連接到物理網路的網路介面做綁定 (前面的 eth0 或 eth1)，但是在實際情況中，每個節點上網路介面卡的名稱可能不相通，因此需要在每個節點上指定物理網路和網路介面的對應。 123# /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings = vlan1:eth0,flat1:eth1,flat2:eth2 接著對不同的網路類型有各自的網路設置，首先是 flat，要指定的東西很簡單，就是可以用於建立 flat network 的物理網路，如果所有網路都可以，則指定為 * 1234# /etc/neutron/plugins/ml2/ml2_conf.ini[ml2_type_flat]flat_networks = flat1, flat2# Example:flat_networks = * 在 vlan 部分則要指定在每個物理網路上允許的租戶網路 vlan id，格式是&lt;physical_network&gt;[:&lt;vlan_min&gt;:&lt;vlan_max&gt;] 123# /etc/neutron/plugins/ml2/ml2_conf.ini[ml2_type_vlan]network_vlan_ranges = vlan1:101:200,vlan1:301:400 最後 vxlan 分成兩個部分，首先和 vlan 類似，我們要指定可使用的 vxlan vni 和廣播域 ip 1234# /etc/neutron/plugins/ml2/ml2_conf.ini[ml2_type_vxlan]vxlan_group = 239.1.1.1vni_ranges = 1:1000 接著我們要設置綁定的網卡 ip 12345678# /etc/neutron/plugins/ml2/ml2_conf.ini[vxlan]enable_vxlan = Truevxlan_group = 239.1.1.1# VXLAN local tunnel endpointlocal_ip = 192.168.56.101l2_population = Falsettl = 32 這邊有一個特別的東西是 l2 population，l2 population 是一種降低廣播封包造成網路負載的方式，在傳統網路內 ARP 封包需要廣播到所有的節點上，大大增加的網路的負擔，透過 l2 population 提供的 proxy ARP 機制，ARP 會在節點上直接由 neutron 回應，而不用透過物理網路廣播到所有節點上，大大降低網路負擔，由於 openstack 內所有的 VM IP、網卡 mac address 都會受到 openstack 的管理，因此 openstack 可以做到 proxy ARP。如果需要啟用 l2 population 則需要額外的 driver，這邊一樣不做詳細介紹。 最後是網路安全的部分，在 openstack 我們可以使用 iptables 來建立 VM 的網路管理，iptables 會阻擋所有進出虛擬機網路的流量，並透過 security group 來下達 iptables 的規則允許特定流量進出。 1234# /etc/neutron/plugins/ml2/linuxbridge_agent.ini[securitygroup]firewall_driver = iptablesenable_security_group = True 1234# /etc/neutron/plugins/ml2/ml2_conf.ini[securitygroup]enable_security_group = Trueenable_ipset = True Provider vs Self-service networks接著我們要介紹 openstack 裡面另外一個重要的網路概念，provider network 跟 self-service network 的差別。 首先要特別注意的是，這邊提到的分類方式和前面的網路類型是兩種不同的角度和分類方式。 雖然 flat network 通常會搭配 provider networks，vlan 和 vxlan 通常會搭配 self service network 但是這並不是一定而且必要的。 前面的網路類型我們探討的是要如何在跨節點的網路架構下提供一個或多個 L2 網路讓 VMs 之間可以彼此連接。 這邊 provider 和 self-service 的差別則是要探討，如何提供前面切割出來的網路 L3 的網路功能 (gateway, routing…) 在 provider 網路模式下，我們假設 L3 的網路功能可以直接由物理網路提供，因此 openstack 只負責提供 DHCP，以及將虛擬機接到物理網路上，其餘的 gateway, routing 功能，openstack 只假設存在，不多做處理。在最簡單的 flat 網路模式下這麼做當然是非常簡單可行的，最簡單的方法就是將 flat 網路直接接入節點間 openstack 管理用的網段，並與節點共用 ip subnet，直接由物理網路提供功能。 然而在 vlan 和 vxlan 模式下使用 proider 網路就不是這麼好用了，前面提到相較於 flat，vlan 和 vxlan 的特點就是能夠動態建立獨立的租戶網路，如果要使用 provider 機制，變成說我們要導入額外的自動化方式，替每個 vlan 或 vxlan 提供 gateway 的功能，因此通常會使用 self-service network。 self-service network 指的是 openstack self service，也就是由 openstack 本身來提供 L3 網路的 gateway, router 的功能。 為了要提供 router 的功能，openstack 的作法是使用 network namespace。 首先我們需要在一個節點上部署 neutron 的 L3 agent。當我們透過 openstack 的 API 建立一個 virtual router 的時候，netron L3 agent 會在節點上建立一個獨立的 network namespace 當作這台 virtual router 的主體。 接著我們就可以將不同的租戶網路接入 router，透過 linux 本身的 iptables, routing 的功能來提供租戶網路間 routing 的功能。我們也可以透過把其中一個租戶網路或 flat 網路接入物理網路，使用 provider network 的方式，其他的租戶網路可以透過 virtual router routing 到 provider network 的租戶網路來上網。 Openstack self-service 指令這邊補充一下如何透過 cli 在 openstack 上建立 router 還有把租戶網路加入 router 1openstack router create router1 首先建立 router1 1openstack port create --network net1 net1-router-port 接著為了讓租戶網路能夠 “接線” 到 router，我們需要幫網路建立一個 port 1openstack router add port router1 net1-router-port 最後把 port 接到 router 上","link":"/Openstack/Openstack-Deployment-Serial-1-Network-Architecure-and-Config/"},{"title":"CNI Spec 導讀","text":"前言這次來嘗試寫寫看 spec 導讀，今天要講的是 Container Network Interface (CNI) Specification。CNI 是 CNCF 的一個專案，這個專案包含了今天要講的 CNI SPEC 以及基於這個 SPEC 開發出來 libraries 還有一系列的 CNI plugins。 CNI 定義了一套 plugin-based 的網路解決方案，包含了設定還有呼叫執行的標準，使用 CNI 最知名的專案應該就是 kubernetes 了，在 k8s 環境中，容器的網路並不是直接由 container runtime (ex. Docker) 處理的，container runtime 建立好容器後，kubelet 就會依照 CNI 的設定和通訊標準去呼叫 CNI plugin，由 CNI plugin 來完成容器的網路設置。 Container runtime 在執行容器建立時，會依據設定檔 (後續稱為 network configuration) 的指示，依序呼叫一個或多個的 CNI plugin 來完成網路功能的設置，一個網路功能的設置可能會需要多個 CNI plugins 之間的相互合作，或著由多個 CNI plugin 提供多個不同的網路功能。 Overview目前 CNI spec 的版本是 1.0.0，CNI 的 repository 裡面除了 spec 文件外也包含了基於 CNI spec 開發的 go library，用於 CNI plugin 的開發。不過 CNI spec 和 library 的版本號是獨立的，當前的 library 版本是 1.1.2。 首先要對幾個基本名詞定義 container 是一個網路獨立的環境，在 linux 上通常透過 network namespace 的機制來切割，不過也可能是一個獨立的 VM。 network 是 endpoints 的集合，每個 endpoint 都有著一個唯一是別的的地址 (通常是 ip address) 可用於互相通訊。一個端點可能是一個容器、VM 或著路由器之類的網路設備。 runtime 指的是呼叫執行 CNI plugin 的程式，以 k8s 來說，kubelet 作為這個角色 plugin 指的是一個用來完成套用特定網路設定的程式。 CNI Spec 包含五個部分 Network configuration format: CNI 網路設定的格式 Execution Protocol: runtime 和 CNI plugin 之間溝通的 protocol Execution of Network Configurations: 描述 runtime 如何解析 network configuration 和操作 CNI plugin。 Plugin Delegation: 描述 CNI plugin 如何呼叫 CNI plugin。 Result Types: 描述 CNI plugin 回傳的結果格式。 Section 1: Network configuration formatCNI 定義了一個網路設定的格式，這個格式用於 runtime 讀取的設定檔，也用於 runtime 解析後，plugin 接收的格式，通常來說設定檔是以 ** 靜態 ** 檔案的方式存在主機上，不會任意變更。 CNI 使用了 JSON 作為設定檔的格式，並包含以下幾個主要欄位 cniVersion (string): 對應的 CNI spec 版本，當前是 1.0.0 name (string): 一個在主機上不重複的網路名稱 disableCheck (boolean): 如果 disableCheck 是 true 的話，runtime 就不能呼叫 CHECK，CHECK 是 spec 定義的一個指令用來檢查網路是否符合 plugin 依據設定的結果，由於一個網路設定可能會呼叫多個 CNI plugins，因此可能會出現網路狀態符合管理員預期，但是 CNI plugin 之間衝突檢查失敗的情況，這時就可以設定 disableCheck plugin (list): CNI plugin 設定 (Plugin configuration object) 的列表。 Plugin configuration objectplugin configuration object 包含了一些明定的欄位，但 CNI plugin 可能根據需要增加欄位，會由 runtime 在不修改的情況下送給 CNI plugin。 必填: type (string): CNI plugin 的執行檔名稱 可選欄位 (CNI protocol 使用): capabilities (dictionary): 定義 CNI plugin 支援的 capabilities，後面在 section 3 會介紹。 保留欄位：這些欄位是在執行過程中，由 runtime 生成出來的，因此不應該在設定檔內被定義。 runtimeConfig args 任何 cni.dev/ 開頭的 key 可選欄位：不是 protocol 定義的欄位，但是由於很多 CNI plugin 都有使用，因此具有特定的意義。 ipMasq (boolean): 如果 plugin 支援的話，會在 host 上替該網路設置 IP masquerade，如果 host 要做為該網路的 gateway 的話，可能需要該功能。 ipam (dictionary): IPAM (IP Address Management) 設置，後面在 section 4 會介紹。 dns (dictionary): DNS 設置相關設置 nameservers (list of strings): DNS server 的 IP 列表 domain (string): DNS search domain search (list of strings),: DNS search domain 列表 options (list of strings): DNS options 列表 其他欄位: CNI plugin 自己定義的額外欄位。 設定檔範例 1234567891011121314151617181920212223242526272829303132333435363738{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;name&quot;: &quot;dbnet&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;bridge&quot;, //plugin specific parameters &quot;bridge&quot;: &quot;cni0&quot;, &quot;keyA&quot;: [&quot;some more&quot;, &quot;plugin specific&quot;, &quot;configuration&quot;], &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, //ipam specific &quot;subnet&quot;: &quot;10.1.0.0/16&quot;, &quot;gateway&quot;: &quot;10.1.0.1&quot;, &quot;routes&quot;: [ {&quot;dst&quot;: &quot;0.0.0.0/0&quot;} ] }, &quot;dns&quot;: { &quot;nameservers&quot;: [ &quot;10.1.0.1&quot; ] } }, { &quot;type&quot;: &quot;tuning&quot;, &quot;capabilities&quot;: { &quot;mac&quot;: true }, &quot;sysctl&quot;: { &quot;net.core.somaxconn&quot;: &quot;500&quot; } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: {&quot;portMappings&quot;: true} } ]} Section 2: Execution ProtocolCNI 的工作模式是由 container runtime 去呼叫 CNI plugin 的 binaries，CNI Protocol 定義了 runtime 和 plugin 之間的溝通標準。 CNI plugin 的工作是完成容器網路介面的某種設置，大致上可以分成兩類 Interface plugin: 建立容器內的網路介面，並確保其連接可用 Chained plugin: 調整修改一個已建立的介面 (可能同時會需要建立更多額外的介面) Runtime 透過兩種方式傳遞參數，一是透過環境變數，二是透過 stdin 傳遞 Section 1 定義的 configuration。如果成功的話，結果會透過 stdout 傳遞，如果失敗的話就會錯誤資訊會透過 stderr 傳遞。configuration 和結果、錯誤都是使用 JSON 格式。 Runtime 必須在 Runtime 的網路執行，大多數情況下就是在主機的預設 network namespace/dom0 ParametersProtocol 的參數都是透過環境變數來傳遞的，可能的參數如下 CNI_COMMAND: 當前執行的 CNI 操作 (可能是 ADD, DEL, CHECK. VERSION) CNI_CONTAINERID: 容器的ＩＤ CNI_NETNS: 容器網路空間的參考，如果是使用 namespaces 的方式來切割的話，就是 namespce 的路徑（e.g. /run/netns/[nsname] ) CNI_IFNAME: 要建立在容器內的介面名稱，如果 plugin 無法建立該名稱則回傳錯誤 CNI_ARGS: 其他參數，Alphanumeric 格式的 key-value pairs，使用分號隔開”e.g. FOO=BAR;ABC=123 CNI_PATH: CNI plugin 的搜尋路徑，因為 CNI 存在 CNI plugin 呼叫 CNI plugin 的情況，所以需要這個路徑。如果包含多個路徑，使用 OS 定義的分隔符號分割。Linux 使用 : ，Windows 使用 ; Errors如果執行成功，CNI Plugin 應該回傳 0。如果失敗則回傳非 0，並從 stderr 回傳 error result type structure。 CNI operationsADDCNI plugin 會在 CNI_NETNS 內建立 CNI_IFNAME 介面或對該介面套用特定的設置 如果 CNI plugin 執行成功，應該從 stdout 回傳一個 result structure。如果 plugin 的 stdin 輸入包含 prevResult，他必些直接把 prevResult 包在 result structure 內或對他修改後在包在 result structure 內，runtime 會把前一個 CNI 輸出的 prevResult 包在下一個 CNI 的 stdin 輸入內。 如果 CNI plugin 嘗試建立介面時，該介面已經存在，應該發出錯誤。 Runtime 不應該在沒有 DEL 的情況下對一個 CNI_CONTAINERID 容器的一個 CNI_IFNAME 介面多次呼叫 ADD。不過可能對同一個 container 的不同介面呼叫 ADD。 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 、 CNI_NETNS 和 CNI_IFNAME 。 CNI_ARGS 和 CNI_PATH 為可選項。 DEL移除 CNI_NETNS 內的 CNI_IFNAME 介面，或還原 ADD 套用的網路設定 通常來說，如果要釋放的資源已經不存在，DEL 也應該視為成功。例如容器網路已經不存在了，一個 IPAM plugin 應該還是要正常的釋放 IP 和回傳成功，除非 IPAM plugin 對容器網路存在與否有嚴格的要求。即便是 DHCP plugin，雖然執行 DEL 操作的時侯，需要透過容器網路發送 DHCP release 訊息，但是由於 DHCP leases 有 lifetime 的機制，超時後會自動回收，因此即便容器網路不存在，DHCP plugin 在執行 DEL 操作時，也應該該回傳成功。 如果重複對一個 CNI_CONTAINERID 容器的 CNI_IFNAME 介面執行多次 DEL 操作，plguin 都應該回傳，即便介面已經不存在或 ADD 套用的修改已經還原。 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 和 CNI_IFNAME。 CNI_NETNS、CNI_ARGS 和 CNI_PATH 為可選項。 CHECKRuntime 透過 CHECK 檢查容器的狀態，並確保容器網路符合預期。CNI spec 可以分為 plugin 和 runtime 的兩部分 Plugin: plugin 必須根據 prevResult 來判斷介面和地址是否符合預期 plugin 必須接受其他 chained plugin 對介面修改的結果 如果 plugin 建立並列舉在 prevResult 的 CNI Result type 資源 (介面、地址、路由規則) 不存在或不符合預期狀態，plugin 應該回傳錯誤 如果其他不在 Result type 內的資源不存在或不符合預期也應該回垂錯誤。可能的資源如下 防火牆規則 流量控管 (Traffic shaping controls) IP 保留 (Reservation) 外部依賴，如連接所需的 daemon 如果發現容器網路是無法訪問的應該回傳錯誤 plugin 必須在完成 ADD 後能夠立即處理 CHECK 指令，應此 plguin 應該要容忍非同步完成的網路設置在一定的時間內不符合預期。 plugin 執行 CHECK 時，應該呼叫所有 delegated plugin 的 CHECK，並把 delegated plugin 的錯誤傳給 plugin 的呼叫者 Runtime: Runtime 不應該在未執行 ADD 前或已經 DEL 後沒在執行一次 ADD 的容器執行 CHECK 如果 configuration 的 disableCheck 為真，runtime 不應呼叫 disableCheck Runtime 呼叫 CHECK 時，configuration 必須在 prevResult 包含前一次 ADD 操作時最後一個 plugin 的 Result。Runtime 可能會使用 libcni 提供的 Result caching 功能。 如果其中一個 plugin 回傳錯誤，runtime 可能不會呼叫後面 plugin 的 CHECK Runtime 可能會在 ADD 完後的下一刻一直到 DEL 執行前的任何時間執行 CHECK Runtime 可能會假設一個 CHECK 失敗的容器會永久處於配置錯誤的狀態 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID、CNI_NETNS 和 CNI_IFNAME。 CNI_ARGS 和 CNI_PATH 為可選項。 除了 CNI_PATH 以外的參數必須和 ADD 時一致。 VERSIONPlugin 透過 stdout 輸出 JSON 格式的 version result type object，用於查看 CNI plugin 的版本。 Stdin 輸入的 JSON 物件只包含 cniVersion。環境變數參數只需要 CNI_COMMAND。 Section 3: Execution of Network Configurations這個章節描述了 runtime 如何解析 network configuration，並執行 CNI plugins。Runtime 可能會想要新增、刪除、檢查 configuration，並對應到 CNI plugin 的 ADD, DELETE, CHECK 操作。這個章節也定義 configuration 是如何改變並提供給 plugin 的。 對容器的 network configuration 操作稱之為 attachment，一個 attachment 通常對應到一個特定 CNI_CONTAINERID 容器的 CNI_IFNAME 介面。 生命週期 Runtime 必須在呼叫任何 CNI Plugin 之前，為容器建立新的 network namespace Runtime 一定不能同時在一個容器執行多個 plugin 命令，但是同時處理多個容器是可以的。因此 plugin 必須能夠處理多容器 concurrency 的問題，並在共享的資源 (e.g. IPAM DB) 實作 lock 機制 Runtime 必須確保 ADD 操作後必定執行一次 DEL 操作，即便 ADD 失敗。唯一可能的例外是如結點直接丟失之類的災難性事件。 DEL 操作可能連續多次執行 network configuration 在 ADD 和 DEL 操作之間，以及不同的為 attachment 之間應該保持一致不變 runtime 必須負責清除容器的 network namespace Attachment ParametersNetwork configuration 在不同的 attachments 間應該保持一致。不過 runtime 會傳遞其他每個 attachment 獨立的參數。 Container ID: 對應到 Section 2 CNI_CONTAINERID 環境變數 Namespace: 對應到 CNI_NETNS 環境變數 Container interface name: 對應到 CNI_IFNAME 環境變數 Generic Arguments: 對應到 CNI_ARGS 環境變數 Capability Arguments: CNI plugins search path: 對應到 CNI_PATH 環境變數 Adding an attachment對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 生成送給 plugin sdtin 的 configuration 只有第一個執行的 plugin 不會帶 prevResult 欄位，後續執行的 plugin 都會把前一個的 plugin 的結果放在 prevResult 執行 plugin 的執行檔。設置 CNI_COMMAND=ADD，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Runtime 必須持久保存最後一個 plugin 的結果，用於 check 和 delete 操作。 Deleting an attachment刪除 attachment 和添加基本上差不多的，差別是 plugin 的執行順序是反過來的，從最後一個開始 prevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 對 configuration 的 plugins 欄位反序的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=DEL，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Checking an attachment如同 Section 2 所述，Runtime 會透過 CNI plugin 檢查每個 attachment 是否正常運作中。需注意的是 Runtime 必須使用和 add 操作時一致的 attachment parameters 檢查和添加只有兩個差別 prevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 如果 network configuation 中 disableCheck 為真，則直接回傳成功 對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=CHECK，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Deriving execution configuration from plugin configuration在 add, delete, check 操作時，runtime 必須根據 network configuration 生成出 plugin 可以存取的 execution configuration (基本對應到 plugin configuration)，並填入內容。 如同 section 2 所述，execution configuration 使用 JSON 格式並透過 stdin 傳給 CNI plugin。 必要的欄位如下: cniVersion: 同 network configuraion 中 cniVersion 的值 name: 同 network configuraion 中 name 的值 runtimeConfig: runtime 需要和 plugin 可提供的 capabilities 的聯集 (capability 會在後面討論) prevResult: CNI plugin 回傳的 Result type 結果 capabilities 欄位必須被移除 其他 plugin configuration 的欄位應該被放入 execution configuration Deriving runtimeConfig相對於靜態的 network configuration 來說，runtime 可能會需要根據每個不同的 attachment 產生動態參數。雖然 runtime 可以透過 CNI_ARGS 傳遞動態參數給 CNI plugin，但是我們沒辦法預期說 CNI plugin 會不會接收這個參數。透過 capabilities 欄位，可以明定 plugin 支援的功能，runtime 根據 capabilities 及需求，動態生成設定並填入 runtimeConfig。CNI spec 沒有定義 capability，但是比較通用的 capability 有列舉在另外一份 文件。 以 kubernetes 常用的 Node port 功能來說，需要 CNI plugin 支援 portMappings 這個 capability。在 section 1 的定義中，plugin configuration 包含了 capabilities 欄位，在這個欄位填入 portMappings，讓 runtime 知道可以透過該 plugin 處理 port mapping。 123456{ &quot;type&quot;: &quot;myPlugin&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true }} Runtime 執行 CNI plugin 時，會根據 capabilities 生成 runtimeConfig，並填入對應的動態參數。 12345678910111213{ &quot;type&quot;: &quot;myPlugin&quot;, &quot;runtimeConfig&quot;: { &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 8080, &quot;containerPort&quot;: 80, &quot;protocol&quot;: &quot;tcp&quot; } ] } ...} Section 4: Plugin Delegation雖然 CNI 的主要的架構是一系列的 CNI plkugin 依次執行，但這樣的方式有些時候沒辦法滿足 CNI 的需求。CNI plugin 可能會需要將某些功能委託給另外一個 CNI plugin，並存在 plugin 呼叫 plugin 的情況，最常見的案例就是 IP 地址的分配管理。 通常來說，CNI plugin 應該要指定並維護容器的 IP 地址以及下達必要的 routing rules，雖然由 CNI plugin 自主完成可以讓 CNI plugin 有更大的彈性但是也加重了 CNI plugin 的職責和開發難度，讓不同的 CNI plugin 需要重複開發相同的 IP 地址管理邏輯，因此許多 CNI 將 IP 管理的邏輯委託給另一個獨立的 plugin，讓相關邏輯可以直接被複用。對此除了前面提到的 interface plugin 和 chanined plugin，我們定義了第三類的 plugin - IP Address Management Pligin (IPAM plugin)。 由主要的 CNI plugin 去呼叫 IPAM plugin，IPAM plugin 判斷網路介面的 IP 地址、gateway、routing rules，並回傳資訊給主要的 CNI plugin 去完成相對應設置。(IPAM plugin 可能會透過 dhcp 之類的 protocl, 儲存在本地的檔案系統資訊或 network configuration 的 ipam section 取得資訊) Delegated Plugin protocol和 Runtime 執行 CNI plugin 的方式一樣，delegated plugin 也是透過執行 CNI plugin 可執行程式的方式。主要的 plugin 在 CNI_PATH 路徑下搜尋 CNI plugin。delegated plugin 必須接收和主要 plugin 完全一致的環境變數參數，以及主要 plugin 透過 stdin 接收到的完整 execute configuration。如果執行成功則回傳 0，並透過 stdout 返回 Success result type output。 Delegated plugin execution procedure 當 CNI plugin 執行 delegated plugin 時: 在 CNI_PATH 路徑下搜尋 plugin 的可執行程式 使用 CNI plugin 的環境變數參數和 execute configuration，作為 delegated plugin 的輸入 確保 delegated plugin 的 stderr 會輸出到 CNI plugin 的 stderr 當 plugin 執行 delete 和 check 時，必須執行所有的 delegated plugin，並將 delegated plugin 的錯誤回傳給 runtime 當 ADD 失敗時，plugin 應該在回傳錯誤前，先執行 delegated plugin 的 DEL Section 5: Result Types Plugin 的回傳結果使用 JSON 格式，並有三種 Success Error Version Success 如果 plugin 的輸入包含 prevResult，輸出必須包含該欄位的值，並加上該 plugin 對網路修改的資訊，如果該 plugin 沒有任何操作，則該欄位必須保持原輸入內容。 Success Type Result 的欄位如下 cniVersion: 同輸入的 cniVersion 版本 interfaces: 一個該 plugin 建立的 interface 資訊的陣列，包含 host 的 interface。 name: interface 的名子 mac: interface 的 mac address sandbox: 該介面的網路環境參考，例如 network namespace 的路徑，如果是 host 介面則該欄位為空，容器內的介面該值應為 CNI_NETNS ips: 該 plugin 指定的 ip address: CIDR 格式的 ip address (e.g. 192.168.1.1/24) gateway: default gateway (如果存在的話) interface: 介面的 index，對應到前述 interfaces 陣列 routes: plugin 建立的 routing rules dst: route 的目的 (CIDR) gw: nexthop 地址 dns nameservers: DNS server 位置陣列 (ipv4 或 ipv6 格式) domain: DNS 搜尋的 local domain search (list of strings): 有優先度的 search domain options (list of strings): 其他給 dns resolver 的參數 Delgated plugin 可能會忽略不需要的欄位 IPAM 必須回傳一個 abbreviated Success Type result (忽略 interfaces 和 ips 裡面的 interface 欄位) Errorplugin 回傳的錯誤資訊，欄位有 cniVersion, code, msg, details。 123456{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;code&quot;: 7, &quot;msg&quot;: &quot;Invalid Configuration&quot;, &quot;details&quot;: &quot;Network 192.168.0.0/31 too small to allocate from.&quot;} Error code 0-99 被保留給通用的錯誤，100 以上是 plugin 自定義的錯誤。 Error code 描述 1 不支援的 CNI 版本 2 不支援的 network configuration 欄位，error message 會包含不支援的欄位名稱和數值 3 未知或不存在的容器，這個錯誤同時代表 runtime 不需要執行 DEL 之類的清理操作 4 無效的環境環境變數參數，error message 會包含無效的欄位名稱 5 IO 錯誤，例如無法讀取 stdin 的 execute configuration 6 解析錯誤，例如無效的 execute configuration JSON 格式 7 無效的 network configuration 11 稍後在嘗試，存在暫時無法操作的資源，runtime 應該稍後重試 此外，stderr 也可被用於非 JSON 結構的錯誤訊息，如 log Version Version Type Result 的欄位如下 cniVersion: 同輸入的 cniVersion 版本 supportedVersions: 一個支援的 CNI 版本陣列 1234{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;supportedVersions&quot;: [ &quot;0.1.0&quot;, &quot;0.2.0&quot;, &quot;0.3.0&quot;, &quot;0.3.1&quot;, &quot;0.4.0&quot;, &quot;1.0.0&quot; ]} Appendix:在 CNI SPEC 裏面包含了一些 configuration 和執行過程中 plugin 輸入輸出的範例，可以參考 原始文件另外還有一份 CNI 的文件 CONVENTIONS，描述許多 spec 裡面沒有定義但是許多 plugin 常用的欄位。 小節以上是對 CNI spec 1.0.0 的導讀，希望對大家有幫助。","link":"/Kubernetes/CNI-Spec-Guiding/"},{"title":"ONOS P4 Switch Pipeconf 開發","text":"這篇文章會介紹一下，在 ONOS 上 P4 相關的模組和功能，以及怎麼開發 ONOS APP 與設置 ONOS，讓 ONOS 可以控制 p4 switch 的 pipeline。 背景介紹P4RuntimeP4Runtime 是 P4 API Working Group 制定的一套基於 Protobuf 以及 gRPC 的傳輸協定，他可以提供不同的 P4 交換機和不同的 SDN 控制器一套統一的 API 標準，提供控制器直接透過 p4runtime 將編譯出來 p4 pipeline 直接上傳到 p4 switch 上、設置 p4 pipeline 的 table entry 以及接收 packet-in 的封包和 counter 的資訊等功能。 ONOS 架構 ONOS 使用分層架構和介面抽象的方式隱藏了底層交換機和和控制協定的具體內容，讓上層的應用可以用統一的 API 來管理網路的行為，因此上層網路可以用完全相同的方式來控制 Openflow switch 及 P4 switch，也可以不用理會各個 switch table 順序和具體規則的下達方式。 ONOS Flow programmable 即便是在北向提供給使用者的 API，ONOS 也對其做了很多層級的抽象，讓使用者可以自由決定要使用哪一個層級的 API。 在 flow rule 的部分，ONOS 大致上提供了三個層級的抽象，分別是 Flow Rule, Flow Objective 及 Intent ，不論在哪一個層級，我們主要都是要操作兩個集合 Selector 和 Treatment Selector 決定了哪些封包受這條 flow rule 管理。一個 Selector 包含了若干個 Criterion ，ONOS 透過 Enum 定義了常用的 Criterion Type，來對應封包的不同欄位，例如 IPV4_SRC , ETH_SRC 等。 Treatment 則是 Instruction 的集合，一個 instruction 通常對應到對封包的某個欄位進行修改，或著指定封包在交換機的 output port。 三個抽象層積的差別在於這兩個集合套用到的對象，在最高層級的 Intent ，我們操作的對象是整個網路流，除了 Selector 和 Treatment，我們還要定義整個網路流在 SDN 網路的入口 (Ingress Point) 和出口 (Egress Point)，ONOS 核心的 Intent Service 會幫我們把一個 Intent 編譯成多個 FlowObjective。 由於 Intent 操作的是整個網路流，在這個層級定義 Output port 是沒有意義的，但是由於在 ONOS 使用的 JAVA API 是共通的，所以 intent service 會忽略掉這個 instruction，這個在 ONOS 的實作上是很重要的觀念，對 treatment 裡的 instructions，底層的編譯器只會取他可以處理的 instructions 往更底層送，對於不可以處理的 instructions，有些會有 warnning log，有些會直接跳 exception，更有的會直接忽略，因此如果 selector 和 treatment 的執行結果不符合我們的預期，有可能是有些不支援的 instruction 在轉換成交換機可以懂得規則的過程中被忽略的。 FlowObjective 操作的對象是一台網路設備 (通常是一台交換機)，同樣我們定義一個 Selector 和 Treatment ，告訴這台交換機我們要處理哪些封包和怎麼處理。 最底下到了 Flow Rule 這個層級，Flow rule 這個層級對象是交換機上的一張 table，因此他加入了 table id 這個欄位。一個 FlowObjective 可能會包含多個不同的 instruction，例如我們要修改封包的 mac address，修改 ip 的 ttl 欄位，同時也要決定這個封包的 output port，這些 instruction 在 flow objective 層級可以包含在一個 treatment 內，但是在實際的交換機上這些 instruction 可能分別屬於不同的 table，因此一個 flow objective 會需要對應到一條或多條得 flow rule，這依據底下交換機的不同、傳輸協定的不同而不同，因此 ONOS 引入了 Pipeliner ，Driver 可以實作 Pipeliner 的介面，讓 ONOS 知道如何把 flow objective 轉換成 flow rules。 P4 in ONOS 上圖是 p4 在 ONOS 南向架構上的組件 在 Protocol layer 由 P4Runtime 組件實作 p4runtime procotol，維護 switch 的連線和具體的 gRPC/Protobuf 傳輸內容 接著是 Driver layer，不同的 p4 switch 在 pipeline 的結構等方面存在差異，因此在 ONOS 設定交換機資訊時要根據不同的 Switch 選擇 driver 如果是 bmv2 switch 使用 org.onosproject.bmv2，如果是 tofino 交換機使用 org.onosproject.barefoot 最上面是 ONOS 核心，這邊有 translation services 和 pipeconf 架構。p4 交換機的特色是能透過 p4lang 定義出完全不同的 pipeline，在使用 ONOS 控制 p4 switch 的時候，我們就需要針對 pipeline 定義註冊 pipeconf，ONOS 核心可以調用 pipeconf 將 flow objective 或 flow rule 的 flow rule 轉換成真正可以下達到 p4 pipeline table 上的 entry。在 ONOS 核心定義的這套 pipeconf 及轉換架構被稱之為 Pipeline Independent (PI) framework，因此 ONOS 相關的 class 和 interface 都會有一個 PI 的前綴。 另外就要提到 Pipeline-agnostic APP 和 Pipeline-aware APP 的差別，這邊指的都是北向介面上面處理網路邏輯的 APP，差別在於 Pipeline-agnostic app 完全不考慮底下的 pipeline，因此通常操作的是 flow objective，而 pipeline aware app 必須知道底層 pipeline 的架構，直接產出特定的 flow rule。 開發 Pipeline-agnostic APP 的好處是他足夠抽象因此可以應用在各種不同的交換機和網路，但是我們就需要額外實作 pipeliner 等編譯器來做轉換，因此直接如果只針對單一 pipeline 的情況下，直接開發 pipeline aware app 會比較簡單。 Pipeconf 開發在使用 ONOS 控制 p4 switch 時，最基本要做的就是撰寫 pipeline 對應的 pipeconf。一個完整的 Pipeconf 會包含 從 p4 compiler 拿到 pipeline 資訊檔案，p4info, bmv2 json, Tofino binary…. PipeconfLoader: 一個 pipeconf 的進入點，向 PiPipeconfService 註冊一個或多個 pipeconf，定義 pipeconf 的 id, 對應的 interpreter, pipeliner, p4info 檔案路徑等資訊。 Interpreter: 主要負責兩件事 提供 ONOS 核心資訊並協助將 common flow rule 轉換成 protocol independent flow rule，包含了 table id 的 mapping, 欄位名稱和數值的轉換等 處理 packet-in/packet-out 的封包，當封包從 p4 switch packet-in 到 controller 時，會把 metadata (input port 等資訊) 當作 packet 的一個 header 附加在 packet 中，一起送到 controller，pipeconf 需要解析封包，將封包資訊提取出來。當封包 packet-out 時，同樣需要 metadata 轉換成 pipeline 定義的 packet-out header，附加在封包內送至交換機，pipeline parser 才能重新將資訊解析出來處理。 Pipeliner: 負責將 flow objective 轉換成 flow rule 但是 Interpreter 和 Pipeliner 的功能是不一定要實作的，如果對應的功能沒有被實作，那北向的 APP 就只能呼叫比較底層的 API 而無法調動 flow objective 等功能。 開發目標下面會用一個非常簡單的 p4 pipeline 作為範例，我們預期要實作一個基本的 Layer 2 switch pipeline，使 ProxyARP APP 和 Reactive Forwarding APP 能夠正常的運作。(使用 bmv2 和 ONOS v2.7.0 開發) 建立 ONOS APP跟任何其他 ONOS 模組一樣，pipeconf 可以作為一個獨立的 ONOS APP 開發，再安裝到 ONOS 上，所以首先建立我們的 simepleswitch APP 12345onos-create-appDefine value for property 'groupId': me.louisif.pipelinesDefine value for property 'artifactId': simpleswitchDefine value for property 'version' 1.0-SNAPSHOT: :Define value for property 'package' me.louisif: : me.louisif.pipelines.simpleswitch pom.xml在 pipeconf 的開發中，使用到 p4 相關的 api 並沒有被包含在 onos 標準 api 內，需要額外加入 p4runtime-model 這個 dependency。 12345&lt;dependency&gt; &lt;groupId&gt;org.onosproject&lt;/groupId&gt; &lt;artifactId&gt;onos-protocols-p4runtime-model&lt;/artifactId&gt; &lt;version&gt;${onos.version}&lt;/version&gt;&lt;/dependency&gt; 另外可以在 onos 的 dependency app 列表內加入 pipeline 對應的 driver，這樣啟動 pipeconf 時就會自動啟用相關的 driver app，而不用事前手動啟動。 123&lt;properties&gt; &lt;onos.app.requires&gt;org.onosproject.drivers.bmv2&lt;/onos.app.requires&gt;&lt;/properties&gt; P4 撰寫接著撰寫我們的 p4 檔案，路徑是 src/main/resource/simpleswitch.p4。resource 資料夾在編譯的時候會被附加到編譯出來的 oar 裡面，所以可以直接在 ONOS 執行的時候存取到編譯出來的 p4info 等檔案。完整的檔案在 github 上。 在 simpleswitch 的 ingress pipeline 內只包含一張 table0，用於 L2 的 packet forwarding，使用 send 這個 action 來將封包丟到指定的 output port，在 bmv2 switch 會定義一個 cpu port，當 egress_port 為該 port number 時，封包就會被送至 ONOS，因此 send_to_cpu 這個 action 只單純做 set egress port 這個動作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950control table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send(port_t port) { standard_metadata.egress_spec = port; } action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; } action drop() { mark_to_drop (standard_metadata); } table table0 { key = { standard_metadata.ingress_port : ternary; hdr.ethernet.src_addr : ternary; hdr.ethernet.dst_addr : ternary; hdr.ethernet.ether_type : ternary; } actions = { send; send_to_cpu; drop; } default_action = drop; size = 512; } apply { table0.apply (); }}control MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { // 這個後面在介紹 //if (standard_metadata.ingress_port == CPU_PORT) { // standard_metadata.egress_spec = hdr.packet_out.egress_port; // hdr.packet_out.setInvalid (); // exit; // } else { table0_control.apply (hdr, meta, standard_metadata); // } }} P4 編譯編譯 bmv2 pipeline 可以直接複製 ONOS 內建的 basic pipeline 使用的 編譯腳本，將 Makefile 和 bmv2-compile.sh 這兩個檔案複製到 resources 資料夾下，然後修改 Makefile 把 basic 改成 simpleswitch，並刪除 int pipeline。可以簡單下 make 來完成 pipeline 的編譯。 12345678910111213141516ROOT_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST))))/..all: p4 constantsp4: simpleswitch.p4 @./bmv2-compile.sh &quot;simpleswitch&quot; &quot;&quot;constants: docker run -v $(ONOS_ROOT):/onos -w /onos/tools/dev/bin \\ -v $(ROOT_DIR):/source \\ --entrypoint ./onos-gen-p4-constants opennetworking/p4mn:stable \\ -o /source/java/me/louisif/pipelines/simpleswitch/SimpleswitchConstants.java \\ simpleswitch /source/resources/p4c-out/bmv2/simpleswitch_p4info.txtclean: rm -rf p4c-out/bmv2/* 這個 makefile 主要分為兩個部分。p4 會呼叫 bmv2-compile，編譯出 bmv2 的 p4info 和描述 pipeline 的 json 檔案。constants 則會使用 p4mn 這個 container 生成出一個 SimpleswitchConstants.java 的檔案，這個檔案會把 pipeline 所有 table 名稱、欄位、action 名稱列舉出來，方便 pipeconf 的程式碼直接調用，以 table0 來說，它的完整名稱為 MyIngress.table0_control.table0 ，可以使用 MY_INGRESS_TABLE0_CONTROL_TABLE0 變數來代表。 12public static final PiTableId MY_INGRESS_TABLE0_CONTROL_TABLE0 = PiTableId.of (&quot;MyIngress.table0_control.table0&quot;); 最小可執行 Pipeconf撰寫 PipeconfLoader接著我們要先寫一個最小可以動的 pipeconf，只包含 PipeconfLoader.java 這個檔案，路徑是 src/main/java/me/louisif/simpleswitch/PipeconfLoader.java，前文提到 PipeconfLoader 的工作是向 PipeconfService 註冊 pipeconf 的資訊，因此我們幫 PipeconfLoader 加上 Component 的 Annotation，讓 activate function 在 APP 啟動時被呼叫，接著在 activate function 裡面去註冊 pipeconf。 以我們的例子而言，我們使用的是 bmv2 的 pipeline，所以我們可以這樣寫 12345678910final URL jsonUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/bmv2/simpleswitch.json&quot;);final URL p4InfoUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/bmv2/simpleswitch_p4info.txt&quot;);PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build ();piPipeconfService.register (pipeconf); 檔案路徑要填寫相對於 resource 這個資料夾的路徑，另外 addExtension 的內容會根據 switch 的不同而不同，如果我們今天使用的是 tonifo 的 pipeline 那就要改成 123456789101112final URL binUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/tofino.bin&quot;);final URL p4InfoUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/p4info.txt&quot;);final URL contextJsonUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/context.json&quot;);DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (parseP4Info (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (TOFINO_BIN, binUrl) .addExtension (TOFINO_CONTEXT_JSON, contextJsonUrl) .build ();piPipeconfService.register (pipeconf); 測試完成 PipeconfLoader.java 就構成了一個可以運作的 pipeconf，為了測試我們使用 opennetworking/p4mn 這個 container 來實驗，p4mn 是一個 mininet 的 docker image，可以很簡單的啟動一個 mininet 的測試拓譜，並使用 bmv2 switch 取代 mininet 原本使用的 openflow switch。 1docker run -v /tmp/p4mn:/tmp --privileged --rm -it -p 50001:50001 opennetworking/p4mn:stable 使用這個指令可以啟動一個包含一個叫做 bmv2-s1 的 bmv2 switch 和兩個 host 同時會生成一個 onos 的 netcfg 檔案，路徑 /tmp/p4mn/bmv2-s1-netcfg.json 1234567891011121314151617181920212223242526272829{ &quot;devices&quot;: { &quot;device:bmv2:s1&quot;: { &quot;ports&quot;: { &quot;1&quot;: { &quot;name&quot;: &quot;s1-eth1&quot;, &quot;speed&quot;: 10000, &quot;enabled&quot;: true, &quot;number&quot;: 1, &quot;removed&quot;: false, &quot;type&quot;: &quot;copper&quot; }, &quot;2&quot;: { &quot;name&quot;: &quot;s1-eth2&quot;, &quot;speed&quot;: 10000, &quot;enabled&quot;: true, &quot;number&quot;: 2, &quot;removed&quot;: false, &quot;type&quot;: &quot;copper&quot; } }, &quot;basic&quot;: { &quot;managementAddress&quot;: &quot;grpc://localhost:50001?device_id=1&quot;, &quot;driver&quot;: &quot;bmv2&quot;, &quot;pipeconf&quot;: &quot;me.louisif.pipelines.simpleswitch&quot; } } }} 這個檔案包含了 ONOS P4 需要的所有資訊，主要分為兩個部分，ports 定義了這個交換機所有的 port 資訊。basic 部分，managementAddress 是 ONOS 用來使用 p4runtime 連線到 switch 的路徑，pipeconf 指定了這個 switch 使用的 pipeconf，預設會是 org.onosproject.pipelines.basic，在我們的範例中需要改為 me.louisif.pipelines.simpleswitch。 只要將 me.louisif.pipelines.simpleswitch 的 app 啟用，並上傳 bmv2-s1-netcfg.json，ONOS 就可以成功連線到這個 bmv2 switch，並正常提供北向的 APP 服務了。 如何下 flow rule完成 pipeconf 後，我們就可以透過下 flow rule 的方式來讓 switch 工作了，為了要讓 h1 和 h2 能夠互相溝通，最簡單的方法就是將所有 port 1 進來的封包送到 port 2、所有從 port 2 進來的封包送到 port 1。 為此我們需要兩條 table0 的 entry，分別是 standard_metadata.ingress_port == 1 (mask 0x1ff) → send port=2 standard_metadata.ingress_port == 2 (mask 0x1ff) → send port=1 由於 standard_metadata.ingress_port 這個 key 是 ternary，因此我們需要包含 mask, port 這個 type 的長度是 9 bits，因為要完全一致，所以 mask 是 0x1ff。 123456789101112final PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (PiMatchFieldId.of (&quot;standard_metadata.ingress_port&quot;), inPortNumber.toLong (), 0x1ff);final PiAction piAction = PiAction.builder ().withId (PiActionId.of (&quot;MyIngress.table0_control.send&quot;)) .withParameter (new PiActionParam(PiActionParamId.of (&quot;port&quot;), outPortNumber.toLong ())) .build ();final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (PiTableId.of (&quot;MyIngress.table0_control.table0&quot;)).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();flowRuleService.applyFlowRules (flowRule); 我們可以透過這樣的方式來下達第一條 table entry，可以發現和平常的 flow entry 不一樣的地方是 table id 使用的是 PiTableId 這個 type，並指定了 table0 的完整 id MyIngress.table0_control.table0，另外 Selector 和 Treatment 分別使用了 matchPi 和 piTableAction 這兩個特別的函數。 我們這邊將只使用 PiTableId, PiCriterion 和 PiAction 定義的 flow rule 稱之為 PI flow rule，PI flow rule 是 onos 的 p4runtime 可以直接處理的 flow rule，所有的欄位名稱都唯一對應到 p4 pipeline 的某個欄位，前面提到這些 PiTableId, PiMatchFieldId 等都會在 SimpleswitchConstants.java 內被定義，因此可以直接使用來縮短程式碼長度。 1234567891011121314import static me.louisif.pipelines.simpleswitch.SimpleswitchConstants.*;final PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (INGRESS_PORT, inPortNumber.toLong (), 0x1ff);final PiAction piAction = PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (PORT, outPortNumber.toLong ())) .build ();final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();flowRuleService.applyFlowRules (flowRule); 在範例程式碼裡面包含了簡單的 cli 指令實作，因此可以在 ONOS CLI 使用 add-pi-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt; 的方式來下達上面的 flow rule。詳情可以 參考檔案。 當然這樣編寫出來的 flow rule 會產生一個問題，如果相同的 pipeline，我們希望能從 bmv2 移植到 tonifo 上面去使用，由於欄位的名稱會存在差異，因此所有下達 flow rule 的 APP 都需要重新寫，顯然這樣不是一個很好的做法，增加了程式碼維護上的困難，因此 ONOS 加入前面提到的 Interpreter 還有 translator 機制，下一節會介紹如何為 pipeline 編寫 Interpreter 還有用比較通用的方法來下 flow rule，在完成 interpreter 後上述的 flow rule，可以用下面這個我們比較熟悉的方法來下達。 1234567891011private static final int TABLE0_TABLE_ID = 0;final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (TABLE0_TABLE_ID).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build ();flowRuleService.applyFlowRules (flowRule); 撰寫 Interpreter在前一節我們完成了基本的 pipeconf 註冊，並使用 PI flow rule 的方式來控制交換機，但是直接使用 PI flow rule 會降低 APP 的彈性，使移植到不同 pipeline 的困難度提高。另外 SDN 的一個特色是可以使用 packet-in/packet-out 的方式來讓 controller 即時性的處理封包，因此本結會介紹如何實作 Interpreter 來提供 packet-in/packet-out，以及 flow rule translation 的功能。 要提供一個 pipeline interpreter，我們需要實作 PiPipelineInterpreter 這個介面，要注意的是 Interpreter class 需要繼承 AbstractHandlerBehaviour，然後再 PipeconfLoader 去指定這個實作 1234567891011public class SimpleSwitchInterpreterImpl extends AbstractHandlerBehaviour implements PiPipelineInterpreter {}PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addBehaviour (PiPipelineInterpreter.class, SimpleSwitchInterpreterImpl.class) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build (); 當一條 flow rule 被加入到 ONOS 時，PI framework 會將其翻譯成 PI flow rule，也就是將非 PI * 的欄位轉換成 PI flow rule 的欄位。 123456final FlowRule flowRule = DefaultFlowRule.builder () .forTable (0) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build (); Table Id translation以前面提到的 port 1 送到 port 2 的 flow rule 來示範，我們需要把 table id 0，轉換成 PiTableId.of (&quot;MyIngress.table0_control.table0&quot;) ，這對應到 Interpreter 的 mapFlowRuleTableId 函數，我們可以定義一個 map 來記錄，table index 跟 Pi table id 之間的關係，然後實作 mapFlowRuleTableId。這邊的 id 0 並不具有特定的意義，只是單純我們 interpreter 定義的 table index，因此當 APP 使用時，需要知道這個 index 對應到的 table 具體是什麼功能，當然通常我們會再加上一層 pipeliner，透過 flow objective 來隱藏 table id 的細節。 12345678private static final Map&lt;Integer, PiTableId&gt; TABLE_MAP = new ImmutableMap.Builder&lt;Integer, PiTableId&gt;() .put (0, MY_INGRESS_TABLE0_CONTROL_TABLE0) .build ();@Overridepublic Optional&lt;PiTableId&gt; mapFlowRuleTableId(int flowRuleTableId) { return Optional.ofNullable (TABLE_MAP.get (flowRuleTableId));} Selector Translation我們通常使用 DefaultTrafficSelector.builder 來定義 Selector。 matchInPort 會在 selector 內加入一個 PortCriterion ，他的 criterion tpye 是 Criterion.Type.IN_PORT ，為此 Interpreter 需要根據 Criterion type，將其轉換成 p4 table 對應的 key，同樣我們使用 map 的方式來維護其關係。 123456789101112private static final Map&lt;Criterion.Type, PiMatchFieldId&gt; CRITERION_MAP = new ImmutableMap.Builder&lt;Criterion.Type, PiMatchFieldId&gt;() .put (Criterion.Type.IN_PORT, HDR_STANDARD_METADATA_INGRESS_PORT) .put (Criterion.Type.ETH_SRC, HDR_HDR_ETHERNET_SRC_ADDR) .put (Criterion.Type.ETH_DST, HDR_HDR_ETHERNET_DST_ADDR) .put (Criterion.Type.ETH_TYPE, HDR_HDR_ETHERNET_ETHER_TYPE) .build ();@Overridepublic Optional&lt;PiMatchFieldId&gt; mapCriterionType(Criterion.Type type) { return Optional.ofNullable (CRITERION_MAP.get (type));} 可能有些人會有些疑惑說，p4 table key 可能會是 exact 或是 ternary，那 ONOS 要怎麼把 matchInPort 轉換成 ternary mask 0x1ff 前面提到在註冊 pipeconf 時，我們透過 .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) 載入 p4info (在 ONOS 內稱之為 PiPipelineModel)，每個 key 具體的類型和資料長度會包含在 p4info 內，PI framework 在轉換時會根據不同 criterion type 和 key type 的不同和需求自動做轉換。 Treatment Translation不像 table id 和 criterion，在 p4 內，每個 action 是一個可帶參數的 function，和 ONOS 定義的 treatment 不存在簡單的對應關係，因此 interpreter 定義了 mapTreatment，輸入是 treatment 和 table id，輸出是 PiAction，讓 interpreter 完整的處理整個 treatment。 123456789101112131415161718192021222324252627282930@Overridepublic PiAction mapTreatment(TrafficTreatment treatment, PiTableId piTableId) throws PiInterpreterException { // 檢查 table id 是否有效，由於只有 table0 一張 table，因此這邊直接檢查 table id 是不是 table 0 if (!piTableId.equals (MY_INGRESS_TABLE0_CONTROL_TABLE0)) { throw new PiInterpreterException(&quot;Unsupported table id: &quot; + piTableId); } // 我們的 pipeline 只支援 set output port 一個 instruction if (treatment.allInstructions ().size () != 1 || !treatment.allInstructions ().get (0).type ().equals (Instruction.Type.OUTPUT)) { throw new PiInterpreterException(&quot;Only output instruction is supported&quot;); } PortNumber port = ((Instructions.OutputInstruction) treatment.allInstructions ().get (0)).port (); // 像 controller、flooding 這些特別的 port，稱之為 Logical port if (port.isLogical ()) { if (port.exactlyEquals (PortNumber.CONTROLLER)) { // 我們支援使用 send_to_controller 這個 action 將封包送到 ONOS return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND_TO_CPU) .build (); } else { throw new PiInterpreterException(&quot;Unsupported logical port: &quot; + port); } } 一般的使用用 send 這個 action 來傳送封包 < br> return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (new PiActionParam(PORT, port.toLong ())) .build ();} 到此我們已經完成了 flow rule 的轉換，可以使用 ONOS 標準的 flow rule 來操作 pipeline 了。 如果希望對轉換機制有更詳細的瞭解可以查看 ONOS 原始碼 Packet-in/Packet-outInterpreter 另外一個重要的功能是使 pipeline 支援 packet-in/packet-out 的功能。為此我們需要先修改我們的 p4 pipeline。 首先我們會先加入兩個特別的 header，並使用 controller_header anotation 標記，@controller_header (&quot;packet_in&quot;) 來得知這個 packet_in_header_t 對應的是 packet-in 時，附加在這個封包的 meta data，通常會定義 ingress port 來表示封包的 input port。同樣的 packet_out_header_t 是 packet-out 時，ONOS 送來 pipeline 處理的 meta data 1234567891011@controller_header (&quot;packet_in&quot;)header packet_in_header_t { bit&lt;7&gt; _padding; bit&lt;9&gt; ingress_port;}@controller_header (&quot;packet_out&quot;)header packet_out_header_t { bit&lt;7&gt; _padding; bit&lt;9&gt; egress_port;} 接著我們要修改 header、parser，從 ONOS packet out 出來的封包對 p4 交換機來說相當於從特定一個 port 送進來的封包，因此一樣會經過整個 pipeline 以 bmv2 來說，controller 的 port number 可以自由指定，在我們使用的 p4mn container 內這個 port 被定義成 255，為此我們在 pipeline 的 p4 檔案內定義了 CPU_PORT 巨集為 255 我們將 packet-in/packet-out header 加入到 headers 內，當 packet-out 時，packet_out 會在封包的開頭，因此在 parser 的 start state，我們先根據 ingress port 是不是 CPU_PORT 來決定是不是要 parse packet_out header。 12345678910111213141516171819202122232425262728struct headers_t { packet_in_header_t packet_in; packet_out_header_t packet_out; ethernet_h ethernet;}parser MyParser(packet_in packet, out headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { state start { transition select(standard_metadata.ingress_port) { CPU_PORT: parse_packet_out; default: parse_ethernet; } } state parse_packet_out { packet.extract (hdr.packet_out); transition parse_ethernet; } state parse_ethernet { packet.extract (hdr.ethernet); transition accept; }} 在 Ingress 部分，如果是 packet-out packet，我們沒有必要讓他經過整個 ingress pipeline，為此我們直接將 egress_spec 設置為 packet_out header 內的 egress_port，然後直接呼叫 exit。 123456789101112control MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.ingress_port == CPU_PORT) { standard_metadata.egress_spec = hdr.packet_out.egress_port; hdr.packet_out.setInvalid (); exit; } table0_control.apply (hdr, meta, standard_metadata); }} 我們的 table0 則加入了 send_to_cpu 這個 action，做的事情就是把 egress_spec 設定成 CPU port。 1234567control table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; }} 當封包要 packet-in 到 ONOS 時，需要將 packet-in header 設為 valid，並填入對應的資料，這個部分會再 Egress pipeline 完成 1234567891011control MyEgress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.egress_port == CPU_PORT) { hdr.packet_in.setValid (); hdr.packet_in.ingress_port = standard_metadata.ingress_port; hdr.packet_in._padding = 0; } }} 最後為了讓 packet-in header 能後被傳到 ONOS，需要再 deparser 加入該 header，注意 packet-out header 是為了讓 pipeline 能夠根據該 header 來處理 pecket-out header 用的，因此他不應該被加入到 deparser。 123456control MyDeparser(packet_out packet, in headers_t hdr) { apply { packet.emit (hdr.packet_in); packet.emit (hdr.ethernet); }} 接著回到我們的 interpreter，在 interpreter 內定義了兩個函數 mapInboundPacket 和 mapOutboundPacket，分別對應 packet-in 和 packet-out 封包的處理。先前我們在 p4 pipeline 定義了 packet_in 和 packet_out 的 header，這兩個函數最基本的功能是讀取和寫入這兩個 header 的資訊。由於這兩個函數的功能相對固定，因此可以直接從 ONOS 的 basic pipeline interpreter 複製過來修改。 12345678910111213141516171819202122232425262728@Override public Collection&lt;PiPacketOperation&gt; mapOutboundPacket(OutboundPacket packet) throws PiInterpreterException { TrafficTreatment treatment = packet.treatment (); // 由於 outbound packet 的內容通常不用在 switch 上在做修改，因此我們只需要取得 //set output port 的 instruction // 當然如果有特別的功能需求，可以透過修改 pipeline 來支援更多 instruction List&lt;Instructions.OutputInstruction&gt; outInstructions = ... ImmutableList.Builder&lt;PiPacketOperation&gt; builder = ImmutableList.builder (); for (Instructions.OutputInstruction outInst : outInstructions) { ... // 這邊透過呼叫 createPiPacketOperation 來填入 packet_out header 的資訊 builder.add (createPiPacketOperation (packet.data (), outInst.port ().toLong ())); ... } return builder.build (); }private PiPacketOperation createPiPacketOperation(ByteBuffer data, long portNumber) throws PiInterpreterException { PiPacketMetadata metadata = createPacketMetadata (portNumber); return PiPacketOperation.builder () .withType (PACKET_OUT) .withData (copyFrom (data)) .withMetadatas (ImmutableList.of (metadata)) .build (); } 123456789101112131415161718@Overridepublic InboundPacket mapInboundPacket(PiPacketOperation packetIn, DeviceId deviceId) throws PiInterpreterException { Ethernet ethPkt; ... ethPkt = Ethernet.deserializer ().deserialize (packetIn.data ().asArray (), 0, ... //packet_in header 的資訊會以 key-value 的方式存在 packetIn.metadatas Optional&lt;PiPacketMetadata&gt; packetMetadata = packetIn.metadatas () .stream ().filter (m -&gt; m.id ().equals (INGRESS_PORT)) .findFirst () // 從中提取出 input port number ImmutableByteSequence portByteSequence = packetMetadata.get ().value (); short s = portByteSequence.asReadOnlyBuffer ().getShort (); ConnectPoint receivedFrom = new ConnectPoint(deviceId, PortNumber.portNumber (s)); ByteBuffer rawData = ByteBuffer.wrap (packetIn.data ().asArray ()); return new DefaultInboundPacket(receivedFrom, ethPkt, rawData); ...} 到此我們已經完成了 interpreter 的實現了，Interpreter 目前還包含 mapLogicalPortNumber 和 getOriginalDefaultAction ，不過基本的 interpreter 不需要實現這兩個功能，所以這邊就不再展開介紹。 同樣使用範例程式碼時，可以在 ONOS CLI 使用 add-common-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt; 的方式來使用下達 ONOS 標準的 flow rule。詳情可以 參考檔案。 撰寫 Pipeliner到目前為止我們已經完成了 pipeconf 的基本功能，可以下 flow rule 還有使用 packet-in/packet-out 的功能，不過到目前我們還是需要直接使用 flow rule，要知道 table id 對應的功能，為了能夠隱藏 table 的細節還有銜接 ONOS 內建的網路功能 APP，我們需要實作 pipeliner 讓我們的交換機支援 flow objective 的功能。 和 Interpreter 類似，我們需要實作 Pipeliner 這個介面並繼承 AbstractHandlerBehaviour 然後在 PipeconfLoader 透過 addBehaviour (Pipeliner.class, SimpleSwitchPipeliner.class) 的方式加入。 123456789101112public class SimpleSwitchPipeliner extends AbstractHandlerBehaviour implements Pipeliner { private final Logger log = getLogger (getClass ()); private FlowRuleService flowRuleService; private DeviceId deviceId; @Override public void init(DeviceId deviceId, PipelinerContext context) { this.deviceId = deviceId; this.flowRuleService = context.directory ().get (FlowRuleService.class); }} 首先我們實作 init 方法，pipeliner 和交換機之間是 1 對 1 的關係，因此當交換機被初始化的時候，可以透過 init 方法取得 device 的 id。context 最主要的部份是使用 directory ().get 方法。平常我們在 APP 開發時是使用 Reference annotation 來取得 onos 的 service，這邊我們可以直接透過 get 方法來取得 service，由於 pipeliner 需要完成 flow ojbective 到 flow rule 的轉換，並直接送到 flow rule service，因此這邊先取得 flow rule service。 在 ONOS Flow Objective Service 的架構內，其實總共有三種 objective，分別是 forward、filter 和 next，他們都需要透過 pipeliner 來和 ONOS 核心互動，由於我們只想要實作 forwarding objective 的部分，因此 filter 和 next 可以單純回應不支援的錯誤訊息 12345678910111213@Overridepublic void filter(FilteringObjective obj) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));}@Overridepublic void next(NextObjective obj) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));}@Overridepublic List&lt;String&gt; getNextMappings(NextGroup nextGroup) { // We do not use nextObjectives or groups. return Collections.emptyList ();} 接著就到了我們的主角 forward objective，在實作邏輯上其實與 interpreter 對 treatment 的處理方式類似，forward 方法會取得一個 ForwardingObjective 物件，我們根據 treatment 和 selector 生成出一條或多條 flow rule，然後透過 flow rule service 下放到交換機上。 12345678910111213141516171819202122232425262728293031323334@Overridepublic void forward(ForwardingObjective obj) { if (obj.treatment () == null) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED)); } // Simply create an equivalent FlowRule for table 0. final FlowRule.Builder ruleBuilder = DefaultFlowRule.builder () .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0) .forDevice (deviceId) .withSelector (obj.selector ()) .fromApp (obj.appId ()) .withPriority (obj.priority ()) .withTreatment (obj.treatment ()); if (obj.permanent ()) { ruleBuilder.makePermanent (); } else { ruleBuilder.makeTemporary (obj.timeout ()); } switch (obj.op ()) { case ADD: flowRuleService.applyFlowRules (ruleBuilder.build ()); break; case REMOVE: flowRuleService.removeFlowRules (ruleBuilder.build ()); break; default: log.warn (&quot;Unknown operation {}&quot;, obj.op ()); } obj.context ().ifPresent (c -&gt; c.onSuccess (obj));} 最後我們還需要實作 purgeAll，當刪除所有 flow obejctive 的時候，刪除所有的 flow rule，這邊我們只需要簡單呼叫 flow rule service 的 purgeFlowRules 就好。 1234@Overridepublic void purgeAll(ApplicationId appId) { flowRuleService.purgeFlowRules (deviceId, appId);} 到此我們已經完成了整個 pipeconf 的實作，可以透過 flow objective 的方式來管理交換機並與 ONOS 內建的 APP 整合，因此我們可以透過使用 proxyarp 和 fwd 兩個 APP 來讓我們的交換機能夠正常的工作。 小結以上就是 ONOS P4 Pipeconf 的基本開發教學，使用的範例程式碼在 github，如果有遇到任何問題或有說明不清楚的地方，歡迎留言提問，我會盡力為大家解答。 參考資料https://hackmd.io/@cnsrl/onos_p4 https://wiki.onosproject.org/pages/viewpage.action?pageId=16122675 https://github.com/p4lang/tutorials/blob/master/exercises/basic/solution/basic.p4 ONOS Basic Pipeline","link":"/ONOS/ONOS-P4-Switch-Pipeconf-Development/"},{"title":"使用 Terraform 部屬 Proxmox 虛擬機","text":"前言Proxmox 提供了 web GUI 來方便的建立和管理 LXC 和虛擬機，但是如果有大量的虛擬機需要建立，那麼使用 GUI 就會變得非常繁瑣，而且不利於版本控制。因此我們可以使用 Terraform 來完成自動化建立的過程。然而在 Proxmox 上使用 Terraform，我覺得相對 openstack 來說概念會比較複雜一點，因此花了一點點時間來釐清。這邊記錄下使用 terrform 管理 Proxmox 的基本操作，希望對大家有幫助。 Cloud-init 基本觀念在使用 Terraform 建立 Proxmox VM 的過程中，我們會使用到 cloud-init 這個技術。在使用 Promox GUI 設置虛擬機的過程中會有兩大麻煩的地方，第一個是需要在 web GUI 介面上一台一台的建立出來，第二個是需要在每台虛擬機上完成 OS 的安裝，設置硬碟、網路、SSH 等。前者我們透過 terraform 來解決，後者我們則會搭配利用 cloud-init。cloud-init 是一個業界標準，在許多 Linux 發行版還有公 / 私有雲上都有相對應的支援。各 Linux 發行版會發行特製的 cloud image 來支持 cloud-init。支援 cloud-init 的作業系統會在開機執行的時候執行透過特定方式去讀取使用者設定檔，自動完成前面提到的網路、帳號等設置，來達到自動化的目的。 Data Source在 cloud image 中，cloud-init 會根據設定檔來完成設置，而設定檔的來源 (Data source) 有很多種，不同的 cloud (AWS, Azure, GCP, Openstack, Proxmox) 在 cloud-init 標準下制定了不同的設定檔來源。(可參考 文件) 在 Proxmox 上支援 NoCloud 和 ConfigDrive 兩種資料源，兩種的執行方式相似，將使用者設定檔轉成一個特製的印象檔掛載到虛擬機上，當 VM 開機時 cloud-init 可以自動搜索到該印象檔，並讀取裡面的設定檔來完成設置。 前置作業首先我們要先安裝 Terraform 和在 proxmox 上安裝 cloud-init 的工具，這邊簡單直接把 Terraform 也裝在 promox host 上面。 1234567# cloud-initapt-get install cloud-init# Terraformwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpgecho &quot;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main&quot; | sudo tee /etc/apt/sources.list.d/hashicorp.listsudo apt update &amp;&amp; sudo apt install terraform Proxmox &amp; Cloud-init再透過 Terraform 自動部屬之前，我們要先看看怎麼在 Proxmox 上搭配 cloud-init 手動部屬 VM。 這邊我們透過 promox 的 CLI 工具來完成設置，不過操作也都可以透過 GUI 完成。 建立 VM123456789101112131415161718192021222324export VM_ID=&quot;9001&quot;cd /var/lib/vz/template/iso/wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.imgqm create $VM_ID \\ --name ubuntu-2004-focal-fossa \\ --ostype l26 \\ --memory 8192 \\ --balloon 2048 \\ --sockets 1 \\ --cores 1 \\ --vcpu 1 \\ --net0 virtio,bridge=vmbr11 \\ --onboot 1qm importdisk $VM_ID focal-server-cloudimg-amd64.img local-lvm --format qcow2qm set $VM_ID --scsihw virtio-scsi-pciqm set $VM_ID --scsi0 local-lvm:vm-$VM_ID-disk-0qm set $VM_ID --boot c --bootdisk scsi0qm set $VM_ID --ide2 local-lvm:cloudinitqm set $VM_ID --serial0 socket 前面提到 cloud-init 要使用特製的印象檔，這邊我們透過 wget 抓取印象檔，放到 /var/lib/vz/template/iso/ 路徑下，這是 proxmox 預設放置 ISO 檔的路徑，因此可以透過 GUI 到 storage/local/ISO image 的頁面看到我們剛剛下載的印象檔。 接著透過 qm create 指令建立 VM，這邊 balloon 參數對應到 Minimum memory 的設定。 這邊提供的 cloud image 提供的印象檔並不是通常我們用來安裝作業系統的 iso 安裝檔，而是 qcow2 文件，qcow2 是一種虛擬機硬碟快照格式，因此這邊我們透過 importdisk 指令，直接將 img 轉換成硬碟。 接著我們要將建立好的硬碟掛載到 VM 上，這邊我們指定 scsi0 介面，將硬碟掛載上去，同時由於我們要從 cloud image 開機，因此這邊直接將 bootdisk 設定為 scsi0。 Proxmox 官方文件有提到，ubuntu 的 cloud-init 映像，如果使用 scsi 接口掛載的話，需要將控制器設置為 virtio-scsi-pci。 接著我們需要添加兩個特殊的設備，首先是 cloudinit (GUI 顯示為 cloud driver)，這個是前面提到用於傳輸 cloud-init 設定檔的設備，當在 proxmox 上完成 cloud-init 設定後，proxmox 會生成對應的印象檔掛到 cloud driver 上， 另外由於 cloud image 的特殊性，我們需要添加一個 srial 設備。 到這邊設址結果如下圖： 設定 cloud-init接著我們要設定 cloud-init，這邊我們透過 GUI 的方式來完成設定。在 proxmox 上我們可以簡單的在 GUI 完成 cloudinit 的設定 (包含帳號、密碼、SSH key 等)，接著按下 Regenerage image 按鈕，proxmox 會生成設定檔，並掛載到前面建立的 cloud driver 上。 啟動 VM接著我們只要按下 Start 按鈕，VM 就會開機，並自動完成前面的 cloud-init 設定。 Cloud image 不太建議使用密碼登入，因此預設 VM 通常都會把 SSH 密碼登入關閉，因此需要透過 SSH key 登入，或著使用最後後提到的 cicustom 來修改 SSH 設定。 使用 Terraform接著我們就要搭配 Terraform 來將完成自動化部屬了。(Proxmox provider) 首先前面的指令不能丟，我們在最後加上一行 qm template $VM_ID，將 VM 轉成模板用於後續的 Terraform 部屬。這邊使用模板，目前研究起來有兩個原因，首先硬碟、cloud driver、serial 這些固定虛擬硬體和 cloud image 可以直接複製，而不用在 Terraform 上重新設定。另外 proxmox 的 terraform provider 好像不支援 importdisk 這樣導入 qcow2 印象檔的方式。 首先是 provider 的基礎設定 12345678910111213141516terraform { required_providers { proxmox = { source = &quot;Telmate/proxmox&quot; version = &quot;2.9.11&quot; } }}provider &quot;proxmox&quot; { # Configuration options pm_api_url = &quot;https://127.0.0.1:8006/api2/json&quot; pm_user = &quot;root@pam&quot; pm_password = &quot;password&quot; pm_tls_insecure = true} 這邊我們直接使用 root 的帳號密碼登入 proxmox web，不過為了安全和控管的話，建議還是建立額外的使用者給 terraform 使用，以及使用 token 來取代密碼。 1234567# 建立 terraform 使用者 pveum role add TerraformProv -privs &quot;VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit&quot;pveum user add terraform-prov@pve --password &lt;password&gt;pveum aclmod /-user terraform-prov@pve -role TerraformProv# 建立 tokenpveum user token add terraform-prov@pve terraform-token 接著我們透過 proxmox_vm_qemu 資源來建立 VM 123456789101112131415161718192021222324252627282930resource &quot;proxmox_vm_qemu&quot; &quot;resource-name&quot; { name = &quot;VM-name&quot; target_node = &quot;Node to create the VM on&quot; clone = &quot;ubuntu-2004-focal-fossa&quot; full_clone = true os_type = &quot;cloud-init&quot; onboot = true cores = 8 sockets = 1 cpu = &quot;host&quot; memory = 8192 balloon = 2048 scsihw = &quot;virtio-scsi-pci&quot; bootdisk = &quot;virtio0 disk { slot = 0 size = &quot;65536M&quot; type = &quot;scsi&quot; storage = &quot;local-lvm&quot; iothread = 1 } ipconfig0 = &quot;192.168.0.1/24,gw=192.168.0.254&quot; ciuser=&quot;username&quot; cipassword=&quot;password&quot; sshkeys = file (&quot;/root/.ssh/id_rsa.pub&quot;)} 首先當然是指定我們 VM 的名子還有要長在 proxmox cluster 的哪台機器上 (name, target_node)。接著我們指定我們要 clone 的我們剛剛做的 VM 模板 (clone) 並指定為完整複製 (full_clone)，以及指定 OS type 為 cloud-init。 接著是設定 VM 的 CPU、memory 等硬體規格，這邊要特別注意的是這先參數的規格，如果不指定，並不會套用模板的規格，而是 provider 預設的規格，因此我們需要指定這些參數。 接著比較特別的是我們要重新定義我們的硬碟，前面雖然我們已經將 cloud image 轉成硬碟掛載到 VM 上了，但是這樣掛載上去硬碟大小是絕對不夠用的 (以 ubuntu 的 image 來說只有 2G 多的硬碟大小)，因此我們這邊複寫修改 scsi0 的硬碟大小，cloud-init 在第一次開機的時候能夠自我察覺並修改分割區的大小來匹配新的硬碟容量。 最後就是 cloud-init 的設定，這邊我們指定 VM 的 IP、帳號密碼、以及 ssh key。 最後就一樣透過指令完成自動部屬 12terraform initterraform apply 到這邊我們就完成 terraform 與 proxmox 搭配的自動部屬了。 其他雜紀cicustom前面我們都是透過 proxmox 本身的功能來生成 cloud-init 的設定檔，但是 proxmox 提供的設置選項有限，因此有時候我們會需要直接修改 cloud-init 的設定檔，在 proxmox 上提供兩種方式來直接設定 cloud-init 設定檔的參數，一個是直接在指令上提供參數值，另外一個是直接提供 cloud-init 的 yaml 設定檔 12qm set 9000 --cicustom &quot;user=&lt;volume&gt;,network=&lt;volume&gt;,meta=&lt;volume&gt;&quot;qm set 9000 --cicustom &quot;user=local:snippets/userconfig.yaml&quot; 在 terraform 上面，我們一樣可以透過 cicustom 設定來達到相同的事情。 agent在查找資料時，在許多範例會看到指定 agent 參數為 0 或 1，這邊的 agent 指的是 Qemu-guest-agent，簡單來說就是在虛擬機內部安裝一個 agent 來當作 proxmox 直接操作虛擬機內部的後門，不過具體的功能就不在本篇的範圍內了，且預設情況下這個功能是關閉的。 結語這邊簡單紀錄了一下 terraform 和 proxmox 的搭配使用，在一開始研究的時候，cloud-init 還有使用 VM template 這兩件事，是之前在使用 terraform 或 proxmox 不會特別注意到的東西，因此會有點混亂和不知道功能，希望這篇文章能夠幫助到有需要的人。 參考資料 Terraform Provider for Proxmox Proxmox Cloud-Init","link":"/Terraform/Deploy-proxmox-vm-with-terraform/"}],"tags":[{"name":"P4","slug":"P4","link":"/tags/P4/"},{"name":"IaC Terraform Proxmox","slug":"IaC-Terraform-Proxmox","link":"/tags/IaC-Terraform-Proxmox/"}],"categories":[{"name":"ONOS","slug":"ONOS","link":"/categories/ONOS/"},{"name":"Openstack","slug":"Openstack","link":"/categories/Openstack/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"Terraform","slug":"Terraform","link":"/categories/Terraform/"}],"pages":[{"title":"bookmark","text":"","link":"/bookmark/index.html"},{"title":"about","text":"","link":"/about/index.html"}]}