{"posts":[{"title":"分析 ONOS Packet Processor Treatment 無效之原因","text":"ONOS 踩坑日記 前言最近嘗試使用 ONOS 目前最新的 2.7 版來開發 APP，用 OpenFlow 來讓交換機實現 router 的功能。結果踩到 ONOS Packet-in 封包處理實作未完全的坑。 當封包經過 router 時，會根據 routing table 和封包的目標決定要往哪個 interface 送出，同時將封包的 source mac address 改為交換機的 mac address、封包的 destination mac address 改為 nexthop 的 mac address。因此我們需要在交換機上安裝一條 flow rule，selector 是 destination mac address，treatment 有三個 instructions 分別是：修改 src mac、dst mac 和決定 output port。 為了減少交換機上的 flow entry 的數量，所以採用 reactive 的方式，也就是當交換機收到第一封包時，先將封包送 (packet-in) 給 SDN controller，controller 根據 routing table，直接修改該封包的 mac address，並從交換機特定的 port 送出 (packet-out)，同時生成對應的 flow rule 並安裝到交換機上，後續的封包就可以直接根據 flow rule 轉送而不用再經過 controller。 問題然而問題就出現在第一個封包上，根據 tcpdump 看到的結果，封包的 source 和 destination mac address 都沒有被修改到。 由於我是使用 OVS 來模擬 Openflow 交換機，因此首先懷疑是不是 OVS 本身實作限制，不支援同時包含上述三個 instructions 導致。然而，後續經過 flow rule 直接送出的封包，都有成功修改到 mac address。由於只有第一個 packet-in 到 controller，再 packet-out 回 switch 的封包沒有被修改，因此開始懷疑是 ONOS 的問題。 追蹤在 ONOS 裡面，一般使用 PacketProcessor 的方式來處理 packet-in 到 controller 的封包。首先實作 PacketProcessor 介面，然後向 PacketService 註冊，ONOS 就會調用 processor 處理 packet-in 的封包。 1234567891011121314151617private PacketProcessor processor = new PacketProcessor() { @Override public void process(PacketContext context) { //.... 處理封包的邏輯 // 修改設定封包的 mac address 和決定 output port context.treatmentBuilder () .setEthSrc (srcMac) .setEthDst (dstMac) .setOutput (outPort.port ()); context.send (); // 將封包 packet-out 回交換機 }};@Activateprotected void activate() { packetService.addProcessor (processor, PacketProcessor.director (1));} PacketContext 會包含 packet-in 進來的封包內容，並可透過 context.treatmentBuilder 修改封包和決定要往哪個 port 送出去，最後透過 send 指令，packet-out 回交換機。 搜查一下 ONOS 的原始碼，會在 core/api 下面找到 DefaultPacketContext ，這個 class 實作了 PacketContext 這個 Interface，但是這個 class 是一個 abstract class，因此一定有人繼承了它，繼續搜查 PacketContext 這個字會找到兩個跟 Openflow 相關的，DefaultOpenFlowPacketContext 和 OpenFlowCorePacketContext，但是後者才有繼承 DefaultPacketContext 和實作 PacketContext 介面，因此 PacketProcesser 在處理 openflow packet-in 進來的封包時，拿到的 PacketContext 具體應該是 OpenFlowCorePacketContext 這個 class。 打開 OpenFlowCorePacketContext.java 會看到它實現了 send 這個 function，經過簡單的檢查後呼叫 sendPacket 這個 function，然後你就會看到… 123456789101112private void sendPacket(Ethernet eth) { List&lt;Instruction&gt; ins = treatmentBuilder ().build ().allInstructions (); OFPort p = null; // TODO: support arbitrary list of treatments must be supported in ofPacketContext for (Instruction i : ins) { if (i.type () == Type.OUTPUT) { p = buildPort (((OutputInstruction) i).port ()); break; //for now... } } .......} 謎底揭曉，原來 ONOS 只有實作 output 這個 instruction (決定 output port)，因此它直接忽略的 set source mac 和 set destination mac 兩個指令，交換機送出來的封包當然就只有往對的 port 送，而沒有改到 mac address。 結論結論就是在當前 ONOS 2.7 環境下，PacketProcesser 在處理 Openflow 交換機封包 packet-out 的時候，只能決定該封包的 output port，其餘對該封包的修改都是無效的。 參考資料 ONOS Source code","link":"/ONOS/Analyze-why-ONOS-Packet-Processor-Treatment-not-Work/"},{"title":"ONOS P4 Switch Pipeconf 開發","text":"這篇文章會介紹一下，在 ONOS 上 P4 相關的模組和功能，以及怎麼開發 ONOS APP 與設置 ONOS，讓 ONOS 可以控制 p4 switch 的 pipeline。 背景介紹P4RuntimeP4Runtime 是 P4 API Working Group 制定的一套基於 Protobuf 以及 gRPC 的傳輸協定，他可以提供不同的 P4 交換機和不同的 SDN 控制器一套統一的 API 標準，提供控制器直接透過 p4runtime 將編譯出來 p4 pipeline 直接上傳到 p4 switch 上、設置 p4 pipeline 的 table entry 以及接收 packet-in 的封包和 counter 的資訊等功能。 ONOS 架構 ONOS 使用分層架構和介面抽象的方式隱藏了底層交換機和和控制協定的具體內容，讓上層的應用可以用統一的 API 來管理網路的行為，因此上層網路可以用完全相同的方式來控制 Openflow switch 及 P4 switch，也可以不用理會各個 switch table 順序和具體規則的下達方式。 ONOS Flow programmable 即便是在北向提供給使用者的 API，ONOS 也對其做了很多層級的抽象，讓使用者可以自由決定要使用哪一個層級的 API。 在 flow rule 的部分，ONOS 大致上提供了三個層級的抽象，分別是 Flow Rule, Flow Objective 及 Intent ，不論在哪一個層級，我們主要都是要操作兩個集合 Selector 和 Treatment Selector 決定了哪些封包受這條 flow rule 管理。一個 Selector 包含了若干個 Criterion ，ONOS 透過 Enum 定義了常用的 Criterion Type，來對應封包的不同欄位，例如 IPV4_SRC , ETH_SRC 等。 Treatment 則是 Instruction 的集合，一個 instruction 通常對應到對封包的某個欄位進行修改，或著指定封包在交換機的 output port。 三個抽象層積的差別在於這兩個集合套用到的對象，在最高層級的 Intent ，我們操作的對象是整個網路流，除了 Selector 和 Treatment，我們還要定義整個網路流在 SDN 網路的入口 (Ingress Point) 和出口 (Egress Point)，ONOS 核心的 Intent Service 會幫我們把一個 Intent 編譯成多個 FlowObjective。 由於 Intent 操作的是整個網路流，在這個層級定義 Output port 是沒有意義的，但是由於在 ONOS 使用的 JAVA API 是共通的，所以 intent service 會忽略掉這個 instruction，這個在 ONOS 的實作上是很重要的觀念，對 treatment 裡的 instructions，底層的編譯器只會取他可以處理的 instructions 往更底層送，對於不可以處理的 instructions，有些會有 warnning log，有些會直接跳 exception，更有的會直接忽略，因此如果 selector 和 treatment 的執行結果不符合我們的預期，有可能是有些不支援的 instruction 在轉換成交換機可以懂得規則的過程中被忽略的。 FlowObjective 操作的對象是一台網路設備 (通常是一台交換機)，同樣我們定義一個 Selector 和 Treatment ，告訴這台交換機我們要處理哪些封包和怎麼處理。 最底下到了 Flow Rule 這個層級，Flow rule 這個層級對象是交換機上的一張 table，因此他加入了 table id 這個欄位。一個 FlowObjective 可能會包含多個不同的 instruction，例如我們要修改封包的 mac address，修改 ip 的 ttl 欄位，同時也要決定這個封包的 output port，這些 instruction 在 flow objective 層級可以包含在一個 treatment 內，但是在實際的交換機上這些 instruction 可能分別屬於不同的 table，因此一個 flow objective 會需要對應到一條或多條得 flow rule，這依據底下交換機的不同、傳輸協定的不同而不同，因此 ONOS 引入了 Pipeliner ，Driver 可以實作 Pipeliner 的介面，讓 ONOS 知道如何把 flow objective 轉換成 flow rules。 P4 in ONOS 上圖是 p4 在 ONOS 南向架構上的組件 在 Protocol layer 由 P4Runtime 組件實作 p4runtime procotol，維護 switch 的連線和具體的 gRPC/Protobuf 傳輸內容 接著是 Driver layer，不同的 p4 switch 在 pipeline 的結構等方面存在差異，因此在 ONOS 設定交換機資訊時要根據不同的 Switch 選擇 driver 如果是 bmv2 switch 使用 org.onosproject.bmv2，如果是 tofino 交換機使用 org.onosproject.barefoot 最上面是 ONOS 核心，這邊有 translation services 和 pipeconf 架構。p4 交換機的特色是能透過 p4lang 定義出完全不同的 pipeline，在使用 ONOS 控制 p4 switch 的時候，我們就需要針對 pipeline 定義註冊 pipeconf，ONOS 核心可以調用 pipeconf 將 flow objective 或 flow rule 的 flow rule 轉換成真正可以下達到 p4 pipeline table 上的 entry。在 ONOS 核心定義的這套 pipeconf 及轉換架構被稱之為 Pipeline Independent (PI) framework，因此 ONOS 相關的 class 和 interface 都會有一個 PI 的前綴。 另外就要提到 Pipeline-agnostic APP 和 Pipeline-aware APP 的差別，這邊指的都是北向介面上面處理網路邏輯的 APP，差別在於 Pipeline-agnostic app 完全不考慮底下的 pipeline，因此通常操作的是 flow objective，而 pipeline aware app 必須知道底層 pipeline 的架構，直接產出特定的 flow rule。 開發 Pipeline-agnostic APP 的好處是他足夠抽象因此可以應用在各種不同的交換機和網路，但是我們就需要額外實作 pipeliner 等編譯器來做轉換，因此直接如果只針對單一 pipeline 的情況下，直接開發 pipeline aware app 會比較簡單。 Pipeconf 開發在使用 ONOS 控制 p4 switch 時，最基本要做的就是撰寫 pipeline 對應的 pipeconf。一個完整的 Pipeconf 會包含 從 p4 compiler 拿到 pipeline 資訊檔案，p4info, bmv2 json, Tofino binary…. PipeconfLoader: 一個 pipeconf 的進入點，向 PiPipeconfService 註冊一個或多個 pipeconf，定義 pipeconf 的 id, 對應的 interpreter, pipeliner, p4info 檔案路徑等資訊。 Interpreter: 主要負責兩件事 提供 ONOS 核心資訊並協助將 common flow rule 轉換成 protocol independent flow rule，包含了 table id 的 mapping, 欄位名稱和數值的轉換等 處理 packet-in/packet-out 的封包，當封包從 p4 switch packet-in 到 controller 時，會把 metadata (input port 等資訊) 當作 packet 的一個 header 附加在 packet 中，一起送到 controller，pipeconf 需要解析封包，將封包資訊提取出來。當封包 packet-out 時，同樣需要 metadata 轉換成 pipeline 定義的 packet-out header，附加在封包內送至交換機，pipeline parser 才能重新將資訊解析出來處理。 Pipeliner: 負責將 flow objective 轉換成 flow rule 但是 Interpreter 和 Pipeliner 的功能是不一定要實作的，如果對應的功能沒有被實作，那北向的 APP 就只能呼叫比較底層的 API 而無法調動 flow objective 等功能。 開發目標下面會用一個非常簡單的 p4 pipeline 作為範例，我們預期要實作一個基本的 Layer 2 switch pipeline，使 ProxyARP APP 和 Reactive Forwarding APP 能夠正常的運作。(使用 bmv2 和 ONOS v2.7.0 開發) 建立 ONOS APP跟任何其他 ONOS 模組一樣，pipeconf 可以作為一個獨立的 ONOS APP 開發，再安裝到 ONOS 上，所以首先建立我們的 simepleswitch APP 12345onos-create-appDefine value for property 'groupId': me.louisif.pipelinesDefine value for property 'artifactId': simpleswitchDefine value for property 'version' 1.0-SNAPSHOT: :Define value for property 'package' me.louisif: : me.louisif.pipelines.simpleswitch pom.xml在 pipeconf 的開發中，使用到 p4 相關的 api 並沒有被包含在 onos 標準 api 內，需要額外加入 p4runtime-model 這個 dependency。 12345&lt;dependency&gt; &lt;groupId&gt;org.onosproject&lt;/groupId&gt; &lt;artifactId&gt;onos-protocols-p4runtime-model&lt;/artifactId&gt; &lt;version&gt;${onos.version}&lt;/version&gt;&lt;/dependency&gt; 另外可以在 onos 的 dependency app 列表內加入 pipeline 對應的 driver，這樣啟動 pipeconf 時就會自動啟用相關的 driver app，而不用事前手動啟動。 123&lt;properties&gt; &lt;onos.app.requires&gt;org.onosproject.drivers.bmv2&lt;/onos.app.requires&gt;&lt;/properties&gt; P4 撰寫接著撰寫我們的 p4 檔案，路徑是 src/main/resource/simpleswitch.p4。resource 資料夾在編譯的時候會被附加到編譯出來的 oar 裡面，所以可以直接在 ONOS 執行的時候存取到編譯出來的 p4info 等檔案。完整的檔案在 github 上。 在 simpleswitch 的 ingress pipeline 內只包含一張 table0，用於 L2 的 packet forwarding，使用 send 這個 action 來將封包丟到指定的 output port，在 bmv2 switch 會定義一個 cpu port，當 egress_port 為該 port number 時，封包就會被送至 ONOS，因此 send_to_cpu 這個 action 只單純做 set egress port 這個動作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950control table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send(port_t port) { standard_metadata.egress_spec = port; } action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; } action drop() { mark_to_drop (standard_metadata); } table table0 { key = { standard_metadata.ingress_port : ternary; hdr.ethernet.src_addr : ternary; hdr.ethernet.dst_addr : ternary; hdr.ethernet.ether_type : ternary; } actions = { send; send_to_cpu; drop; } default_action = drop; size = 512; } apply { table0.apply (); }}control MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { // 這個後面在介紹 //if (standard_metadata.ingress_port == CPU_PORT) { // standard_metadata.egress_spec = hdr.packet_out.egress_port; // hdr.packet_out.setInvalid (); // exit; // } else { table0_control.apply (hdr, meta, standard_metadata); // } }} P4 編譯編譯 bmv2 pipeline 可以直接複製 ONOS 內建的 basic pipeline 使用的 編譯腳本，將 Makefile 和 bmv2-compile.sh 這兩個檔案複製到 resources 資料夾下，然後修改 Makefile 把 basic 改成 simpleswitch，並刪除 int pipeline。可以簡單下 make 來完成 pipeline 的編譯。 12345678910111213141516ROOT_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST))))/..all: p4 constantsp4: simpleswitch.p4 @./bmv2-compile.sh &quot;simpleswitch&quot; &quot;&quot;constants: docker run -v $(ONOS_ROOT):/onos -w /onos/tools/dev/bin \\ -v $(ROOT_DIR):/source \\ --entrypoint ./onos-gen-p4-constants opennetworking/p4mn:stable \\ -o /source/java/me/louisif/pipelines/simpleswitch/SimpleswitchConstants.java \\ simpleswitch /source/resources/p4c-out/bmv2/simpleswitch_p4info.txtclean: rm -rf p4c-out/bmv2/* 這個 makefile 主要分為兩個部分。p4 會呼叫 bmv2-compile，編譯出 bmv2 的 p4info 和描述 pipeline 的 json 檔案。constants 則會使用 p4mn 這個 container 生成出一個 SimpleswitchConstants.java 的檔案，這個檔案會把 pipeline 所有 table 名稱、欄位、action 名稱列舉出來，方便 pipeconf 的程式碼直接調用，以 table0 來說，它的完整名稱為 MyIngress.table0_control.table0 ，可以使用 MY_INGRESS_TABLE0_CONTROL_TABLE0 變數來代表。 12public static final PiTableId MY_INGRESS_TABLE0_CONTROL_TABLE0 = PiTableId.of (&quot;MyIngress.table0_control.table0&quot;); 最小可執行 Pipeconf撰寫 PipeconfLoader接著我們要先寫一個最小可以動的 pipeconf，只包含 PipeconfLoader.java 這個檔案，路徑是 src/main/java/me/louisif/simpleswitch/PipeconfLoader.java，前文提到 PipeconfLoader 的工作是向 PipeconfService 註冊 pipeconf 的資訊，因此我們幫 PipeconfLoader 加上 Component 的 Annotation，讓 activate function 在 APP 啟動時被呼叫，接著在 activate function 裡面去註冊 pipeconf。 以我們的例子而言，我們使用的是 bmv2 的 pipeline，所以我們可以這樣寫 12345678910final URL jsonUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/bmv2/simpleswitch.json&quot;);final URL p4InfoUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/bmv2/simpleswitch_p4info.txt&quot;);PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build ();piPipeconfService.register (pipeconf); 檔案路徑要填寫相對於 resource 這個資料夾的路徑，另外 addExtension 的內容會根據 switch 的不同而不同，如果我們今天使用的是 tonifo 的 pipeline 那就要改成 123456789101112final URL binUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/tofino.bin&quot;);final URL p4InfoUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/p4info.txt&quot;);final URL contextJsonUrl = PipeconfLoader.class.getResource (&quot;/p4c-out/context.json&quot;);DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (parseP4Info (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (TOFINO_BIN, binUrl) .addExtension (TOFINO_CONTEXT_JSON, contextJsonUrl) .build ();piPipeconfService.register (pipeconf); 測試完成 PipeconfLoader.java 就構成了一個可以運作的 pipeconf，為了測試我們使用 opennetworking/p4mn 這個 container 來實驗，p4mn 是一個 mininet 的 docker image，可以很簡單的啟動一個 mininet 的測試拓譜，並使用 bmv2 switch 取代 mininet 原本使用的 openflow switch。 1docker run -v /tmp/p4mn:/tmp --privileged --rm -it -p 50001:50001 opennetworking/p4mn:stable 使用這個指令可以啟動一個包含一個叫做 bmv2-s1 的 bmv2 switch 和兩個 host 同時會生成一個 onos 的 netcfg 檔案，路徑 /tmp/p4mn/bmv2-s1-netcfg.json 1234567891011121314151617181920212223242526272829{ &quot;devices&quot;: { &quot;device:bmv2:s1&quot;: { &quot;ports&quot;: { &quot;1&quot;: { &quot;name&quot;: &quot;s1-eth1&quot;, &quot;speed&quot;: 10000, &quot;enabled&quot;: true, &quot;number&quot;: 1, &quot;removed&quot;: false, &quot;type&quot;: &quot;copper&quot; }, &quot;2&quot;: { &quot;name&quot;: &quot;s1-eth2&quot;, &quot;speed&quot;: 10000, &quot;enabled&quot;: true, &quot;number&quot;: 2, &quot;removed&quot;: false, &quot;type&quot;: &quot;copper&quot; } }, &quot;basic&quot;: { &quot;managementAddress&quot;: &quot;grpc://localhost:50001?device_id=1&quot;, &quot;driver&quot;: &quot;bmv2&quot;, &quot;pipeconf&quot;: &quot;me.louisif.pipelines.simpleswitch&quot; } } }} 這個檔案包含了 ONOS P4 需要的所有資訊，主要分為兩個部分，ports 定義了這個交換機所有的 port 資訊。basic 部分，managementAddress 是 ONOS 用來使用 p4runtime 連線到 switch 的路徑，pipeconf 指定了這個 switch 使用的 pipeconf，預設會是 org.onosproject.pipelines.basic，在我們的範例中需要改為 me.louisif.pipelines.simpleswitch。 只要將 me.louisif.pipelines.simpleswitch 的 app 啟用，並上傳 bmv2-s1-netcfg.json，ONOS 就可以成功連線到這個 bmv2 switch，並正常提供北向的 APP 服務了。 如何下 flow rule完成 pipeconf 後，我們就可以透過下 flow rule 的方式來讓 switch 工作了，為了要讓 h1 和 h2 能夠互相溝通，最簡單的方法就是將所有 port 1 進來的封包送到 port 2、所有從 port 2 進來的封包送到 port 1。 為此我們需要兩條 table0 的 entry，分別是 standard_metadata.ingress_port == 1 (mask 0x1ff) → send port=2 standard_metadata.ingress_port == 2 (mask 0x1ff) → send port=1 由於 standard_metadata.ingress_port 這個 key 是 ternary，因此我們需要包含 mask, port 這個 type 的長度是 9 bits，因為要完全一致，所以 mask 是 0x1ff。 123456789101112final PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (PiMatchFieldId.of (&quot;standard_metadata.ingress_port&quot;), inPortNumber.toLong (), 0x1ff);final PiAction piAction = PiAction.builder ().withId (PiActionId.of (&quot;MyIngress.table0_control.send&quot;)) .withParameter (new PiActionParam(PiActionParamId.of (&quot;port&quot;), outPortNumber.toLong ())) .build ();final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (PiTableId.of (&quot;MyIngress.table0_control.table0&quot;)).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();flowRuleService.applyFlowRules (flowRule); 我們可以透過這樣的方式來下達第一條 table entry，可以發現和平常的 flow entry 不一樣的地方是 table id 使用的是 PiTableId 這個 type，並指定了 table0 的完整 id MyIngress.table0_control.table0，另外 Selector 和 Treatment 分別使用了 matchPi 和 piTableAction 這兩個特別的函數。 我們這邊將只使用 PiTableId, PiCriterion 和 PiAction 定義的 flow rule 稱之為 PI flow rule，PI flow rule 是 onos 的 p4runtime 可以直接處理的 flow rule，所有的欄位名稱都唯一對應到 p4 pipeline 的某個欄位，前面提到這些 PiTableId, PiMatchFieldId 等都會在 SimpleswitchConstants.java 內被定義，因此可以直接使用來縮短程式碼長度。 1234567891011121314import static me.louisif.pipelines.simpleswitch.SimpleswitchConstants.*;final PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (INGRESS_PORT, inPortNumber.toLong (), 0x1ff);final PiAction piAction = PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (PORT, outPortNumber.toLong ())) .build ();final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build ();flowRuleService.applyFlowRules (flowRule); 在範例程式碼裡面包含了簡單的 cli 指令實作，因此可以在 ONOS CLI 使用 add-pi-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt; 的方式來下達上面的 flow rule。詳情可以 參考檔案。 當然這樣編寫出來的 flow rule 會產生一個問題，如果相同的 pipeline，我們希望能從 bmv2 移植到 tonifo 上面去使用，由於欄位的名稱會存在差異，因此所有下達 flow rule 的 APP 都需要重新寫，顯然這樣不是一個很好的做法，增加了程式碼維護上的困難，因此 ONOS 加入前面提到的 Interpreter 還有 translator 機制，下一節會介紹如何為 pipeline 編寫 Interpreter 還有用比較通用的方法來下 flow rule，在完成 interpreter 後上述的 flow rule，可以用下面這個我們比較熟悉的方法來下達。 1234567891011private static final int TABLE0_TABLE_ID = 0;final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (TABLE0_TABLE_ID).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build ();flowRuleService.applyFlowRules (flowRule); 撰寫 Interpreter在前一節我們完成了基本的 pipeconf 註冊，並使用 PI flow rule 的方式來控制交換機，但是直接使用 PI flow rule 會降低 APP 的彈性，使移植到不同 pipeline 的困難度提高。另外 SDN 的一個特色是可以使用 packet-in/packet-out 的方式來讓 controller 即時性的處理封包，因此本結會介紹如何實作 Interpreter 來提供 packet-in/packet-out，以及 flow rule translation 的功能。 要提供一個 pipeline interpreter，我們需要實作 PiPipelineInterpreter 這個介面，要注意的是 Interpreter class 需要繼承 AbstractHandlerBehaviour，然後再 PipeconfLoader 去指定這個實作 1234567891011public class SimpleSwitchInterpreterImpl extends AbstractHandlerBehaviour implements PiPipelineInterpreter {}PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addBehaviour (PiPipelineInterpreter.class, SimpleSwitchInterpreterImpl.class) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build (); 當一條 flow rule 被加入到 ONOS 時，PI framework 會將其翻譯成 PI flow rule，也就是將非 PI * 的欄位轉換成 PI flow rule 的欄位。 123456final FlowRule flowRule = DefaultFlowRule.builder () .forTable (0) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build (); Table Id translation以前面提到的 port 1 送到 port 2 的 flow rule 來示範，我們需要把 table id 0，轉換成 PiTableId.of (&quot;MyIngress.table0_control.table0&quot;) ，這對應到 Interpreter 的 mapFlowRuleTableId 函數，我們可以定義一個 map 來記錄，table index 跟 Pi table id 之間的關係，然後實作 mapFlowRuleTableId。這邊的 id 0 並不具有特定的意義，只是單純我們 interpreter 定義的 table index，因此當 APP 使用時，需要知道這個 index 對應到的 table 具體是什麼功能，當然通常我們會再加上一層 pipeliner，透過 flow objective 來隱藏 table id 的細節。 12345678private static final Map&lt;Integer, PiTableId&gt; TABLE_MAP = new ImmutableMap.Builder&lt;Integer, PiTableId&gt;() .put (0, MY_INGRESS_TABLE0_CONTROL_TABLE0) .build ();@Overridepublic Optional&lt;PiTableId&gt; mapFlowRuleTableId(int flowRuleTableId) { return Optional.ofNullable (TABLE_MAP.get (flowRuleTableId));} Selector Translation我們通常使用 DefaultTrafficSelector.builder 來定義 Selector。 matchInPort 會在 selector 內加入一個 PortCriterion ，他的 criterion tpye 是 Criterion.Type.IN_PORT ，為此 Interpreter 需要根據 Criterion type，將其轉換成 p4 table 對應的 key，同樣我們使用 map 的方式來維護其關係。 123456789101112private static final Map&lt;Criterion.Type, PiMatchFieldId&gt; CRITERION_MAP = new ImmutableMap.Builder&lt;Criterion.Type, PiMatchFieldId&gt;() .put (Criterion.Type.IN_PORT, HDR_STANDARD_METADATA_INGRESS_PORT) .put (Criterion.Type.ETH_SRC, HDR_HDR_ETHERNET_SRC_ADDR) .put (Criterion.Type.ETH_DST, HDR_HDR_ETHERNET_DST_ADDR) .put (Criterion.Type.ETH_TYPE, HDR_HDR_ETHERNET_ETHER_TYPE) .build ();@Overridepublic Optional&lt;PiMatchFieldId&gt; mapCriterionType(Criterion.Type type) { return Optional.ofNullable (CRITERION_MAP.get (type));} 可能有些人會有些疑惑說，p4 table key 可能會是 exact 或是 ternary，那 ONOS 要怎麼把 matchInPort 轉換成 ternary mask 0x1ff 前面提到在註冊 pipeconf 時，我們透過 .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) 載入 p4info (在 ONOS 內稱之為 PiPipelineModel)，每個 key 具體的類型和資料長度會包含在 p4info 內，PI framework 在轉換時會根據不同 criterion type 和 key type 的不同和需求自動做轉換。 Treatment Translation不像 table id 和 criterion，在 p4 內，每個 action 是一個可帶參數的 function，和 ONOS 定義的 treatment 不存在簡單的對應關係，因此 interpreter 定義了 mapTreatment，輸入是 treatment 和 table id，輸出是 PiAction，讓 interpreter 完整的處理整個 treatment。 123456789101112131415161718192021222324252627282930@Overridepublic PiAction mapTreatment(TrafficTreatment treatment, PiTableId piTableId) throws PiInterpreterException { // 檢查 table id 是否有效，由於只有 table0 一張 table，因此這邊直接檢查 table id 是不是 table 0 if (!piTableId.equals (MY_INGRESS_TABLE0_CONTROL_TABLE0)) { throw new PiInterpreterException(&quot;Unsupported table id: &quot; + piTableId); } // 我們的 pipeline 只支援 set output port 一個 instruction if (treatment.allInstructions ().size () != 1 || !treatment.allInstructions ().get (0).type ().equals (Instruction.Type.OUTPUT)) { throw new PiInterpreterException(&quot;Only output instruction is supported&quot;); } PortNumber port = ((Instructions.OutputInstruction) treatment.allInstructions ().get (0)).port (); // 像 controller、flooding 這些特別的 port，稱之為 Logical port if (port.isLogical ()) { if (port.exactlyEquals (PortNumber.CONTROLLER)) { // 我們支援使用 send_to_controller 這個 action 將封包送到 ONOS return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND_TO_CPU) .build (); } else { throw new PiInterpreterException(&quot;Unsupported logical port: &quot; + port); } } 一般的使用用 send 這個 action 來傳送封包 < br> return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (new PiActionParam(PORT, port.toLong ())) .build ();} 到此我們已經完成了 flow rule 的轉換，可以使用 ONOS 標準的 flow rule 來操作 pipeline 了。 如果希望對轉換機制有更詳細的瞭解可以查看 ONOS 原始碼 Packet-in/Packet-outInterpreter 另外一個重要的功能是使 pipeline 支援 packet-in/packet-out 的功能。為此我們需要先修改我們的 p4 pipeline。 首先我們會先加入兩個特別的 header，並使用 controller_header anotation 標記，@controller_header (&quot;packet_in&quot;) 來得知這個 packet_in_header_t 對應的是 packet-in 時，附加在這個封包的 meta data，通常會定義 ingress port 來表示封包的 input port。同樣的 packet_out_header_t 是 packet-out 時，ONOS 送來 pipeline 處理的 meta data 1234567891011@controller_header (&quot;packet_in&quot;)header packet_in_header_t { bit&lt;7&gt; _padding; bit&lt;9&gt; ingress_port;}@controller_header (&quot;packet_out&quot;)header packet_out_header_t { bit&lt;7&gt; _padding; bit&lt;9&gt; egress_port;} 接著我們要修改 header、parser，從 ONOS packet out 出來的封包對 p4 交換機來說相當於從特定一個 port 送進來的封包，因此一樣會經過整個 pipeline 以 bmv2 來說，controller 的 port number 可以自由指定，在我們使用的 p4mn container 內這個 port 被定義成 255，為此我們在 pipeline 的 p4 檔案內定義了 CPU_PORT 巨集為 255 我們將 packet-in/packet-out header 加入到 headers 內，當 packet-out 時，packet_out 會在封包的開頭，因此在 parser 的 start state，我們先根據 ingress port 是不是 CPU_PORT 來決定是不是要 parse packet_out header。 12345678910111213141516171819202122232425262728struct headers_t { packet_in_header_t packet_in; packet_out_header_t packet_out; ethernet_h ethernet;}parser MyParser(packet_in packet, out headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { state start { transition select(standard_metadata.ingress_port) { CPU_PORT: parse_packet_out; default: parse_ethernet; } } state parse_packet_out { packet.extract (hdr.packet_out); transition parse_ethernet; } state parse_ethernet { packet.extract (hdr.ethernet); transition accept; }} 在 Ingress 部分，如果是 packet-out packet，我們沒有必要讓他經過整個 ingress pipeline，為此我們直接將 egress_spec 設置為 packet_out header 內的 egress_port，然後直接呼叫 exit。 123456789101112control MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.ingress_port == CPU_PORT) { standard_metadata.egress_spec = hdr.packet_out.egress_port; hdr.packet_out.setInvalid (); exit; } table0_control.apply (hdr, meta, standard_metadata); }} 我們的 table0 則加入了 send_to_cpu 這個 action，做的事情就是把 egress_spec 設定成 CPU port。 1234567control table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; }} 當封包要 packet-in 到 ONOS 時，需要將 packet-in header 設為 valid，並填入對應的資料，這個部分會再 Egress pipeline 完成 1234567891011control MyEgress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.egress_port == CPU_PORT) { hdr.packet_in.setValid (); hdr.packet_in.ingress_port = standard_metadata.ingress_port; hdr.packet_in._padding = 0; } }} 最後為了讓 packet-in header 能後被傳到 ONOS，需要再 deparser 加入該 header，注意 packet-out header 是為了讓 pipeline 能夠根據該 header 來處理 pecket-out header 用的，因此他不應該被加入到 deparser。 123456control MyDeparser(packet_out packet, in headers_t hdr) { apply { packet.emit (hdr.packet_in); packet.emit (hdr.ethernet); }} 接著回到我們的 interpreter，在 interpreter 內定義了兩個函數 mapInboundPacket 和 mapOutboundPacket，分別對應 packet-in 和 packet-out 封包的處理。先前我們在 p4 pipeline 定義了 packet_in 和 packet_out 的 header，這兩個函數最基本的功能是讀取和寫入這兩個 header 的資訊。由於這兩個函數的功能相對固定，因此可以直接從 ONOS 的 basic pipeline interpreter 複製過來修改。 12345678910111213141516171819202122232425262728@Override public Collection&lt;PiPacketOperation&gt; mapOutboundPacket(OutboundPacket packet) throws PiInterpreterException { TrafficTreatment treatment = packet.treatment (); // 由於 outbound packet 的內容通常不用在 switch 上在做修改，因此我們只需要取得 //set output port 的 instruction // 當然如果有特別的功能需求，可以透過修改 pipeline 來支援更多 instruction List&lt;Instructions.OutputInstruction&gt; outInstructions = ... ImmutableList.Builder&lt;PiPacketOperation&gt; builder = ImmutableList.builder (); for (Instructions.OutputInstruction outInst : outInstructions) { ... // 這邊透過呼叫 createPiPacketOperation 來填入 packet_out header 的資訊 builder.add (createPiPacketOperation (packet.data (), outInst.port ().toLong ())); ... } return builder.build (); }private PiPacketOperation createPiPacketOperation(ByteBuffer data, long portNumber) throws PiInterpreterException { PiPacketMetadata metadata = createPacketMetadata (portNumber); return PiPacketOperation.builder () .withType (PACKET_OUT) .withData (copyFrom (data)) .withMetadatas (ImmutableList.of (metadata)) .build (); } 123456789101112131415161718@Overridepublic InboundPacket mapInboundPacket(PiPacketOperation packetIn, DeviceId deviceId) throws PiInterpreterException { Ethernet ethPkt; ... ethPkt = Ethernet.deserializer ().deserialize (packetIn.data ().asArray (), 0, ... //packet_in header 的資訊會以 key-value 的方式存在 packetIn.metadatas Optional&lt;PiPacketMetadata&gt; packetMetadata = packetIn.metadatas () .stream ().filter (m -&gt; m.id ().equals (INGRESS_PORT)) .findFirst () // 從中提取出 input port number ImmutableByteSequence portByteSequence = packetMetadata.get ().value (); short s = portByteSequence.asReadOnlyBuffer ().getShort (); ConnectPoint receivedFrom = new ConnectPoint(deviceId, PortNumber.portNumber (s)); ByteBuffer rawData = ByteBuffer.wrap (packetIn.data ().asArray ()); return new DefaultInboundPacket(receivedFrom, ethPkt, rawData); ...} 到此我們已經完成了 interpreter 的實現了，Interpreter 目前還包含 mapLogicalPortNumber 和 getOriginalDefaultAction ，不過基本的 interpreter 不需要實現這兩個功能，所以這邊就不再展開介紹。 同樣使用範例程式碼時，可以在 ONOS CLI 使用 add-common-flow-rule &lt;device id&gt; &lt;input port&gt; &lt;output port&gt; 的方式來使用下達 ONOS 標準的 flow rule。詳情可以 參考檔案。 撰寫 Pipeliner到目前為止我們已經完成了 pipeconf 的基本功能，可以下 flow rule 還有使用 packet-in/packet-out 的功能，不過到目前我們還是需要直接使用 flow rule，要知道 table id 對應的功能，為了能夠隱藏 table 的細節還有銜接 ONOS 內建的網路功能 APP，我們需要實作 pipeliner 讓我們的交換機支援 flow objective 的功能。 和 Interpreter 類似，我們需要實作 Pipeliner 這個介面並繼承 AbstractHandlerBehaviour 然後在 PipeconfLoader 透過 addBehaviour (Pipeliner.class, SimpleSwitchPipeliner.class) 的方式加入。 123456789101112public class SimpleSwitchPipeliner extends AbstractHandlerBehaviour implements Pipeliner { private final Logger log = getLogger (getClass ()); private FlowRuleService flowRuleService; private DeviceId deviceId; @Override public void init(DeviceId deviceId, PipelinerContext context) { this.deviceId = deviceId; this.flowRuleService = context.directory ().get (FlowRuleService.class); }} 首先我們實作 init 方法，pipeliner 和交換機之間是 1 對 1 的關係，因此當交換機被初始化的時候，可以透過 init 方法取得 device 的 id。context 最主要的部份是使用 directory ().get 方法。平常我們在 APP 開發時是使用 Reference annotation 來取得 onos 的 service，這邊我們可以直接透過 get 方法來取得 service，由於 pipeliner 需要完成 flow ojbective 到 flow rule 的轉換，並直接送到 flow rule service，因此這邊先取得 flow rule service。 在 ONOS Flow Objective Service 的架構內，其實總共有三種 objective，分別是 forward、filter 和 next，他們都需要透過 pipeliner 來和 ONOS 核心互動，由於我們只想要實作 forwarding objective 的部分，因此 filter 和 next 可以單純回應不支援的錯誤訊息 12345678910111213@Overridepublic void filter(FilteringObjective obj) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));}@Overridepublic void next(NextObjective obj) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED));}@Overridepublic List&lt;String&gt; getNextMappings(NextGroup nextGroup) { // We do not use nextObjectives or groups. return Collections.emptyList ();} 接著就到了我們的主角 forward objective，在實作邏輯上其實與 interpreter 對 treatment 的處理方式類似，forward 方法會取得一個 ForwardingObjective 物件，我們根據 treatment 和 selector 生成出一條或多條 flow rule，然後透過 flow rule service 下放到交換機上。 12345678910111213141516171819202122232425262728293031323334@Overridepublic void forward(ForwardingObjective obj) { if (obj.treatment () == null) { obj.context ().ifPresent (c -&gt; c.onError (obj, ObjectiveError.UNSUPPORTED)); } // Simply create an equivalent FlowRule for table 0. final FlowRule.Builder ruleBuilder = DefaultFlowRule.builder () .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0) .forDevice (deviceId) .withSelector (obj.selector ()) .fromApp (obj.appId ()) .withPriority (obj.priority ()) .withTreatment (obj.treatment ()); if (obj.permanent ()) { ruleBuilder.makePermanent (); } else { ruleBuilder.makeTemporary (obj.timeout ()); } switch (obj.op ()) { case ADD: flowRuleService.applyFlowRules (ruleBuilder.build ()); break; case REMOVE: flowRuleService.removeFlowRules (ruleBuilder.build ()); break; default: log.warn (&quot;Unknown operation {}&quot;, obj.op ()); } obj.context ().ifPresent (c -&gt; c.onSuccess (obj));} 最後我們還需要實作 purgeAll，當刪除所有 flow obejctive 的時候，刪除所有的 flow rule，這邊我們只需要簡單呼叫 flow rule service 的 purgeFlowRules 就好。 1234@Overridepublic void purgeAll(ApplicationId appId) { flowRuleService.purgeFlowRules (deviceId, appId);} 到此我們已經完成了整個 pipeconf 的實作，可以透過 flow objective 的方式來管理交換機並與 ONOS 內建的 APP 整合，因此我們可以透過使用 proxyarp 和 fwd 兩個 APP 來讓我們的交換機能夠正常的工作。 小結以上就是 ONOS P4 Pipeconf 的基本開發教學，使用的範例程式碼在 github，如果有遇到任何問題或有說明不清楚的地方，歡迎留言提問，我會盡力為大家解答。 參考資料https://hackmd.io/@cnsrl/onos_p4 https://wiki.onosproject.org/pages/viewpage.action?pageId=16122675 https://github.com/p4lang/tutorials/blob/master/exercises/basic/solution/basic.p4 ONOS Basic Pipeline","link":"/ONOS/ONOS-P4-Switch-Pipeconf-Development/"},{"title":"CNI Spec 導讀","text":"前言這次來嘗試寫寫看 spec 導讀，今天要講的是 Container Network Interface (CNI) Specification。CNI 是 CNCF 的一個專案，這個專案包含了今天要講的 CNI SPEC 以及基於這個 SPEC 開發出來 libraries 還有一系列的 CNI plugins。 CNI 定義了一套 plugin-based 的網路解決方案，包含了設定還有呼叫執行的標準，使用 CNI 最知名的專案應該就是 kubernetes 了，在 k8s 環境中，容器的網路並不是直接由 container runtime (ex. Docker) 處理的，container runtime 建立好容器後，kubelet 就會依照 CNI 的設定和通訊標準去呼叫 CNI plugin，由 CNI plugin 來完成容器的網路設置。 Container runtime 在執行容器建立時，會依據設定檔 (後續稱為 network configuration) 的指示，依序呼叫一個或多個的 CNI plugin 來完成網路功能的設置，一個網路功能的設置可能會需要多個 CNI plugins 之間的相互合作，或著由多個 CNI plugin 提供多個不同的網路功能。 Overview目前 CNI spec 的版本是 1.0.0，CNI 的 repository 裡面除了 spec 文件外也包含了基於 CNI spec 開發的 go library，用於 CNI plugin 的開發。不過 CNI spec 和 library 的版本號是獨立的，當前的 library 版本是 1.1.2。 首先要對幾個基本名詞定義 container 是一個網路獨立的環境，在 linux 上通常透過 network namespace 的機制來切割，不過也可能是一個獨立的 VM。 network 是 endpoints 的集合，每個 endpoint 都有著一個唯一是別的的地址 (通常是 ip address) 可用於互相通訊。一個端點可能是一個容器、VM 或著路由器之類的網路設備。 runtime 指的是呼叫執行 CNI plugin 的程式，以 k8s 來說，kubelet 作為這個角色 plugin 指的是一個用來完成套用特定網路設定的程式。 CNI Spec 包含五個部分 Network configuration format: CNI 網路設定的格式 Execution Protocol: runtime 和 CNI plugin 之間溝通的 protocol Execution of Network Configurations: 描述 runtime 如何解析 network configuration 和操作 CNI plugin。 Plugin Delegation: 描述 CNI plugin 如何呼叫 CNI plugin。 Result Types: 描述 CNI plugin 回傳的結果格式。 Section 1: Network configuration formatCNI 定義了一個網路設定的格式，這個格式用於 runtime 讀取的設定檔，也用於 runtime 解析後，plugin 接收的格式，通常來說設定檔是以 ** 靜態 ** 檔案的方式存在主機上，不會任意變更。 CNI 使用了 JSON 作為設定檔的格式，並包含以下幾個主要欄位 cniVersion (string): 對應的 CNI spec 版本，當前是 1.0.0 name (string): 一個在主機上不重複的網路名稱 disableCheck (boolean): 如果 disableCheck 是 true 的話，runtime 就不能呼叫 CHECK，CHECK 是 spec 定義的一個指令用來檢查網路是否符合 plugin 依據設定的結果，由於一個網路設定可能會呼叫多個 CNI plugins，因此可能會出現網路狀態符合管理員預期，但是 CNI plugin 之間衝突檢查失敗的情況，這時就可以設定 disableCheck plugin (list): CNI plugin 設定 (Plugin configuration object) 的列表。 Plugin configuration objectplugin configuration object 包含了一些明定的欄位，但 CNI plugin 可能根據需要增加欄位，會由 runtime 在不修改的情況下送給 CNI plugin。 必填: type (string): CNI plugin 的執行檔名稱 可選欄位 (CNI protocol 使用): capabilities (dictionary): 定義 CNI plugin 支援的 capabilities，後面在 section 3 會介紹。 保留欄位：這些欄位是在執行過程中，由 runtime 生成出來的，因此不應該在設定檔內被定義。 runtimeConfig args 任何 cni.dev/ 開頭的 key 可選欄位：不是 protocol 定義的欄位，但是由於很多 CNI plugin 都有使用，因此具有特定的意義。 ipMasq (boolean): 如果 plugin 支援的話，會在 host 上替該網路設置 IP masquerade，如果 host 要做為該網路的 gateway 的話，可能需要該功能。 ipam (dictionary): IPAM (IP Address Management) 設置，後面在 section 4 會介紹。 dns (dictionary): DNS 設置相關設置 nameservers (list of strings): DNS server 的 IP 列表 domain (string): DNS search domain search (list of strings),: DNS search domain 列表 options (list of strings): DNS options 列表 其他欄位: CNI plugin 自己定義的額外欄位。 設定檔範例 1234567891011121314151617181920212223242526272829303132333435363738{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;name&quot;: &quot;dbnet&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;bridge&quot;, //plugin specific parameters &quot;bridge&quot;: &quot;cni0&quot;, &quot;keyA&quot;: [&quot;some more&quot;, &quot;plugin specific&quot;, &quot;configuration&quot;], &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, //ipam specific &quot;subnet&quot;: &quot;10.1.0.0/16&quot;, &quot;gateway&quot;: &quot;10.1.0.1&quot;, &quot;routes&quot;: [ {&quot;dst&quot;: &quot;0.0.0.0/0&quot;} ] }, &quot;dns&quot;: { &quot;nameservers&quot;: [ &quot;10.1.0.1&quot; ] } }, { &quot;type&quot;: &quot;tuning&quot;, &quot;capabilities&quot;: { &quot;mac&quot;: true }, &quot;sysctl&quot;: { &quot;net.core.somaxconn&quot;: &quot;500&quot; } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: {&quot;portMappings&quot;: true} } ]} Section 2: Execution ProtocolCNI 的工作模式是由 container runtime 去呼叫 CNI plugin 的 binaries，CNI Protocol 定義了 runtime 和 plugin 之間的溝通標準。 CNI plugin 的工作是完成容器網路介面的某種設置，大致上可以分成兩類 Interface plugin: 建立容器內的網路介面，並確保其連接可用 Chained plugin: 調整修改一個已建立的介面 (可能同時會需要建立更多額外的介面) Runtime 透過兩種方式傳遞參數，一是透過環境變數，二是透過 stdin 傳遞 Section 1 定義的 configuration。如果成功的話，結果會透過 stdout 傳遞，如果失敗的話就會錯誤資訊會透過 stderr 傳遞。configuration 和結果、錯誤都是使用 JSON 格式。 Runtime 必須在 Runtime 的網路執行，大多數情況下就是在主機的預設 network namespace/dom0 ParametersProtocol 的參數都是透過環境變數來傳遞的，可能的參數如下 CNI_COMMAND: 當前執行的 CNI 操作 (可能是 ADD, DEL, CHECK. VERSION) CNI_CONTAINERID: 容器的ＩＤ CNI_NETNS: 容器網路空間的參考，如果是使用 namespaces 的方式來切割的話，就是 namespce 的路徑（e.g. /run/netns/[nsname] ) CNI_IFNAME: 要建立在容器內的介面名稱，如果 plugin 無法建立該名稱則回傳錯誤 CNI_ARGS: 其他參數，Alphanumeric 格式的 key-value pairs，使用分號隔開”e.g. FOO=BAR;ABC=123 CNI_PATH: CNI plugin 的搜尋路徑，因為 CNI 存在 CNI plugin 呼叫 CNI plugin 的情況，所以需要這個路徑。如果包含多個路徑，使用 OS 定義的分隔符號分割。Linux 使用 : ，Windows 使用 ; Errors如果執行成功，CNI Plugin 應該回傳 0。如果失敗則回傳非 0，並從 stderr 回傳 error result type structure。 CNI operationsADDCNI plugin 會在 CNI_NETNS 內建立 CNI_IFNAME 介面或對該介面套用特定的設置 如果 CNI plugin 執行成功，應該從 stdout 回傳一個 result structure。如果 plugin 的 stdin 輸入包含 prevResult，他必些直接把 prevResult 包在 result structure 內或對他修改後在包在 result structure 內，runtime 會把前一個 CNI 輸出的 prevResult 包在下一個 CNI 的 stdin 輸入內。 如果 CNI plugin 嘗試建立介面時，該介面已經存在，應該發出錯誤。 Runtime 不應該在沒有 DEL 的情況下對一個 CNI_CONTAINERID 容器的一個 CNI_IFNAME 介面多次呼叫 ADD。不過可能對同一個 container 的不同介面呼叫 ADD。 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 、 CNI_NETNS 和 CNI_IFNAME 。 CNI_ARGS 和 CNI_PATH 為可選項。 DEL移除 CNI_NETNS 內的 CNI_IFNAME 介面，或還原 ADD 套用的網路設定 通常來說，如果要釋放的資源已經不存在，DEL 也應該視為成功。例如容器網路已經不存在了，一個 IPAM plugin 應該還是要正常的釋放 IP 和回傳成功，除非 IPAM plugin 對容器網路存在與否有嚴格的要求。即便是 DHCP plugin，雖然執行 DEL 操作的時侯，需要透過容器網路發送 DHCP release 訊息，但是由於 DHCP leases 有 lifetime 的機制，超時後會自動回收，因此即便容器網路不存在，DHCP plugin 在執行 DEL 操作時，也應該該回傳成功。 如果重複對一個 CNI_CONTAINERID 容器的 CNI_IFNAME 介面執行多次 DEL 操作，plguin 都應該回傳，即便介面已經不存在或 ADD 套用的修改已經還原。 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 和 CNI_IFNAME。 CNI_NETNS、CNI_ARGS 和 CNI_PATH 為可選項。 CHECKRuntime 透過 CHECK 檢查容器的狀態，並確保容器網路符合預期。CNI spec 可以分為 plugin 和 runtime 的兩部分 Plugin: plugin 必須根據 prevResult 來判斷介面和地址是否符合預期 plugin 必須接受其他 chained plugin 對介面修改的結果 如果 plugin 建立並列舉在 prevResult 的 CNI Result type 資源 (介面、地址、路由規則) 不存在或不符合預期狀態，plugin 應該回傳錯誤 如果其他不在 Result type 內的資源不存在或不符合預期也應該回垂錯誤。可能的資源如下 防火牆規則 流量控管 (Traffic shaping controls) IP 保留 (Reservation) 外部依賴，如連接所需的 daemon 如果發現容器網路是無法訪問的應該回傳錯誤 plugin 必須在完成 ADD 後能夠立即處理 CHECK 指令，應此 plguin 應該要容忍非同步完成的網路設置在一定的時間內不符合預期。 plugin 執行 CHECK 時，應該呼叫所有 delegated plugin 的 CHECK，並把 delegated plugin 的錯誤傳給 plugin 的呼叫者 Runtime: Runtime 不應該在未執行 ADD 前或已經 DEL 後沒在執行一次 ADD 的容器執行 CHECK 如果 configuration 的 disableCheck 為真，runtime 不應呼叫 disableCheck Runtime 呼叫 CHECK 時，configuration 必須在 prevResult 包含前一次 ADD 操作時最後一個 plugin 的 Result。Runtime 可能會使用 libcni 提供的 Result caching 功能。 如果其中一個 plugin 回傳錯誤，runtime 可能不會呼叫後面 plugin 的 CHECK Runtime 可能會在 ADD 完後的下一刻一直到 DEL 執行前的任何時間執行 CHECK Runtime 可能會假設一個 CHECK 失敗的容器會永久處於配置錯誤的狀態 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID、CNI_NETNS 和 CNI_IFNAME。 CNI_ARGS 和 CNI_PATH 為可選項。 除了 CNI_PATH 以外的參數必須和 ADD 時一致。 VERSIONPlugin 透過 stdout 輸出 JSON 格式的 version result type object，用於查看 CNI plugin 的版本。 Stdin 輸入的 JSON 物件只包含 cniVersion。環境變數參數只需要 CNI_COMMAND。 Section 3: Execution of Network Configurations這個章節描述了 runtime 如何解析 network configuration，並執行 CNI plugins。Runtime 可能會想要新增、刪除、檢查 configuration，並對應到 CNI plugin 的 ADD, DELETE, CHECK 操作。這個章節也定義 configuration 是如何改變並提供給 plugin 的。 對容器的 network configuration 操作稱之為 attachment，一個 attachment 通常對應到一個特定 CNI_CONTAINERID 容器的 CNI_IFNAME 介面。 生命週期 Runtime 必須在呼叫任何 CNI Plugin 之前，為容器建立新的 network namespace Runtime 一定不能同時在一個容器執行多個 plugin 命令，但是同時處理多個容器是可以的。因此 plugin 必須能夠處理多容器 concurrency 的問題，並在共享的資源 (e.g. IPAM DB) 實作 lock 機制 Runtime 必須確保 ADD 操作後必定執行一次 DEL 操作，即便 ADD 失敗。唯一可能的例外是如結點直接丟失之類的災難性事件。 DEL 操作可能連續多次執行 network configuration 在 ADD 和 DEL 操作之間，以及不同的為 attachment 之間應該保持一致不變 runtime 必須負責清除容器的 network namespace Attachment ParametersNetwork configuration 在不同的 attachments 間應該保持一致。不過 runtime 會傳遞其他每個 attachment 獨立的參數。 Container ID: 對應到 Section 2 CNI_CONTAINERID 環境變數 Namespace: 對應到 CNI_NETNS 環境變數 Container interface name: 對應到 CNI_IFNAME 環境變數 Generic Arguments: 對應到 CNI_ARGS 環境變數 Capability Arguments: CNI plugins search path: 對應到 CNI_PATH 環境變數 Adding an attachment對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 生成送給 plugin sdtin 的 configuration 只有第一個執行的 plugin 不會帶 prevResult 欄位，後續執行的 plugin 都會把前一個的 plugin 的結果放在 prevResult 執行 plugin 的執行檔。設置 CNI_COMMAND=ADD，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Runtime 必須持久保存最後一個 plugin 的結果，用於 check 和 delete 操作。 Deleting an attachment刪除 attachment 和添加基本上差不多的，差別是 plugin 的執行順序是反過來的，從最後一個開始 prevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 對 configuration 的 plugins 欄位反序的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=DEL，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Checking an attachment如同 Section 2 所述，Runtime 會透過 CNI plugin 檢查每個 attachment 是否正常運作中。需注意的是 Runtime 必須使用和 add 操作時一致的 attachment parameters 檢查和添加只有兩個差別 prevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 如果 network configuation 中 disableCheck 為真，則直接回傳成功 對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟 根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=CHECK，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Deriving execution configuration from plugin configuration在 add, delete, check 操作時，runtime 必須根據 network configuration 生成出 plugin 可以存取的 execution configuration (基本對應到 plugin configuration)，並填入內容。 如同 section 2 所述，execution configuration 使用 JSON 格式並透過 stdin 傳給 CNI plugin。 必要的欄位如下: cniVersion: 同 network configuraion 中 cniVersion 的值 name: 同 network configuraion 中 name 的值 runtimeConfig: runtime 需要和 plugin 可提供的 capabilities 的聯集 (capability 會在後面討論) prevResult: CNI plugin 回傳的 Result type 結果 capabilities 欄位必須被移除 其他 plugin configuration 的欄位應該被放入 execution configuration Deriving runtimeConfig相對於靜態的 network configuration 來說，runtime 可能會需要根據每個不同的 attachment 產生動態參數。雖然 runtime 可以透過 CNI_ARGS 傳遞動態參數給 CNI plugin，但是我們沒辦法預期說 CNI plugin 會不會接收這個參數。透過 capabilities 欄位，可以明定 plugin 支援的功能，runtime 根據 capabilities 及需求，動態生成設定並填入 runtimeConfig。CNI spec 沒有定義 capability，但是比較通用的 capability 有列舉在另外一份 文件。 以 kubernetes 常用的 Node port 功能來說，需要 CNI plugin 支援 portMappings 這個 capability。在 section 1 的定義中，plugin configuration 包含了 capabilities 欄位，在這個欄位填入 portMappings，讓 runtime 知道可以透過該 plugin 處理 port mapping。 123456{ &quot;type&quot;: &quot;myPlugin&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true }} Runtime 執行 CNI plugin 時，會根據 capabilities 生成 runtimeConfig，並填入對應的動態參數。 12345678910111213{ &quot;type&quot;: &quot;myPlugin&quot;, &quot;runtimeConfig&quot;: { &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 8080, &quot;containerPort&quot;: 80, &quot;protocol&quot;: &quot;tcp&quot; } ] } ...} Section 4: Plugin Delegation雖然 CNI 的主要的架構是一系列的 CNI plkugin 依次執行，但這樣的方式有些時候沒辦法滿足 CNI 的需求。CNI plugin 可能會需要將某些功能委託給另外一個 CNI plugin，並存在 plugin 呼叫 plugin 的情況，最常見的案例就是 IP 地址的分配管理。 通常來說，CNI plugin 應該要指定並維護容器的 IP 地址以及下達必要的 routing rules，雖然由 CNI plugin 自主完成可以讓 CNI plugin 有更大的彈性但是也加重了 CNI plugin 的職責和開發難度，讓不同的 CNI plugin 需要重複開發相同的 IP 地址管理邏輯，因此許多 CNI 將 IP 管理的邏輯委託給另一個獨立的 plugin，讓相關邏輯可以直接被複用。對此除了前面提到的 interface plugin 和 chanined plugin，我們定義了第三類的 plugin - IP Address Management Pligin (IPAM plugin)。 由主要的 CNI plugin 去呼叫 IPAM plugin，IPAM plugin 判斷網路介面的 IP 地址、gateway、routing rules，並回傳資訊給主要的 CNI plugin 去完成相對應設置。(IPAM plugin 可能會透過 dhcp 之類的 protocl, 儲存在本地的檔案系統資訊或 network configuration 的 ipam section 取得資訊) Delegated Plugin protocol和 Runtime 執行 CNI plugin 的方式一樣，delegated plugin 也是透過執行 CNI plugin 可執行程式的方式。主要的 plugin 在 CNI_PATH 路徑下搜尋 CNI plugin。delegated plugin 必須接收和主要 plugin 完全一致的環境變數參數，以及主要 plugin 透過 stdin 接收到的完整 execute configuration。如果執行成功則回傳 0，並透過 stdout 返回 Success result type output。 Delegated plugin execution procedure 當 CNI plugin 執行 delegated plugin 時: 在 CNI_PATH 路徑下搜尋 plugin 的可執行程式 使用 CNI plugin 的環境變數參數和 execute configuration，作為 delegated plugin 的輸入 確保 delegated plugin 的 stderr 會輸出到 CNI plugin 的 stderr 當 plugin 執行 delete 和 check 時，必須執行所有的 delegated plugin，並將 delegated plugin 的錯誤回傳給 runtime 當 ADD 失敗時，plugin 應該在回傳錯誤前，先執行 delegated plugin 的 DEL Section 5: Result Types Plugin 的回傳結果使用 JSON 格式，並有三種 Success Error Version Success 如果 plugin 的輸入包含 prevResult，輸出必須包含該欄位的值，並加上該 plugin 對網路修改的資訊，如果該 plugin 沒有任何操作，則該欄位必須保持原輸入內容。 Success Type Result 的欄位如下 cniVersion: 同輸入的 cniVersion 版本 interfaces: 一個該 plugin 建立的 interface 資訊的陣列，包含 host 的 interface。 name: interface 的名子 mac: interface 的 mac address sandbox: 該介面的網路環境參考，例如 network namespace 的路徑，如果是 host 介面則該欄位為空，容器內的介面該值應為 CNI_NETNS ips: 該 plugin 指定的 ip address: CIDR 格式的 ip address (e.g. 192.168.1.1/24) gateway: default gateway (如果存在的話) interface: 介面的 index，對應到前述 interfaces 陣列 routes: plugin 建立的 routing rules dst: route 的目的 (CIDR) gw: nexthop 地址 dns nameservers: DNS server 位置陣列 (ipv4 或 ipv6 格式) domain: DNS 搜尋的 local domain search (list of strings): 有優先度的 search domain options (list of strings): 其他給 dns resolver 的參數 Delgated plugin 可能會忽略不需要的欄位 IPAM 必須回傳一個 abbreviated Success Type result (忽略 interfaces 和 ips 裡面的 interface 欄位) Errorplugin 回傳的錯誤資訊，欄位有 cniVersion, code, msg, details。 123456{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;code&quot;: 7, &quot;msg&quot;: &quot;Invalid Configuration&quot;, &quot;details&quot;: &quot;Network 192.168.0.0/31 too small to allocate from.&quot;} Error code 0-99 被保留給通用的錯誤，100 以上是 plugin 自定義的錯誤。 Error code 描述 1 不支援的 CNI 版本 2 不支援的 network configuration 欄位，error message 會包含不支援的欄位名稱和數值 3 未知或不存在的容器，這個錯誤同時代表 runtime 不需要執行 DEL 之類的清理操作 4 無效的環境環境變數參數，error message 會包含無效的欄位名稱 5 IO 錯誤，例如無法讀取 stdin 的 execute configuration 6 解析錯誤，例如無效的 execute configuration JSON 格式 7 無效的 network configuration 11 稍後在嘗試，存在暫時無法操作的資源，runtime 應該稍後重試 此外，stderr 也可被用於非 JSON 結構的錯誤訊息，如 log Version Version Type Result 的欄位如下 cniVersion: 同輸入的 cniVersion 版本 supportedVersions: 一個支援的 CNI 版本陣列 1234{ &quot;cniVersion&quot;: &quot;1.0.0&quot;, &quot;supportedVersions&quot;: [ &quot;0.1.0&quot;, &quot;0.2.0&quot;, &quot;0.3.0&quot;, &quot;0.3.1&quot;, &quot;0.4.0&quot;, &quot;1.0.0&quot; ]} Appendix:在 CNI SPEC 裏面包含了一些 configuration 和執行過程中 plugin 輸入輸出的範例，可以參考 原始文件另外還有一份 CNI 的文件 CONVENTIONS，描述許多 spec 裡面沒有定義但是許多 plugin 常用的欄位。 小節以上是對 CNI spec 1.0.0 的導讀，希望對大家有幫助。","link":"/Kubernetes/CNI-Spec-Guiding/"}],"tags":[{"name":"P4","slug":"P4","link":"/tags/P4/"}],"categories":[{"name":"ONOS","slug":"ONOS","link":"/categories/ONOS/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"}],"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"bookmark","text":"","link":"/bookmark/index.html"}]}