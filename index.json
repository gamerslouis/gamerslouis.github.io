[{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/tags/obsidian/","section":"標籤","summary":"","title":"Obsidian","type":"tags"},{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/categories/","section":"分類","summary":"","title":"分類","type":"categories"},{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/posts/","section":"文章","summary":"","title":"文章","type":"posts"},{"content":"繼將部落格框架從 Hexo 換到 Vue 再換到 Hugo，我的部落格撰寫流程也迎來了一個重大的改變。從原本使用 Notion 來管理及撰寫文章，轉換到使用 Obsidian 來完成。今天，我將介紹是什麼促使我從 Notion 轉換到 Obsidian，並且說明在使用 Obsidian 搭配 Hugo 撰寫 Markdown 部落格時，應該如何進行設定與整合。\n選擇 Markdown 部落格的理由 # 首先要說明為什麼，我選擇以 Markdown 為基礎的部落格框架。使用 Markdown 作為部落格的基礎，最大的優勢就是它所提供的極高彈性。由於內容基於 Markdown 檔案，只要任何框架能接收 Markdown 格式並生成部落格網頁，就可以不受平台限制地自由切換。這也是為什麼我能從 Hexo 快速轉移到 VuePress，再到現在的 Hugo。此外，這類部落格工具在功能強大的基礎上，透過模板等機制提供了便捷的修改空間，讓主題更換或特殊功能添加變得輕而易舉。\n同樣地，對於文章的撰寫工具，只要它最終能產出 Markdown 文件，就能夠遷移到任何平台，甚至是直接使用文字編輯器進行寫作。而對於 Markdown 編輯器，我認為最重要的一點是要解決圖片上傳的問題。對於 Markdown 部落格而言，圖片通常會上傳到圖床並將連結嵌入 Markdown 文件中，或者將圖片放在本地的 static 資料夾，然後同樣將路徑放置於 Markdown 文件內。\n使用一般的文字編輯器來撰寫部落格，除了無法預覽之外，我覺得最大的問題是增加圖片的動作會過於繁瑣，嚴重打亂寫作流程。\n其次，當然是要提供良好的寫作體驗。這包括軟體本身的反應速度、預覽功能，甚至是所見即所得（WYSIWYG）的編輯模式，以及字體大小、樣式等可自定義的選項。\n最後一點是 Frontmatter 的管理功能。對於 Markdown 部落格，我們需要使用 Frontmatter 來管理文章的 metadata，例如設定標籤、標記為草稿等。如果編輯器能夠提供相關的管理功能，那無疑是個加分項。\n使用 Notion 的工作流 # 回顧我的部落格撰寫歷程，最早我是使用 VSCode 來撰寫部落格，並將文章保存在 GitHub Repo 中，配合 GitHub Actions 自動編譯成 GitHub Pages。\n當時 VSCode 的 Markdown 編輯器尚未提供貼上圖片自動儲存至同資料夾的功能，因此每次增加圖片都非常麻煩。由於我本身有在使用 Notion，便萌生了能否使用 Notion 來撰寫部落格的想法。首先，Notion 的主要功能與 Markdown 格式相容，可以匯出 Markdown 文件，同時也提供了所見即所得的編輯器和圖片上傳功能。此外，得力於 Notion 的看板功能和資料庫功能，我可以方便地管理文章的 metadata。\n為了將 Notion 與部落格整合，我在網路上找到了一個由他人提供的 GitHub Actions 流程，可以將文章下載下來並提交為 Markdown 文件，同時提供自動將圖片轉傳到圖床的功能。由於我不喜歡將圖片上傳到圖床，所以我對其進行了修改，讓圖片下載到儲存庫中，並以 MD5 值作為檔名。\n就這樣，我的第一代部落格編輯器工作流誕生了。\n遇見 Obsidian # 大約在使用 Notion 後不久，我接觸到了 Obsidian 這個 Markdown 筆記工具。對我來說，Obsidian 最大的重點在於其速度及可調整性。Notion 基於網頁的架構，導致即使下載了 Notion 軟體，每次打開應用程式和切換頁面還是會有肉眼可見的時間延遲。此外，Notion 有限的字體和主題也讓我有些不滿。還有一點是 Notion 基於區塊（block）的結構，有時在修改文章、複製貼上時常常會發生誤操作。\n相反地，Obsidian 的效率非常高，開啟速度極快（我還曾使用 Logseq 作為筆記工具，但其啟動時間和不太直觀的區塊結構讓我卻步）。Obsidian 同樣支援貼上圖片，並提供了方便的主題及字體調整功能。在撰寫 Linux Kernel 網路巡禮 :: 2024 iThome 鐵人賽 的過程中，我就是先在 Obsidian 寫完再貼到 iThome 的編輯器。此後，一部分的部落格文章，也是先在 Obsidian 寫完再貼到 Notion 管理。\n整合 Obsidian 與 Hugo # 原本我只是將 Obsidian 當作單純的筆記軟體來使用，直到最近比較大量地使用 Obsidian 並研究其插件功能之後，我才想到：為什麼我不直接使用 Obsidian 來編輯 Hugo 儲存庫資料夾，直接將 Obsidian 作為部落格編輯器呢？\n前面提到使用 Obsidian 的諸多好處：高效快速、可自定義的主題讓人感到非常舒適，以及插件和屬性（properties）可以方便地管理 Frontmatter，還有模板等插件可以提升寫作效率。對我來說，本地化儲存也不是缺點，反正最終都會保存到 Git Repo。\n使用 Obsidian 作為部落格編輯工具，說起來並不難。直接將 posts 資料夾作為 Vault 打開即可開始撰寫，這得益於 Obsidian 完全基於 Markdown 文件的特性。\n圖片上傳及其他問題 # 然而，說簡單還是有一個很大的問題需要解決，那就是圖片上傳的問題。雖然 Obsidian 支援圖片貼上並保存到特定資料夾，但對於 Hugo 這類部落格系統而言，我們放在 static/images 目錄下的圖片，實際出現在文章中的連結是 /images/xxxx.png，而放在 content/posts/ 中的文章實際網址是 posts/xxxxx/。因此，Obsidian 的預設圖片處理方式無法正常運作。\n網路上有一些解決方案：\n透過插件將圖片上傳到圖床，然後嵌入連結。然而，一方面我不希望上傳到圖床，另一方面這會導致舊文章無法正常顯示圖片。\n在根目錄建立一個 images 目錄，然後指定 Obsidian 將圖片保存到這個目錄下，並在 Hugo 的 config.toml中進行設定。這樣放在 images 中的圖片，就會被 Hugo 視為放在 static 目錄下。但是同樣地，這會導致舊圖片無法被讀取，而且這樣會增加一個資料夾，讓結構顯得有些雜亂。\n[[module.mounts]] source = \u0026#39;images\u0026#39; target = \u0026#39;static/images\u0026#39; 此外，還有兩個不太相關的小問題：預設的圖片名稱是 Pasted image \u0026lt;timestage\u0026gt;，非常不美觀；而且即使設定為絕對路徑模式，Obsidian 插入的連結也不會有最前面的斜線，例如 images/xxxx 而不是 /images/xxxx。這樣的格式 Hugo 是不接受的，因為會被當作相對路徑，而非網站的絕對路徑。\n還有一個不太相關的小問題是，如果將整個儲存庫當作 Vault 打開，Obsidian 裡面會看到一堆不相干的資料夾，顯得很雜亂。\n解決問題 # 最終，我想到的解決方案就是將 repo 與 Vault 分離，而且要做到這點非常容易，只需使用軟連結（symlink）功能即可。首先將 repo 保存在 blog 資料夾，然後建立另外一個 blog-obsidian 作為 Obsidian 的 Vault 目錄。\n# mac cd blog-obsidian ln -s ../blog/static/img ln -s ../blog/content/posts 建立這兩個連結，前者是我放置圖片的目錄（我的文章圖片統一放在 static/img/pages），後者是文章的目錄。這樣在 Obsidian 打開後，就可以正常看到所有舊文章的圖片了！而且 Obsidian 的目錄會非常乾淨，不會看到其他不相干的東西。\n圖片保存設定 # 接著，我們要調整圖片保存的位置和格式。我們需要將圖片放到 img/pages/ 資料夾，使用更好的檔案名稱取代 Pasted image \u0026lt;timestage\u0026gt;，並且採用 Markdown 連結而不是 Obsidian 的 ![[]] 格式。\n這邊我們不修改 Obsidian 的預設設定，而是直接安裝 Image converter 這個插件。\n首先，我們在插件設定裡面指定使用 link format。我們定義一個新的格式 markdown-abs，選擇 Markdown、Absolute。這樣圖片就會以 Markdown 連結格式的絕對路徑嵌入到 Markdown 文件中，並且是帶有最前面斜線的 /img/xxx..，解決了預設貼上的問題。\n注意：建立完按 Save 後，還要點一下新建的選項，不然還是會使用其預設設定。\n接著，Filename 使用預設提供的 NoteName-Timestamp 即可。雖然我原本是使用 MD5 hash 作為檔案名稱，但是這個插件沒有提供以內容 hash 作為檔案名稱的功能，而且會遇到建立多個同名檔案的問題，所以乾脆採用這種方式，還可以從檔案名稱看出最初來源於哪篇文章，還不錯。\n最後，location 就指定到我的 img/pages 目錄。\n這樣，圖片上傳的問題就完全解決了。\n其他推薦的 plugin # 除了上述的設定，這裡額外推薦一些實用的 Obsidian 插件：\nAuto Link Title: 當貼上連結時，會自動抓取網站的標題作為連結文字。\nQuickAdd: 透過設定快速鍵，可以快速建立新文章頁面到 content/posts 目錄下。它還提供了許多其他功能，這裡不多做介紹。我會搭配我的模板使用，模板內容如下：\n--- categories: description: tags: date: {{date:YYYY-MM-DD}} title: draft: true --- Dataview: 可以透過類似 SQL 的指令抓取 Frontmatter 的資料。你可以建立一個首頁（homepage），然後提供所有草稿的列表，方便查閱。\nlist file.frontmatter.title from \u0026#34;posts\u0026#34; WHERE file.frontmatter.draft = true 只要貼上這段程式碼即可。 其他待解決的問題 # 儘管目前的設定已經非常流暢，但仍有一些問題有待解決：\nObsidian 不支援中間帶空白的標籤（tag）。例如，我有一個標籤是 2022 IThome 鐵人賽 - 學習 EBPF 系列，會被 Obsidian 切成 2022、IThome 等好幾個標籤。我的解決方案是在 Markdown 文件中使用 _ 替代空白，然後在編譯 Hugo 部落格的 CI/CD 流程中再將其替換回來。\n目前貼上的圖片如果後來文章中移除，圖片仍然會保留在儲存庫中，需要手動刪除。雖然點擊圖片可以選擇 Delete Link and Image，但擔心遺忘而未刪除乾淨。未來會研究是否有更好的方法處理這個問題。\nObsidian 獨立成一個目錄後，真的要保存檔案時還是需要到 repo 那邊進行 commit/push 操作。現階段而言，我覺得這並不麻煩，而且可以在上傳前進行最終檢查。不過理想情況是能在 Obsidian 裡面新增一個按鈕，按下去後自動 commit 並將 draft 狀態改為 false。得益於強大的插件機制，相信這是可以實現的。\n最後，Hugo 有一個 lastmod 欄位來表示最後上傳時間。同樣希望能夠藉由 Obsidian 自動更新這個欄位，不過也有考慮應該放在 GitHub Actions 的 CI/CD 流程中處理會比較合適，所以也等待未來再解決。\n結語 # 總的來說，從 Notion 轉換到 Obsidian 進行部落格撰寫，雖然在初期設定上需要一些調整，但其帶來的高效、彈性和可自定義性，大幅提升了我的寫作體驗。期待未來能進一步探索更多自動化與效率提升的可能性。希望這篇文章能為同樣在尋找理想部落格撰寫流程的你，提供一些有價值的參考。\n","date":"May 24 2025","externalUrl":null,"permalink":"/posts/use-obsidian-as-markdown-blog-editor/","section":"文章","summary":"","title":"使用 Obdidian 作為 markdown 部落格的編輯器","type":"posts"},{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/categories/%E9%83%A8%E8%90%BD%E6%A0%BC/","section":"分類","summary":"","title":"部落格","type":"categories"},{"content":"","date":"May 24 2025","externalUrl":null,"permalink":"/tags/","section":"標籤","summary":"","title":"標籤","type":"tags"},{"content":"","date":"May 22 2025","externalUrl":null,"permalink":"/categories/networking/","section":"分類","summary":"","title":"Networking","type":"categories"},{"content":"","date":"May 22 2025","externalUrl":null,"permalink":"/tags/%E6%8A%80%E8%A1%93%E5%88%86%E4%BA%AB/","section":"標籤","summary":"","title":"技術分享","type":"tags"},{"content":"憑證在驗證網站身份、確保 TLS 連線安全中扮演關鍵角色，然而，憑證背後的各種標準、編碼格式、驗證機制以及相關的技術細節，往往容易讓人感到模糊。這篇文章將會梳理數位憑證的技術脈絡，從其底層的編碼格式、核心的 X.509 標準、到如何層層驗證憑證鏈，以及各種安全機制。\nPEM 與 DER：憑證的編碼格式 # 當我們談論數位憑證時，首先會遇到的是它們的儲存與傳輸格式。其中，DER (Distinguished Encoding Rules) 是基於 ASN.1 (Abstract Syntax Notation One) 的一種嚴格的二進位編碼格式。\nASN.1 是一個獨立於特定語言和平台的標準，它定義了一套描述資料結構的規則（類似於 gRPC 使用的 Protobuf），並能將這些結構化的資料轉換為特定的二進位格式。這種標準化的方式確保了不同系統間資料交換的一致性。\nDER 編碼的憑證通常以 .der 作為副檔名，其內容是緊湊的二進位資料。\n例如，以下是一個 ASN.1 定義的簡單資料結構：\nEmployeeRecord ::= SEQUENCE { employeeID INTEGER, isFullTime BOOLEAN } 這個定義表示 EmployeeRecord 是一個包含 employeeID（整數）和 isFullTime（Boolean）欄位的複合結構。\n進一步地，ITU-T 制定的 X.690 技術規範詳細定義了多種將 ASN.1 資料結構轉換成二進位的方式，其中就包含了常見的 BER (Basic Encoding Rules) 和 DER 編碼。在實際應用中，憑證通常會使用 DER 編碼進行保存，而其具體內容結構則由 X.509 等標準定義的各種 ASN.1 結構來規範。\n與 DER 相輔相成的是 PEM (Privacy Enhanced Mail) 格式。PEM 本質上是對 DER 編碼的二進位檔案進行 Base64 編碼，使其能夠以純文字形式表示，方便在電子郵件或文字環境中傳輸。PEM 格式的特徵是其內容會被 -----BEGIN...----- 開頭和 -----END...----- 結尾的區塊所包圍，例如我們常見的 -----BEGIN CERTIFICATE-----。\nPEM 檔案通常使用 .pem 作為副檔名。值得注意的是，一個 .pem 檔案可以包含多個這類 BEGIN/END 區塊，這意味著它能夠在單一檔案中儲存多個憑證（例如憑證鏈中的多個憑證）或金鑰。\nX.509：憑證的標準 # 了解了憑證的編碼格式後，我們需要探究憑證本身的內容結構。在數位憑證領域，X.509 是最廣泛使用的標準。我們通常所說的 X.509，特指由 RFC 5280 描述的 X.509 v3 版本，它詳細定義了公開金鑰憑證的資料結構和語義。\nRFC 5280 定義了憑證的核心結構如下：\nCertificate ::= SEQUENCE { tbsCertificate TBSCertificate, signatureAlgorithm AlgorithmIdentifier, signatureValue BIT STRING } 這個結構清晰地表明，一個憑證由三大部分組成：待簽署的憑證資訊（tbsCertificate）、簽章演算法（signatureAlgorithm）和數位簽章值（signatureValue）。您可以透過 ASN.1 JavaScript decoder 等工具，實際查看憑證的 ASN.1 結構，以加深理解。\nDistinguished Name (DN)：憑證中的實體識別 # 在憑證的內容中，一個重要的概念是 DN (Distinguished Name)。DN 是一種標準化的階層式名稱格式，用於唯一識別憑證的主體或簽發者。它常見於憑證和 LDAP (Lightweight Directory Access Protocol) 中，例如在 LDAP 中，CN=Jeff Smith,OU=Sales,DC=Fabrikam,DC=COM 用於描述一個使用者。\nDN 是一個由 Relative Distinguished Name (RDN) 序列組成的。每個 RDN 都是一個鍵（正式名稱為 AttributeType）- 值對。\nDN 的讀取順序通常是從右到左，表示從上層往下索引。例如，對於 cn=Alice Smith,ou=Sales,o=Example Corp,c=US 這個 DN，它可以被理解為以下的階層結構：\nc=US (國家：美國) | o=Example Corp (組織：範例公司) | ou=Sales (組織單位：銷售部) | cn=Alice Smith (通用名稱：Alice Smith) RFC 4519 (LDAP Schema) 規範了一些常見的屬性類型 (Attribute Type)，這些屬性類型在 X.500 和 X.509 中也廣泛使用：\nString X.500 AttributeType CN commonName (2.5.4.3) L localityName (2.5.4.7) ST stateOrProvinceName (2.5.4.8) O organizationName (2.5.4.10) OU organizationalUnitName (2.5.4.11) C countryName (2.5.4.6) STREET streetAddress (2.5.4.9) DC domainComponent (0.9.2342.19200300.100.1.25) UID userId (0.9.2342.19200300.100.1.1) 追溯歷史，最早出現的是 ITU-T X.500 標準，旨在定義一套全面的目錄服務系統，並利用 Distinguished Name 來描述和定位唯一的實體。X.500 的 DN 定義了諸如 commonName、organizationName 等屬性，每個屬性都有對應的物件識別碼 (OID)。由於 X.500 過於複雜，簡化版的 LDAP 應運而生。LDAP 在 RFC 4514 中規範了應以 CN=Jeff Smith,OU=Sales,DC=Fabrikam,DC=COM 形式的字串來描述 DN，並在 RFC 4519 中定義了 RDN 中可能出現的屬性類型（基本沿用了 X.500 的定義）。同樣地，X.509 憑證標準也採用了 DN 作為描述憑證資訊的格式，並定義了一些必須支援的屬性，這些屬性大多來自 X.500 的定義。\nCertificate：憑證的核心內容解析 # 憑證的核心在於其所包含的資訊，這些資訊共同定義了憑證的身份、有效期和用途。以下我們以 Let\u0026rsquo;s Encrypt 的一個中繼憑證為例，解析憑證中的重要欄位：\ntbsCertificate (To Be Signed Certificate)：這是憑證中所有待簽署的資訊，它包含了憑證的實質內容。\nversion: 憑證的版本號，現今基本都使用 X.509 v3 (0x2) 。\nSerial Number: 由憑證簽發機構 (CA) 給定的一個唯一識別該憑證的序列號，這在憑證撤銷列表 (CRL) 中也扮演重要角色。\nIssuer: 表示這張憑證是由哪個 CA 簽署的，例如 CN=DST Root CA X3, O=Digital Signature Trust Co.。\nsubject: 憑證所屬的主體，即憑證所驗證的實體，例如 CN=Let's Encrypt Authority X3, O=Let's Encrypt, C=US。\nvalidity: 憑證的有效期限，包含起始日期 (Not Before) 和結束日期 (Not After)。例如：\nNot Before: Mar 17 16:40:46 2016 GMT Not After: Mar 17 16:40:46 2021 GMT subjectPublicKeyInfo: 憑證持有者提供的公開金鑰資訊，包含金鑰的演算法（例如 rsaEncryption (PKCS #1)）和金鑰的實際數值。\nextensions: 這是 X.509 v3 憑證的重要功能，透過不同的擴展提供額外的資訊和功能，例如憑證的用途、憑證鏈的相關資訊等。\nsignatureAlgorithm 和 signatureValue：這兩部分共同構成了憑證的數位簽章。CA 機構會使用其上層憑證的私有金鑰對 tbsCertificate 的雜湊值進行加密，生成數位簽章。並可用上層憑證的 Public Key 驗證。\nsignatureAlgorithm: 指明用於生成數位簽章的演算法，例如 sha256WithRSAEncryption。\nsignatureValue: 實際的數位簽章值。\n雜湊演算法在數位簽章中的角色 # 在數位簽章的生成過程中，簽署者會先對原始資料（在憑證中即為 tbsCertificate 的內容）計算其雜湊值，然後使用自己的私有金鑰對這個雜湊值進行加密，得到數位簽章。接收者在驗證時，則會使用簽署者的公開金鑰解密數位簽章，得到原始的雜湊值，同時也對接收到的原始資料重新計算雜湊值，如果兩個雜湊值一致，則證明資料未被篡改且確實由簽署者發出。\nSource: Digital Signature diagram.svg - Wikipedia\n憑證鏈的驗證流程： # 數位憑證的信任模型是建立在「信任鏈」的概念之上。當我們收到一個憑證時，必須驗證它是否由一個受信任的機構簽發。這個驗證過程涉及從目標憑證向上追溯，直到找到一個我們預先信任的根憑證。\n其核心驗證步驟如下：\n身份匹配：目標憑證的 Issuer 欄位必須與其上層憑證的 Subject 欄位完全匹配。這確保了憑證是由聲稱的簽發者所簽發。\n數位簽章驗證：使用上層憑證的 subjectPublicKeyInfo 中包含的公開金鑰，來解密目標憑證的 signatureValue。解密後會得到一個雜湊值。同時，對目標憑證的 tbsCertificate 內容重新計算雜湊值。如果這兩個雜湊值一致，則證明目標憑證的內容未被篡改，且確實由上層憑證的私有金鑰所簽署。 這個過程會層層遞進，直到達到一個「信任錨點」(Trust Anchor)，通常是一個預先安裝在作業系統或瀏覽器中的根憑證 (Root Certificate)。這些根憑證是自我簽署的，它們的信任是內建的，無需進一步驗證。\n憑證鏈中間的憑證哪裡來？ # 在 TLS 握手過程中，伺服器通常會提供其自身的憑證以及任何必要的中繼憑證 (Intermediate Certificates)，以便用戶端能夠建立完整的憑證鏈。然而，有時用戶端可能需要額外獲取這些中繼憑證。\nX.509 憑證提供了一個名為 Authority Information Access (AIA) 的擴展，其中可以包含一個 URL，讓用戶端能夠下載所需的中繼憑證。\n例如，當我們透過 Qualys SSL Labs 網站查看 c2.synology.com 的憑證資訊時： 您可以看到 c2.synology.com 的憑證鏈中，前三個憑證是由伺服器在 TLS 握手過程中發送過來的，而最後一個則是本地信任的根 CA。第四個憑證標註為 Extra download。如果我們進一步解析第三個憑證的資訊，就會發現其中包含了 AIA 的資訊，指明了獲取第四個憑證的 URL： Authority Information Access: OCSP - URI:http://ocsp.rootg2.amazontrust.com CA Issuers - URI:http://crt.rootg2.amazontrust.com/rootg2.cer 透過這個機制，用戶端可以動態地獲取憑證鏈中缺失的部分，從而完成憑證的完整驗證。\n交互簽名 (Cross-Signing) # 在憑證信任鏈中，交互簽名是一種常見的機制，用於讓新興的憑證簽發機構 (CA) 所簽發的憑證能夠被廣泛信任，尤其是在其自身的根憑證尚未被所有瀏覽器和作業系統信任的情況下。\n以 Let\u0026rsquo;s Encrypt 為例，其官方說明中提到：Let\u0026rsquo;s Encrypt Authority X3 中間憑證擁有一對公開金鑰和私有金鑰。Let\u0026rsquo;s Encrypt 使用這個私有金鑰來簽署所有終端憑證，也就是我們最終從 Let\u0026rsquo;s Encrypt 獲得的憑證。Let\u0026rsquo;s Encrypt Authority X3 中間憑證本身則是由 ISRG Root X1 根憑證所簽發。然而，ISRG Root X1 在早期尚未受到大多數瀏覽器的信任。為了解決這個問題，並使 Let\u0026rsquo;s Encrypt 頒發的憑證能夠被廣泛信任，Let\u0026rsquo;s Encrypt 向一個已受主流瀏覽器信任的根憑證 DST Root CA X3 請求了交互簽名。經過 DST Root CA X3 的簽名後，產生了另一個版本的 Let\u0026rsquo;s Encrypt Authority X3 中間憑證。\ngraph TD A[ISRG Root X1] --\u0026gt; C[Let\u0026#39;s Encrypt Authority X3] B[DST Root CA X3] --\u0026gt; C C --\u0026gt; D[Server Cert] C --\u0026gt; E[Server Cert] C --\u0026gt; F[Server Cert] 多路徑信任鏈：彈性與兼容性 # 透過交互簽名機制，一個終端憑證可能不只一條有效的憑證鏈可以被驗證。這表示瀏覽器不論信任 ISRG 根憑證還是 DST Root CA X3 根憑證，都可以建立一條完整的信任鏈。接下來，我們將說明這是如何做到的。\n前面提到，對於一個終端憑證而言，其上層憑證需要滿足兩個要求：\nSubject Name 等於終端憑證的 Issuer。\n上層憑證的公開金鑰要能驗證附加在終端憑證上的數位簽章。\n然而，這兩個要求並沒有強制規定上層憑證必須是完全相同的一張憑證，只要上層憑證的 Subject Name 和 Public Key 能夠滿足匹配和驗證條件即可。因此，實際上，像 Let\u0026rsquo;s Encrypt 這樣的憑證簽發機構會準備兩張（或更多）內容相同但由不同根 CA 簽署的中繼憑證。\n例如，Let\u0026rsquo;s Encrypt 會將一份 CSR 分別提交給 ISRG（其自身的根 CA）和 DST Root CA X3（一個較早且廣泛信任的根 CA）進行簽署。由於簽署過程是使用這兩個根 CA 各自的私有金鑰進行的，所以最終生成的中繼憑證（例如，中繼憑證 A 和中繼憑證 B）雖然其 Subject Name 相同，但其內部包含的 Signature Value 數值會不同。\n這樣一來，就會存在兩條可驗證的憑證鏈：\n鏈一：從 ISRG 根憑證 -\u0026gt; 中繼憑證 A -\u0026gt; 終端憑證。\n鏈二：從 DST Root CA X3 根憑證 -\u0026gt; 中繼憑證 B -\u0026gt; 終端憑證。\n瀏覽器或作業系統只需要信任其中一張根 CA 憑證（無論是 ISRG 或 DST Root CA X3），就可以成功完成終端憑證的驗證。這種設計提供了極大的彈性與兼容性，尤其對於確保舊有系統也能信任新簽發的憑證至關重要。\n憑證吊銷機制： # 基於憑證鏈的信任機制，我們發現上層 CA 機構並沒有直接撤回已簽發憑證的能力，只能等待憑證自然過期。然而，在憑證私有金鑰洩漏、CA 機構受損或憑證資訊錯誤等緊急情況下，我們需要一種機制能夠立即撤銷憑證的有效性。為了解決這個問題，X.509 提供了兩種主要的憑證吊銷機制：憑證撤銷列表 (Certificate Revocation List, CRL) 和 線上憑證狀態協定 (Online Certificate Status Protocol, OCSP)。\n憑證撤銷列表 (Certificate Revocation List, CRL)：\nCA 會定期發布一個列表，其中包含所有已被其撤銷的憑證 Serial Number。用戶端可以下載這個列表，並在驗證憑證時檢查目標憑證的 Serial Number 是否在列表中。\nX.509 憑證中通常包含一個 CRL Distribution Points 擴展，它會告訴用戶端該去哪裡找到可能包含目標憑證的 CRL 列表。\n線上憑證狀態協定 (Online Certificate Status Protocol, OCSP)：\n相較於 CRL，OCSP 提供了一種更即時的憑證狀態查詢方式。用戶端會向 OCSP 伺服器發送查詢，詢問特定憑證的撤銷狀態（有效、已撤銷或未知）。\n由於 CRL 可能存在滯後性且大型 CRL 列表會消耗較多頻寬，瀏覽器通常會優先使用 OCSP 進行憑證狀態檢查。OCSP 伺服器的位址同樣可以透過憑證的 AIA 擴展獲得。\n憑證的簽發：從請求到生成 # 了解了憑證的結構和驗證機制後，我們來看看憑證是如何被簽發的。整個流程通常始於申請人生成一個 憑證簽署請求 (Certificate Signing Request, CSR)。\nCSR 通常基於 PKCS#10 (RFC 2986) 格式，這同樣是一個透過 ASN.1 定義的資料結構，並常使用 .csr作為副檔名。CSR 中包含了憑證所需的各種資訊，例如憑證的主體 (Subject)、申請者的公開金鑰資訊 (subjectPublicKeyInfo) 等。\n透過 openssl req -newkey rsa:2048 -nodes -keyout domain.key -out domain.csr 這樣的命令，可以同時完成私有金鑰 (domain.key) 和 CSR (domain.csr) 的生成。\n由 OpenSSL 產生的 .csr 檔案通常也是 PEM 格式的（即 DER 編碼後再進行 Base64 編碼），其內容會被 -----BEGIN CERTIFICATE REQUEST----- 和 -----END CERTIFICATE REQUEST----- 區塊包圍。\n-----BEGIN CERTIFICATE REQUEST----- MIICijCCAXICAQAwRTELMAkGA1UEBhMCQVUxE ... -----END CERTIFICATE REQUEST----- 此外，私有金鑰檔案通常遵循 PKCS#8 (RFC 5208) 標準。\n-----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- 當 CSR 被提交給 CA 後，CA 會驗證申請者的身份，然後使用其私有金鑰對 CSR 中的資訊（特別是申請者的公開金鑰和憑證主體資訊）進行數位簽章，最終形成一個完整的憑證。這個簽發完成的憑證通常會以 .crt 或 .cer 作為副檔名，儘管它們的內容通常是 PEM 格式的 X.509 憑證，也可能是未經 Base64 編碼的 DER 格式。有時，.ca 副檔名會用於代表中繼或根憑證，但其內容通常也是 PEM 格式的 X.509 憑證。\n值得一提的是，上述提到的 PKCS#1、PKCS#8、PKCS#10 都是 **Public Key Cryptography Standards (PKCS)**標準的一部分，這些標準由 RSA Security LLC 制定，旨在規範公開金鑰密碼學的各種實踐。其他重要的 PKCS 標準還包括：\nPKCS#12：用於將憑證（.crt）和其對應的私有金鑰（.key）封裝到一個單一檔案中，通常使用 *.pfx或 *.p12 副檔名，方便攜帶和部署。\nPKCS#7：用於加密、簽章和傳輸資料。CA 有時會使用 PKCS#7 將中繼憑證和簽署後的目標憑證打包成 .p7b 或 .p7c 檔案交付給申請者。\n憑證，TLS 連線與 Domain Name： # 當我們透過瀏覽器訪問一個網域並建立 TLS 連線時，確保憑證的合法性至關重要。那麼，網域名稱是如何與憑證關聯起來的呢？\n早期，憑證主要透過 subject 欄位中的 Common Name (CN) 來對應網域。例如，如果 github.com 的憑證 subject 是 CN=github.com，那麼這個憑證就可以用來驗證該網域的有效性。\n然而，使用 CN 的問題在於它只能標記一個網域名稱，並且在格式上存在一定的限制。為了解決這個問題，X.509 v3 引入了 Subject Alternative Name (SAN) 擴展。透過 subjectAltName 欄位，憑證可以標記多個網域名稱，也支援電子郵件地址、IP 位址、URI 等多種格式。例如，github.com 的憑證在 subjectAltName 中可能同時標註了 github.com 和 www.github.com，這使得該憑證能夠同時驗證這兩個網域的有效性。SAN 擴展極大地提升了憑證的靈活性和實用性。\n此外，雖然 RFC 5280 並未明確定義萬用字元（wildcard）的使用，但在實際應用中，我們通常可以使用 *.xxxx.xxx 這樣的格式來代表所有子網域，這就是所謂的萬用字元憑證 (Wildcard Certificate)。\n其他重要的憑證擴展 # 除了上述提到的擴展外，X.509 憑證還包含許多其他重要的擴展，它們定義了憑證的行為和約束：\nBasic Constraints Extension：這個擴展透過 cA (boolean) 欄位來控制憑證是否能夠作為 CA 憑證，即是否能夠簽發下一層憑證。如果 cA 為 TRUE，則該憑證是一個 CA 憑證；如果為 FALSE，則它是一個實體憑證，不能用於簽發其他憑證。\nName Constraints Extension：這個擴展通常出現在 CA 憑證中，它可以限制該 CA 憑證能夠簽發的下一層憑證必須屬於特定的網域或 IP 範圍。這對於建立更精細的信任策略和防止 CA 誤發憑證非常有用。\nKey Usage Extension：這個擴展定義了憑證中公開金鑰的特定用途，例如用於數位簽章、金鑰加密、憑證簽章等。\nExtended Key Usage Extension：這個擴展進一步定義了憑證的特定應用程式用途，例如用於伺服器認證 (TLS/SSL 伺服器)、客戶端認證 (TLS/SSL 客戶端)、程式碼簽章等。\n自簽憑證 (Self-Signed Certificates) # 根憑證本質上是一種自簽憑證。自簽憑證是由其自身私有金鑰簽署的憑證，這意味著它的 Issuer 和 Subject欄位是相同的。 憑證安全機制：憑證固定與憑證透明度 # 為了進一步增強憑證的安全性，業界還發展出了一些更高級的機制：\n憑證固定 ( Certificate Pinning )：這是一種安全機制，應用程式（例如行動應用程式）會預先「記住」或「固定」其預期伺服器憑證的公開金鑰或其雜湊值。當應用程式連接到伺服器時，它不僅會驗證憑證鏈，還會檢查伺服器提供的憑證是否與其預先固定的金鑰匹配。如果憑證鏈有效但金鑰不匹配，連線將被拒絕。這有助於防範惡意的中繼憑證攻擊。\n憑證透明度 (Certificate Transparency, CT)：CT 是一種公開日誌系統，要求所有新簽發的憑證都必須被記錄在公開可審計的日誌中。瀏覽器和其他用戶端可以檢查這些日誌，以確保其所信任的 CA 沒有錯誤地或惡意地簽發憑證。這增加了憑證簽發過程的透明度，有助於及早發現和糾正問題憑證。\n結語 # 從底層的 ASN.1 編碼與 PEM/DER 格式，到 X.509 標準定義的憑證結構，以及關鍵的憑證鏈驗證、吊銷機制，乃至於憑證簽發的流程與各種擴展，這些知識對於任何涉及網路安全、系統管理或軟體開發的工程師都至關重要。掌握這些技術細節，不僅能更好地排查問題、設計更安全的系統，也能在面對複雜的憑證議題時，具備更強的分析與解決能力。\n","date":"May 22 2025","externalUrl":null,"permalink":"/posts/understanding-digital-certificates-formats-signatures-and-verification/","section":"文章","summary":"","title":"搞懂數位憑證：檔案格式、簽署與驗證流程","type":"posts"},{"content":"","date":"May 22 2025","externalUrl":null,"permalink":"/tags/%E8%B3%87%E5%AE%89/","section":"標籤","summary":"","title":"資安","type":"tags"},{"content":"","date":"May 22 2025","externalUrl":null,"permalink":"/tags/%E6%86%91%E8%AD%89/","section":"標籤","summary":"","title":"憑證","type":"tags"},{"content":"","date":"April 7 2025","externalUrl":null,"permalink":"/tags/%E7%94%9F%E6%B4%BB%E7%B6%93%E6%AD%B7/","section":"標籤","summary":"","title":"生活經歷","type":"tags"},{"content":"","date":"April 7 2025","externalUrl":null,"permalink":"/categories/%E7%94%9F%E6%B4%BB%E7%B6%93%E6%AD%B7/","section":"分類","summary":"","title":"生活經歷","type":"categories"},{"content":"","date":"April 7 2025","externalUrl":null,"permalink":"/tags/%E9%9D%9E%E6%8A%80%E8%A1%93/","section":"標籤","summary":"","title":"非技術","type":"tags"},{"content":"這篇是我在 2025/3/13 ~ 2025/4/3 期間服役研發替代役 127 梯次 (替代役 263 梯次) 第九中隊的心得及整理，希望能夠幫助到一些未來要進來的朋友們。\n比較詳細的心得、有趣、奇怪的事情，可以參考同梯的文章，這邊只會講一些我覺得重要，對進去有幫助的事項。\n攜帶清單 # 一包衛生紙：可以第一天去洞 K 買\n刮鬍刀：建議帶自己的，洞 K 買的很爛，或著我不會用一般的刮鬍刀，可以帶電動的，只是不能充電\n指甲刀：要有收集盒的，可以第一天去洞 K 買\n濕紙巾、牙線、防蚊液、護手霜：我個人需要的東西，手不知到為啥超級乾的\n奇異筆：是違禁品，但第一天會用來在袋子上面寫字，寫完再被收到違禁品袋子裡面。違禁品放假離開前會還你\n手錶：台中這次公所有發，要看一下徵集令附的文件有沒有提到會送，其他地方就要自己帶了\n帽子：自己的帽子是出來後要帶的，在裡面是帶部隊的帽子\n一段泡棉膠、一些小的長尾夾：有些人建議可以帶來整理內務，我個人認為紙板就夠用的\n很多喉糖，京都念慈那種的話，建議可以帶個三到四盒，第一天去洞 K 買也行。除了練替歌可以吃，每天無聊也會想拿來吃，也可以拿來當玩具，很有用。\n袖珍包的衛生紙：日常使用\n娛樂紙張們：空白紙寫日記、小說、數獨、paper?、或任何東西，記得只能帶散裝的，帶書的話，除非把它拆成散裝，不然是違禁品，搭配 1~2 個資料夾，方便收納，還有好寫的原子筆。\n一定要帶的東西：徵集令、身分證、健保卡、郵局帳戶影本、畢業證書影本、折抵的文件，看公所文件再確認一下還差啥\n錢：3 千足夠了，建議一部分換成百抄，中間如果有機會可以買吃吃喝喝的會比較方便。\n盥洗用品：牙膏、牙刷、三合一\n個人藥品\n帶提把的塑膠袋：其功一個是拿來放第一天的髒衣服，這個用垃圾袋也行。進去之後，違禁品、貴重物品、藥品會分成三個袋子裝，然後統一收起來保管，才需要簽字筆來寫名子，沒袋子第一天會去洞 K 拿，所以也還好，但用自己的袋子在找袋子的時候辨識度就很高，不然全部堆在一起還要翻找。\n不要帶自己的內衣褲、襪子、雨衣、口罩、水壺、藍白脫，因為你只能用成功嶺發的，藍白拖有款式問題，進去在洞 K 買\n洞 K (OK 便利商店) 要買的東西 # 一包衛生紙 (連同自己帶的，總共兩包，檢查用，雖然只帶一包理論上也行，可是這樣比較省事) 藍白拖 額外 1 ~ 2 件內褲：有人說公發的不好穿，我是湊合著專 內衣、內褲、襪子基本上買了就是當備用，避免出問題。但理論上是完全不會需要買，也可以活得好好的，除非你弄丟或被偷走….，建議是最多買一套備用，不然收納不方便，也浪費。又或著你想把他當成消耗品的話，那也可以多帶幾套，穿幾次就直接丟掉。 不要買鋼盔海綿墊：理論上是打靶用的，但是沒有完全沒有任何問題 生活說明 # 總體來說每天的流程其實還蠻規律的，5 點左右起來整理內務，6 點集合跑步，吃早餐，上一個早上的課，接著吃午餐、午休，上下午的課，然後吃晚餐，晚上有時候會有活動或著部隊要填資料發東西，中期之後晚上要練替歌，最後洗打、睡覺，每隔幾天站個夜哨 (一個小時左右)。上課有上重要的技術比如 EMT-1，也有些單純就是聽法規說明。說充實嗎，每天都有事情要做，但是也很多無聊破碎的空閒時間，需要想辦法打發。比較有趣的部分對我來說就是成長營和打靶。最有價值的學習就是 EMT-1 相關的技術和知識。\n會比較不適應的大概有幾個點：\n作息：5 點起床，22 點睡覺的作息 內務整理：內務整理就是整個替代役期間最困難的東西，每天都要整理，一不小心還會被扣分，還要特別提早起來摸黑整理。請記得下列事項，不要跟我一樣被扣分了： 棉被要整平 牙膏是對其牙刷的底擺放 掛運動褲時，線頭不要掉出來，沒固定好可能會隨機在自己掉出來 最邊邊的衣架請貼其牆壁 蚊帳要麻煩臨兵合作幫忙，塞紙板真的是神器 衛生：雖然內衣褲每天都可以洗，但外衣頂多一周洗一次，毛巾不能送洗，前人留下來不一定乾淨的床組，很難洗的餐具。撐過去就沒事了\u0026hellip; 洗打：只有洗打時間可以用手機，洗打就是洗澡加打電話，短可能就只有 40 分鐘，長通常也頂多一個多小時，還要排隊洗澡，時間真的是不多 部隊習慣：一些部隊會有的，比如精神答數、聽口令動作啥的規矩要適應。 餐點：基本上就是減配版的團膳，10 頓裡面有 9 頓小白菜，肉 90% 炸排，永遠是熱的鹹湯、甜湯，但反正就該吃吃該喝喝，不然會餓的。每天只能期待有 85 度 C 補菜的日子。 打發時間：會有不少空檔時間，下課休息的時間、晚上填資料發東西的時間，最後一兩天的很多時間，這些時間不能又不能用手機，還是要想辦法打發一下，不然只能睡覺發待的話很無聊。 重點分享 # 我覺得沒必要購買額外的三寶 (內衣褲、襪子)，首先你只能洗消發或洞 K 買的三寶，此外一天基本上只能洗一套，雖然好像可以額外加錢洗另外一套，但沒啥理由一天洗兩套。洞 K 的話，有傳言洞 K 的內褲比消發好穿，但是我沒穿過，不知道。建議頂多買一套當作避免不小心弄丟或弄髒的備案。 寫日記是一個非常好消磨時間的事情，應該會發一本筆記本，但也可以自己帶白紙，或著帶一本小本的空白筆記本 (好像可以帶進去) 頭髮的標準是短於 3 mm，如果太長，進去第一天會被拉去剪頭髮，要額外花錢。有傳言，因為剪頭髮阿姨可以拿到錢，所以短於標準但是略長的頭髮也會被抓去剪。我們中隊去的時候，有長官用尺量，有一個同梯就真的剛好 3mm，阿姨要把他抓過去剪的時候就被長官擋回去了 建議自己帶好用的原子筆，消發的有點爛，不好寫字 第一天只能洗自己的內衣褲，外衣只能等放假離營帶出去洗，而且會鎖在倉庫裡面 三寶會每天送洗，制服、運動褲大概一周洗一次。毛巾不能送洗，只能自己想辦法洗。 打飯班 (幫大家發盤子、打菜、洗鍋子、處理廚餘) 雖然可以獲得加分機會，但真的是非常累的一個工作，而且我們中隊直接指定 1~40 為打飯班，其他固定公差班也是指定的。基本就看緣分了，我們是第一天一到成功嶺就馬上領到號碼衣，決定學號了，也沒有像有些人說的會到教室裡面才發。 手機會放重要物品櫃，然後鑰匙會給我們自己保管，公所剛好發了一個可以放在口袋大小的文具袋，我就把鑰匙和錢錢都每天隨身保管。 隨時都要戴口罩 錢主要用在三個地方，第一天去洞 K 採購、統一收取的照片費用和洗衣費用 (沒買照片、沒用完會退回)、有機會的話，會有幾次可以買 85 度 C 的小點心、飲料。我自己的花費大約是 2000 元，還是可以帶個三千比較保險。 因為要自己保管筷子和碗，所以建立帶個夾鏈袋比較方便 有必要的話，可以帶一點點雙面膠帶或泡棉膠帶、長尾夾，整理內務的時候會方便一點點，可以固定牙膏牙刷、運動褲的褲頭，避免被臨兵撞飛。 三合一不要買太大罐的，建議長度跟一般的牙膏差不多，太大的話，整理內務的時候可能會出一點小問題 橡皮筋也是可以帶個兩、三條拉，有需要的時候可以拿來用 現在鞋子 (皮鞋、運動鞋) 都是消發 (送給你) 的東西，尺寸不會跟自己的腳差太多，所以不需要鞋根貼了，但是鞋子很爛，有人自己帶好一點的鞋墊來塞鞋子。 不知道為啥，我的手超級乾，可以帶一下護手霜 可以帶電動刮鬍刀，只是不能充電 不會裝 MDM，手機會被收起來，只有洗打時間可以用 放假的時候，會走出來到中山路上面。隊伍會分家長接送和計程車兩種。家長接送的話會往箭頭方向在走過去。 中間有放假的話，真的要把不需要的東西帶回家，不然真的會很麻煩，會發一個黑大 (可以裝兩到三個背包在裡面的超大背包)，但是到時候要塞你的背包，兩雙鞋子，一個盒子，一個臉盆，我還是差點塞不下了。 要把握好 EMT-1 回答問題、互動加分的機會，不考慮打飯班和替歌第一的話，要加分的機會還是比較稀缺的，但是內務又容易扣一大堆，如果你不是打飯般的，想要湊到榮 6 離開還是比較困難的，想早點離開的話，加分的機會都要爭取一下 我們中隊前幾天不懂事，起床集合的時候，沒有把寢室和教室的燈關掉，集體扣 2 分，超級虧，請格外小心。 人生第一次捐血就在部隊裡面了，捐一下還是不錯的 350 塊的照片很貴，很多人說不要買，但就真的看個人喜好拉\u0026hellip;，有沒有想留著紀念，但是又超大張，大家都光頭還很難分辨誰是誰，真的是也挺坑的就是了\u0026hellip;。 要輪流站夜哨，但只站一個小時，在營期間大概會站到四次左右 替歌比賽 (軍歌比賽) 不太知道該怎麼評價這個東西，反正行有餘力的話，參加公差可以額外加 2 分，不過會每天少睡一個小時來做道具、練動作 (特技) 啥的 各個中隊的規局和要求會差蠻多的，這個就真的只能祝好運了，分配到比較好的中隊會稍微輕鬆一點，雖然該整理的內務、該做的事情還是要做，但是就可能比較不會罵人，也不會太過嚴格。 ","date":"April 7 2025","externalUrl":null,"permalink":"/posts/rdss-experiences/","section":"文章","summary":"","title":"研發替代役 127T 心得","type":"posts"},{"content":"","date":"February 19 2025","externalUrl":null,"permalink":"/tags/python/","section":"標籤","summary":"","title":"Python","type":"tags"},{"content":"","date":"February 19 2025","externalUrl":null,"permalink":"/tags/%E5%8E%9F%E5%A7%8B%E7%A2%BC%E5%88%86%E6%9E%90/","section":"標籤","summary":"","title":"原始碼分析","type":"tags"},{"content":"","date":"February 19 2025","externalUrl":null,"permalink":"/categories/%E8%BB%9F%E9%AB%94%E9%96%8B%E7%99%BC/","section":"分類","summary":"","title":"軟體開發","type":"categories"},{"content":" 前言 # 我使用 Python 的 FastAPI 開發了一個專案，並計畫導入 Prometheus 來收集 metrics。然而，在撰寫單元測試時，我遇到了許多神秘的問題。本來以為只是測試寫錯，但深入研究後發現，這其實與 Prometheus 的 Python SDK (prometheus_client) 的 multiprocess 實作方式有關。\n這篇文章將簡單探討 Prometheus 的 Python Client SDK 的多 multiprocess 設計，如何將其整合到 FastAPI 專案中，以及如何實現單元測試。同時，我也會說明我遇到的問題以及目前能想到的解決方案。\nPrometheus Client SDK 基本介紹 # 對於 API Server，我們通常希望收集一些 metrics，例如：\nrequests 的總數 response time HTTP 200 回應數量 其他與商業邏輯相關的 metrics 這些 metrics 可以透過 Prometheus 進行收集，並搭配 Grafana、Alert Manager 等工具進行監控。Prometheus 官方提供了一個 Python SDK，讓開發者能夠快速開發實作 metrics 並導出到 Prometheus。\n首先，我們來看看如何在 FastAPI 中使用 Prometheus Python Client SDK。\n這段程式碼展示了如何定義 Counter，用來計算所有 API Endpoint 的 請求次數：\n# main.py from prometheus_client import Counter, make_asgi_app from fastapi import FastAPI, Request from starlette.middleware.base import BaseHTTPMiddleware counter = Counter(\u0026#39;http_requests_total\u0026#39;, \u0026#39;Total HTTP requests\u0026#39;, [\u0026#39;method\u0026#39;, \u0026#39;endpoint\u0026#39;]) app = FastAPI() class PrometheusMiddleware(BaseHTTPMiddleware): async def dispatch(self, request: Request, call_next): counter.labels(request.method, request.url.path).inc() response = await call_next(request) return response app.add_middleware(PrometheusMiddleware) 這邊我們定義了一個 counter，並註冊成 FastAPI 的 middleware，每次有 request 進來的時候，counter 的數值就會加一 (inc)，且會將 HTTP method 和 URL path 作為 label，以便區分不同的 endpoints。\n接著，我們需要定義 /metrics 端點，讓 Prometheus 能夠拉取 metrics 資料：\napp.mount(\u0026#34;/metrics\u0026#34;, make_asgi_app()) 接著我們訪問端點就可以取得 metrics 資料，因為我們訪問 /metrics/ 這個端點，所以我們可以看到 http_requests_total 這個 counter 的數值是 1。\n# fastapi main.py # curl 127.0.0.1:8000/metrics/ # HELP http_requests_total Total HTTP requests # TYPE http_requests_total counter http_requests_total{endpoint=\u0026#34;/metrics/\u0026#34;,method=\u0026#34;GET\u0026#34;} 1.0 # HELP http_requests_created Total HTTP requests # TYPE http_requests_created gauge http_requests_created{endpoint=\u0026#34;/metrics/\u0026#34;,method=\u0026#34;GET\u0026#34;} 1.7377473771929123e+09 訪問 /metrics 時，FastAPI 會回傳 HTTP 307 並導向 /metrics/，如果使用 curl 測試時，需要加上 -L 參數才能跟隨重導，或者直接訪問 /metrics/。\n單元測試 Middleware # 那我們該怎麼對這個 middleware 進行單元測試呢？我們希望使用 pytest 對這個 middleware 進行單元測試，確保 metrics 能夠正確被記錄，以及導出到端點。\n# test_main.py from main import app from fastapi.testclient import TestClient client = TestClient(app) def test_middle(): response = client.get(\u0026#34;/metrics/\u0026#34;) assert response.status_code == 200 assert \u0026#39;http_requests_total{endpoint=\u0026#34;/metrics/\u0026#34;,method=\u0026#34;GET\u0026#34;} 1.0\u0026#39; in response.text FastAPI 提供 TestClient 讓我們可以模擬 API 請求並進行測試。在這個測試中，我們對 /metrics/ 發送 GET 請求，並檢查回傳的 metrics 是否包含 http_requests_total{endpoint=\u0026quot;/metrics/\u0026quot;,method=\u0026quot;GET\u0026quot;} 1.0。\npytest test.py ... ====================== 1 passed, 1 warning in 0.43s ======================= 當然我們得到了我們預期的結果，這個測試是成功的。\nMultiprocess 設計 # 接著，我們就要談到 prometheus python client 的 multiprocess 設計，上面的 middleware 在 multiprocess 的情況下是會有問題的，因為 counter 存在於每個 process 的記憶體中，因此不同 process 的計數互不相通。這導致 /metrics 端點回傳的內容，僅包含當前 process 的計數，而不是所有 process 的總和。\nPrometheus 的解決方案 # Prometheus Python Client 提供的解法是：\n每個 process 先將 metrics 資料寫入各自的檔案中 當訪問 /metrics 時，統一讀取所有檔案，合併數據後再回傳 使用上也很簡單：\n# main.py import os os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;] = \u0026#39;/dev/shm\u0026#39; 首先，我們需要設定 PROMETHEUS_MULTIPROC_DIR 環境變數，指定 metrics 檔案的儲存路徑。這樣一來，所有 Counter 都會自動進入 multiprocess 模式，並將資料寫入對應的檔案。\ndef make_prometheus_app(): registry = CollectorRegistry() multiprocess.MultiProcessCollector(registry=registry) return make_asgi_app(registry=registry) app.mount(\u0026#34;/metrics\u0026#34;, make_prometheus_app()) 接著，我們需要修改 /metrics 端點的導出方式，確保它能夠讀取多個 process 的數據並合併：\nCollectorRegistry 是用來管理所有計數器的結構，所有的 Counter、Gauge、Histogram、Summary 都會向一個或多個 Registry 註冊。當我們訪問 /metrics 時，實際上是呼叫 registry 的 collect() 方法，registry 會對應調用所有註冊在裡面計數器的 collect() 方法，將所有 metrics 資料合併起來導出。\ngraph TD Endpoint -- Collect --\u0026gt; Registry Registry -- Collect --\u0026gt; Counter1 Registry -- Collect --\u0026gt; Counter2 Registry -- Collect --\u0026gt; Counter3 前面我們定義 Counter 和使用 make_metrics_app 的時候沒有定義 registry，但是 prometheus client 會使用一個預設的 registry 來保存。\n# prometheus_client/asgi.py def make_asgi_app(registry: CollectorRegistry = REGISTRY): ... # prometheus_client/multiprocess.py class Counter(MetricWrapperBase): # prometheus_client/metrics.py class MetricWrapperBase(Collector): def __init__(self: T, ... registry: Optional[CollectorRegistry] = REGISTRY, ... ) -\u0026gt; None: 可以看到，這邊都使用了一個 REGISTRY 作為預設參數，這個 REGISTRY 就是一個預設全域變數的 CollectorRegistry。\n# prometheus_client/registry.py ... REGISTRY = CollectorRegistry() 所以 make_asgi_app 會呼叫 REGISTRY.collect()，而 REGITRY.collect() 則會呼叫到 counter.collect()。\n接著讓我們回到 multi proccess 的程式碼。\ndef make_prometheus_app(): registry = CollectorRegistry() multiprocess.MultiProcessCollector(registry=registry) return make_asgi_app(registry=registry) app.mount(\u0026#34;/metrics\u0026#34;, make_prometheus_app()) 這邊，我們定義了一個空的 registry 覆蓋 make_asgi_app 預設的 REGISTRY，並且其中只有放入 MultiProcessCollector。所以當我們呼叫 /metrics 的時候，等價於呼叫 MultiProcessCollector.collect()，MultiProcessCollector 會讀取 PROMETHEUS_MULTIPROC_DIR 中的所有的檔案，將所有 process 的 metrics 資料合併起來導出。\ngraph TD Endpoint -- Collect --\u0026gt; Registry Registry -- Collect --\u0026gt; MultiProcessCollector MultiProcessCollector -- Read Files --\u0026gt; PROMETHEUS_MULTIPROC_DIR Counter1-- Write --\u0026gt; PROMETHEUS_MULTIPROC_DIR Counter2-- Write --\u0026gt; PROMETHEUS_MULTIPROC_DIR Counter3-- Write --\u0026gt; PROMETHEUS_MULTIPROC_DIR 在 prometheus 裡面，計數器有 Counter, Gauge, Histogram, Summary 等種類，對於每個類型計數器的不同 Process，prometheus client 都會寫到 PROMETHEUS_MULTIPROC_DIR/_.db 的檔案裏面。\nls /dev/shm counter_2463.db counter_3743.db 第一個陷阱：PROMETHEUS_MULTIPROC_DIR 的設置時間 # 如果，如果按照我上面描述的方式撰寫出以下的程式碼，將環境變數的設置放在 Import 後面的第一行。\nimport os from prometheus_client import Counter, CollectorRegistry, multiprocess, make_asgi_app from fastapi import FastAPI, Request from starlette.middleware.base import BaseHTTPMiddleware os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;] = \u0026#39;/dev/shm\u0026#39; 那麼你就會發現，不論怎麼讀 /metrics都是空的。而原因呢，我們就需要了解 Prometheus Client 是怎麼判定是否要啟用 multiprocess 模式，將 metrics 寫入檔案的。\ncounter = Counter(\u0026#39;http_requests_total\u0026#39;, \u0026#39;Total HTTP requests\u0026#39;, [\u0026#39;method\u0026#39;, \u0026#39;endpoint\u0026#39;]) # prometheus_client/metrics.py class Counter(MetricWrapperBase): def _metric_init(self) -\u0026gt; None: self._value = values.ValueClass(self._type, self._name, self._name + \u0026#39;_total\u0026#39;, self._labelnames, self._labelvalues, self._documentation) self._created = time.time() 可以看到，在 Counter 內部會呼叫 _metric_init 函數，實例化用於保存數值的 values.ValueClass。\n### prometheus_client/values.py def get_value_class(): # Should we enable multi-process mode? # This needs to be chosen before the first metric is constructed, # and as that may be in some arbitrary library the user/admin has # no control over we use an environment variable. if \u0026#39;prometheus_multiproc_dir\u0026#39; in os.environ or \u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39; in os.environ: return MultiProcessValue() else: return MutexValue ValueClass = get_value_class() 而這個 ValueClass 實際上是一個全域變數，會被metrics.py 導入，因此，當我們 import prometheus_client 時，prometheus client 就已經根據環境變數 PROMETHEUS_MULTIPROC_DIR 是否存在來決定是否要啟用 multiprocess 模式了。\n這邊 ValueClass 會變成是 MultiProcessValue() 的返回值，但具體是是甚麼，我們後面再來看。\n所以，如果我們在 import prometheus_client 之後才設置環境變數，那麼 prometheus client 就不會啟用 multiprocess 模式，而是單純把數值保存在記憶體中。\n因此，如果要透過程式碼設置去啟用這個功能的話，我們必需要在 import 之前設置好環境變數，又或是在執行 Python 前就完成環境變數的設置。\nimport os os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;] = \u0026#39;/dev/shm\u0026#39; from prometheus_client import Counter, CollectorRegistry, multiprocess, make_asgi_app from fastapi import FastAPI, Request from starlette.middleware.base import BaseHTTPMiddleware 第二個陷阱：環境變數的設置 # 因為我們採用了 multiprocess 的模式，為了單元測試，我們需要確保每次測試前後 PROMETHEUS_MULTIPROC_DIR 是空的，不然就會讀到上一次測試的 metrics 資料以及其他測試的紀錄。\n直覺的想法是我們就修改 PROMETHEUS_MULTIPROC_DIR 的位置，將 metrics 保存到暫時的資料夾中。因此，第一版的測試修改後變成這樣。\n# test_main.py import os import tempfile from main import app import pytest from fastapi.testclient import TestClient client = TestClient(app) @pytest.fixture() def clean_metrics(): os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;] = tempfile.mkdtemp() yield shutil.rmtree(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]) def test_middle(clean_metrics): response = client.get(\u0026#34;/metrics\u0026#34;) assert response.status_code == 200 assert \u0026#39;http_requests_total{endpoint=\u0026#34;/metrics/\u0026#34;,method=\u0026#34;GET\u0026#34;} 1.0\u0026#39; in response.text 要特別注意的是我們在 main.py 的開頭有設置一次 PROMETHEUS_MULTIPROC_DIR，所以我們可以確保 multiprocess 的模式是啟用的。\n這邊藉由 pytest 的語法來插入一個 pretest 和 posttest 的 action。 我們藉由 tempfile.mkdtemp() 取得一個隨機目錄，然後修改 PROMETHEUS_MULTIPROC_DIR 的位置，讓 metrics 保存到這個目錄中，然後，在測試結束後刪除這個目錄。\n跑了這個測試你會發現不論怎麼跑 metrics 這個端點回傳都是空的，或著如果你有先跑過幾次這個 API，然後才跑測試你會發現，他會回復之前的資料，並且這次測試訪問不會被記錄到。\n要瞭解為甚麼發生這種神奇的現象，要了解一下 multiprocess collector 是如何決定要讀取哪個資料夾的。\nMultiprocess Collector 會在初始化的時候就決定要讀取哪個資料夾。\n# site-packages/prometheus_client/multiprocess.py class MultiProcessCollector: \u0026#34;\u0026#34;\u0026#34;Collector for files for multi-process mode.\u0026#34;\u0026#34;\u0026#34; def __init__(self, registry, path=None): if path is None: path = os.environ.get(\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;) self._path = path 並且我們在 import main.py 的時候就已經完成了對 MultiProcessCollector 的初始化。\n# main.py def make_prometheus_app(): registry = CollectorRegistry() multiprocess.MultiProcessCollector(registry=registry) return make_asgi_app(registry=registry) app.mount(\u0026#34;/metrics\u0026#34;, make_prometheus_app()) 所以 MultiprocessCollector 會去查看原本設定的 /dev/shm，自然就讀不到測試的資料。\n陷阱三：消失的 metrics # 那麼有沒有辦法在，不修改主程式的情況下，修復測試造成的錯誤。那我想到，我們就不要改路徑，直接把 PROMETHEUS_MULTIPROC_DIR 清理乾淨，那麼這樣就可以了吧？\n@pytest.fixture() def clean_metrics(): for filename in glob.glob(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]+\u0026#39;/*\u0026#39;): os.remove(filename) yield for filename in glob.glob(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]+\u0026#39;/*\u0026#39;): os.remove(filename) 恭喜你會發現這個測試是行的通的！\n然而當你添加了另外一個測試，並且這個測試跑在 middleware 的測試之前執行，你就會發現，client.get(\u0026quot;/metrics\u0026quot;) 回傳的資料又是空的了，這是為什麼呢？\n這時候，我們就需要深入了解 Counter 是如何把資料保存到檔案中的。\nclass Counter(MetricWrapperBase): def _metric_init(self) -\u0026gt; None: self._value = values.ValueClass(self._type, self._name, self._name + \u0026#39;_total\u0026#39;, self._labelnames, self._labelvalues, self._documentation) 前面我們有說過，Counter 的實際讀寫是透過 values.ValueClass，並且在 import 的時候被指定為 MultiProcessValue()。\ndef MultiProcessValue(process_identifier=os.getpid): \u0026#34;\u0026#34;\u0026#34;Returns a MmapedValue class based on a process_identifier function. The \u0026#39;process_identifier\u0026#39; function MUST comply with this simple rule: when called in simultaneously running processes it MUST return distinct values. Using a different function than the default \u0026#39;os.getpid\u0026#39; is at your own risk. \u0026#34;\u0026#34;\u0026#34; files = {} values = [] pid = {\u0026#39;value\u0026#39;: process_identifier()} # Use a single global lock when in multi-processing mode # as we presume this means there is no threading going on. # This avoids the need to also have mutexes in __MmapDict. lock = Lock() class MmapedValue: \u0026#34;\u0026#34;\u0026#34;A float protected by a mutex backed by a per-process mmaped file.\u0026#34;\u0026#34;\u0026#34; _multiprocess = True def __init__(self, typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode=\u0026#39;\u0026#39;, **kwargs): with lock: ... self.__reset() ... def __reset(self): typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode = self._params file_prefix = typ if file_prefix not in files: filename = os.path.join( os.environ.get(\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;), \u0026#39;{}_{}.db\u0026#39;.format(file_prefix, pid[\u0026#39;value\u0026#39;])) files[file_prefix] = MmapedDict(filename) self._file = files[file_prefix] self._key = mmap_key(metric_name, name, labelnames, labelvalues, help_text) self._value, self._timestamp = self._file.read_value(self._key) def inc(self, amount): ... def set(self, value, timestamp=None): ... def get(self): ... return MmapedValue 可以看到實際上，values.ValueClass 是 MmapedValue 。\nMmapedValue 的 __init__ 會呼叫到 __reset，而 __reset 會實例化一個 MmapedDict，由 MmapedDict 來與實際的檔案互動操作。\n可以發現，這邊會使用 file_prefix 作為 key，將 MmapedDict 保存在 files 這個 dictionary 中，file_prefix 實際上是 Metrics 的類型，也就是 Counter、Gauge、Histogram、Summary 等等。 而 files 是保存在 MultiProcessValue 函數的上下文當中的，因為 MultiProcessValue 只會在 values.py 被 import 得時候被執行一次，所以每個類型 metrics 的 MmapedDict 都是全局唯一的。\n### prometheus_client/values.py def get_value_class(): # Should we enable multi-process mode? # This needs to be chosen before the first metric is constructed, # and as that may be in some arbitrary library the user/admin has # no control over we use an environment variable. if \u0026#39;prometheus_multiproc_dir\u0026#39; in os.environ or \u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39; in os.environ: return MultiProcessValue() else: return MutexValue ValueClass = get_value_class() 當我們第一次寫入 Counter 時，_metric_init 會被觸發建立 MmapedDict 實例，將檔案透過 MMAP 映射到記憶體中操作，後續所有 Counter 的操作都會透過這個 MmapedDict 來操作檔案。\n因為實際上是透過 MMAP 操作，加上這個也只是臨時檔案，所以才會建議將檔案放置在 /dev/shm 這個純記憶體的檔案系統中，提高 IO 效能。\n如果實際去看 Counter 的原始碼，會發現在 Couter 的 base MetricWrapper ，__init__ 會呼叫 _metric_init，但是會先過一個判斷是會先過一個判斷是，if self._is_observable()，實際上這個數值會是 False，導致 _metric_init 不備呼叫。Prometheus 的設計是每次呼叫 counter.labes(...) 的時候，會為每組不同的 labels，會建立一個 Counter 實例，這個子實例的 Observable 才會是 True，才會呼叫 _metric_init。\n@pytest.fixture() def clean_metrics(): for filename in glob.glob(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]+\u0026#39;/*\u0026#39;): os.remove(filename) yield for filename in glob.glob(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]+\u0026#39;/*\u0026#39;): os.remove(filename) 假設，我們 middleware 的 test 不是第一個執行的，當其他測試進行時，就會記錄到 http_requests_total 的 Counter，導致 MmapedDict 被建立。當執行到我們的測試時，clean_metrics會把對應的檔案刪除，但是 MmapedDict 不會被刪除，因此當 mddleware 的 test 執行時，實際上就不會寫入到任何地方，同時因為只是 MMAP 記憶體對應的檔案消失， MmapedDict 也不會報錯。 因為，pytest 執行不同的 test 實際上不會重新 import，所以 MmapedDict 不會被重新建立。\n解決方案 # 最後想到一個比較簡單的方案，來讓測試通過，就是只在測試後刪除檔案。\n@pytest.fixture() def clean_metrics(): yield for filename in glob.glob(os.environ[\u0026#39;PROMETHEUS_MULTIPROC_DIR\u0026#39;]+\u0026#39;/*\u0026#39;): os.remove(filename) 這樣可以確每次測試後，都不會留下任何 metrics 的檔案，因為執行後續其他測試項目時，也不會重建檔案，反正 metrics 的資料對於其他單元測試來說無意義。另外，我們的單元測試會檢查一個特殊的 metrics http_requests_total{endpoint=\u0026quot;/metrics/\u0026quot;,method=\u0026quot;GET\u0026quot;}，我們可以確保其他測試不會呼叫到 /metrics 這個端點，所以這個測試也不會受到其他測試向的影響。\n結論 # 在這次紀錄中，我們探討了 Prometheus SDK 的運作原理。Prometheus SDK 透過檔案來記錄各個 Process 的 metrics，並聚合起來產生實際的 metrics 資料。 PROMETHEUS_MULTIPROC_DIR 會決定 metrics 檔案的保存位置，並在 import prometheus_client 的時候，SDK 會根據該變數是否設置來決定 Couter 要採用 singleprocess 還是 multiprocess 的行為，所以要在 import 之前設置好環境變數。 MultiProcessCollector 會在初始化的時候讀取 PROMETHEUS_MULTIPROC_DIR ，並且在之後的操作都會讀取這個資料夾。 Couter 等計數器會在第一次寫入數值時，打開檔案建立 MMAP，測試切換後，因為 pytest 實際上不會重新 import，所以 MmapedDict 不會被刪除，如果檔案被刪除，並不會報錯，但也不會建立新的檔案。\n","date":"February 19 2025","externalUrl":null,"permalink":"/posts/pitfalls-of-unit-testing-the-multiprocess-prometheus-python-client/","section":"文章","summary":"","title":"對 Prometheus Python SDK 在 Multi Process 模式下進行單元測試的陷阱","type":"posts"},{"content":"","date":"January 19 2025","externalUrl":null,"permalink":"/tags/golang/","section":"標籤","summary":"","title":"Golang","type":"tags"},{"content":"","date":"January 19 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"標籤","summary":"","title":"Hugo","type":"tags"},{"content":" 前言 # Hugo 是一個 Markdown 的靜態網站生成器。不同的 Markdown 靜態網站生成器本質上功能都是一樣的，都是將 Markdown 轉換成 HTML，然後生成一個靜態網站。不同靜態網站生成器的差異就在於它所使用的模板 (Template) 機制，hexo 使用的是 ejs 模板，vuepress 直接使用 Vue.js 搭配 SSR 來生成網站，而 Hugo 使用的是 Go 語言的 text/template 模組。不同的 Template 機制直接決定了怎麼去開發一個靜態網站的主題，以及怎麼去擴展主題的功能。這篇文章將會帶您了解 Hugo 的 Template 機制，了解如何去覆蓋、擴展主題的結構及樣式，增加自己的頁面、功能。\ngolang text/template 模組 # 一個基本的部落個網頁 HTML 結構是由許多個小區塊所組成的，HTML 最基本的結構包含了 head 跟 body，而 body 根據網頁顯示的架構，通常又會分成 header, main, footer 等等區塊，不同的頁面可能會共用其中部分的區塊，也可能會有自己獨立的區塊，這些區塊如同拼圖一般的構成在一起。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; ... \u0026lt;/header\u0026gt; \u0026lt;main\u0026gt; \u0026lt;article\u0026gt; ... \u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;footer\u0026gt; ... \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; 比如說，所有的\u0026quot;文章\u0026quot;頁面，基本上只是在內容上有所差異，所有介面的結構都是一樣的，可是首頁的整個 main 區塊就與文章頁面不同，但是 footer 跟 header 都有所不同。Hugo 就藉由了 Golang 的 template 模組，讓我們可以像拚拚圖一樣的把所有的不同區塊組合在一起，構成每個頁面。\n基本介紹 # Hugo 使用了 Go 的 template 模組，因此要了解 Hugo 的運作就必須要知道 golang 的 template 模組是怎麼運作的，但是這邊只會簡單的介紹一下，建議還是需要先去了解學習 Golang 的 template 模組。\n在 Golang 的模板中，用雙大括號 {{ }} 來標記帶有特殊功能的區塊，比如說變數、函數、條件判斷、迴圈等等。\nHello, {{ .Title }} 比如說在這個範例中，{{ .Title }} 表示要渲染一個變數，名稱是Title，這個變數是在渲染模板時傳入的，假設傳入的數值是World，那麼這個模板就會渲染成Hello, World。\n{{ if .IsHome }} \u0026lt;h1\u0026gt;Home\u0026lt;/h1\u0026gt; {{ else }} \u0026lt;h1\u0026gt;Not Home\u0026lt;/h1\u0026gt; {{ end }} 在這個範例中，{{ if .IsHome }} 表示要進行一個條件判斷，如果.IsHome為真，則渲染\u0026lt;h1\u0026gt;Home\u0026lt;/h1\u0026gt;，否則渲染\u0026lt;h1\u0026gt;Not Home\u0026lt;/h1\u0026gt;。\nBlock # 為了要讓主題開發者，可以以拚積木的方式，我們需要一個方式去定義每一個積木的長相，然後定義如何將這些積木拼裝在一起。\n在 golang 裡面，我們可以使用 define 來定義一個積木，然後在其他地方使用 block 來引用這個積木。\n{{ define \u0026#34;header\u0026#34; }} \u0026lt;header\u0026gt; ... \u0026lt;/header\u0026gt; {{ end }} 在這個範例中，我們定義了一個名為header的積木，然後在其他地方可以使用block來引用這個積木。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; {{ block \u0026#34;header\u0026#34; . }}default content{{ end }} \u0026lt;/html\u0026gt; 最後渲染出來的結果就是\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;header\u0026gt; ... \u0026lt;/header\u0026gt; \u0026lt;/html\u0026gt; 一些使用過 golang template 的人比較熟悉可能是用 {{ template \u0026quot;header\u0026quot; . }} 來插入內容，差別在於在 block 中可以定義一個預設內容，如果沒有任何地方定義header，則會使用預設內容。\nHugo Template 機制 # 渲染邏輯 # Hugo 是建築在 Golang 的 template 模組之上的，Hugo 決定了一個對於一個 m arkdown 文件，要去使用哪一些 Template 檔案，以及如何去渲染這些模板，同時也規範了一些模板結構的原則。\n在討論不同的頁面類型之前，我們先看一個最基本的範例，hugo 的所有模板都放在layouts資料夾中，其中最基礎的是 layouts/_default/baseof.html跟layouts/_default/single.html。\nlayouts/_default/baseof.html 是 hugo 渲染所有 html 頁面的時候的基礎模版 (Base Template)，這個模板定義了網頁的最基礎結構，hugo 產生 HTML 頁面就是去渲染這個基礎模板。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;{{ block \u0026#34;title\u0026#34; . }} \u0026lt;!-- Blocks may include default content. --\u0026gt; {{ .Site.Title }} {{ end }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Code that all your templates share, like a header --\u0026gt; {{ block \u0026#34;main\u0026#34; . }} \u0026lt;!-- The part of the page that begins to differ between templates --\u0026gt; {{ end }} {{ block \u0026#34;footer\u0026#34; . }} \u0026lt;!-- More shared code, perhaps a footer but that can be overridden if need be in --\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 比如說，在 hugo 文件中的這個範例，定義了一個最基本框架。 .Site.Title等變數是在渲染時 hugo 傳入的資訊。\n同時這個模板填入了 main 和 footer 這兩個 block。Hugo 會按照特定的規則、載入去尋找可能定義這些積木的檔案，搜尋的規則我們稍後在討論。對於文章頁面，hugo 會載入 layouts/_default/single.html 這個檔案。\n{{ define \u0026#34;main\u0026#34; }} \u0026lt;article\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/article\u0026gt; {{ end }} 因此，我們可以在 layouts/_default/single.html 中定義 main 這個 block。可以看到這邊填入了Content這個變數，這個變數是 hugo 將 markdown 轉換成的 html 內容，這邊我們將其帶入到\u0026lt;article\u0026gt;標籤中，就可以顯示文章的內容，當然 markdown 文件具體要怎麼被渲染成 html，也是可以控制的，這個我們後面再談。\n這樣一來，當我們打開一篇文章的網址時，hugo 就可以完成 html，並看到 markdown 文件的內容了。\nMarkdown 文件屬性 # 現在我們知道了 hugo 是如何去渲染一個頁面的，但是一個部落格文章不只是由文章頁面所組成，還包含了首頁、分類頁、標籤頁等等，因此我們需要由不同的檔案去定義對應的 \u0026ldquo;main\u0026rdquo; block，然後由 hugo 根據不同的頁面類型去載入不同的檔案。\nHugo 部落格的內容由 content 目錄下的 markdown 文件所組成，對於每個 markdown 文件，都會有各自的 Kind，Type，Layout 三個屬性去共同決定了，這個 markdown 文件要載入哪一些模板檔案，完成渲染。\nKind # Kind 是指幾個 hugo 定義的特定類型 home, page, section, taxonomy, or term。\nhome 代表的是首頁，content/_index.md 就會被設定成 Kind: home。 page 就是基本的獨立頁面和文章頁面，比如說 content/about.md 就會被設定成 Kind: page。 section 代表的是一個列表頁，一般子目錄中的 _index.md 會被設定成 Kind: section，比如說 content/posts/_index.md。 taxonomy 代表的是一個分類列表頁，在常見的部落格框架中，我們會使用 tag 或 category 來分類文章，這些分類的列表頁就會被設定成 Kind: taxonomy，比如說 content/tags/_index.md。 term 代表的是一個分類頁，比如說有一個 tag 是 python，那 content/tags/hugo.md 就會被設定成 Kind: term。 Type # type 是對文章的一種分類方式，hugo 會預設使用 content 下，頂層目錄的名稱作為 type，不過也可以使用 front matter 中的 type 來指定。最常見的 content/posts 目錄下的文章會被設置為 Type: posts，而直接放在 content 目錄下的文章會被設置為 Type: page，如 content/about.md。\nLayout # layout 則是專門用來控式頁面樣式的變數，預設情況下 layout 是空的，要透過 front matter 來指定。\n模板載入規則 # 一個 markdown 文件的 Kind, Type, Layout 這三個屬性共同決定了 hugo 要載入哪一些模板檔案。\n在 hugo 的文件中有完整的列舉範例說明，這邊我們只取一個來當作範例說明。\n假設我們有一篇文章的頁面 content/cool/introduce-python.md並透過 front mater 指定了 layout: demolayout，那麼 hugo 會依序尋找以下檔案：\nlayouts/cool/demolayout.html.html layouts/cool/single.html.html layouts/cool/demolayout.html layouts/cool/single.html layouts/_default/demolayout.html.html layouts/_default/single.html.html layouts/_default/demolayout.html layouts/_default/single.html 因為 Kind 是 page，所以會搜尋 single.html。其他的 Kind 會有不同的搜尋順續，在這邊不一一列舉。\n另外對於基礎模板，hugo 也會按照一定的規則去尋找：\nlayouts/cool/single-baseof.html.html layouts/cool/baseof.html.html layouts/cool/single-baseof.html layouts/cool/baseof.html layouts/_default/single-baseof.html.html layouts/_default/baseof.html.html layouts/_default/single-baseof.html layouts/_default/baseof.html hugo 會載入第一個找到的模板檔案，並利用第一個找到的基礎模板去渲染，要特別注意只有第一個找到的模板檔案和基礎模板檔案會被載入。\nPartials # 如同上面所說，hugo 只會載入第一個找到的模板檔案，但是這樣我們就沒辦法在不同的 layout 和頁面類型中去共用一些模板片段，這時候就可以使用 partials 來解決這個問題。\npartials 是放在 layouts/partials 目錄下的模板片段，這些片段不需要使用 define 包裝，而是直接整個檔案代表一個模板片段。\n我們可以在任何地方使用 partial 來引用這些模板片段。\n{{ partial \u0026#34;header.html\u0026#34; . }} 這樣一來，我們就可以在不同的模板中共用一些片段了。\n主題覆蓋覆蓋 # hugo 的主題，就是透過定義不同的 layouts 檔案來完成主題的開發。藉由 hugo 的特性我們可以在不需要修改主題原始檔案的情況下，去覆蓋主題的樣式、結構，甚至是功能。\n我們可以透過在layouts目錄下定義與 themes/\u0026lt;theme\u0026gt;/layouts 同名的檔案來蓋掉主題的樣式，這樣一來，hugo 就會使用我們定義的檔案來渲染頁面，我們也不需要直接去修改主題。\n這樣的機制帶來了兩個好處。首先，我們可以從layouts目錄得知明確知道我們修改了哪些檔案，而不是去翻遍整個主題目錄。其次，我們可以保持 theme 目錄不修改，這樣一來，我們就可以很方便的更新主題，也不用擔心我們的修改會被覆蓋。\n其他特殊頁面 # 除了上述的 home 以外，其實還有 RSS、404、sitemap 等特殊頁面，這些頁面的模板檔案也是可以自定義的，只要在 layouts 目錄下定義對應的檔案即可。\nMarkdown 處理 # Render Hooks # 前面，我們講述了 hugo 怎麼透過模板機制建立整個網頁的框架，接下來我會說明一下那 markdown 文件的內容是怎麼被轉換成 html 的，並且我們要怎麼去修改這個轉換過程。\nhugo 會解析 markdown 文件的內容，並解析成一個個的元素。\n[Hugo](https://gohugo.io) ![kitten](kitten.jpg) 比如說這個 markdown 文件，hugo 會解析成一個連結元素，一個圖片元素等等，接著對於每個元素，hugo 就同樣可以透過模板去渲染。\n對於每個元素類型應該怎麼去渲染，在 hugo 中稱之為 render hooks，我們同樣可以透過自定義的模板去覆蓋掉預設的 render hooks，來修改元素的渲染方式。\nrender hooks 放置在 layouts/_default/_markuip/render-\u0026lt;element\u0026gt;.html 中，比如說對於連結元素，我們可以在 layouts/_default/_markup/render-link.html 中定義如何去渲染連結元素。\n對於每個元素 hugo 會傳入什麼參數，可以參考 hugo 的文件。\n\u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34; {{- with .Title }} title=\u0026#34;{{ . }}\u0026#34;{{ end -}} \u0026gt; {{- with .Text }}{{ . }}{{ end -}} \u0026lt;/a\u0026gt; 這是一個渲染連結元素的範例。\nShortcodes # Shortcodes 則是讓我們可以額外定義，可以在 mardown 文件中可以使用的自定義元素。\n假設建立了一個 layouts/shortcodes/audio.html\n{{ with resources.Get (.Get \u0026#34;src\u0026#34;) }} \u0026lt;audio controls preload=\u0026#34;auto\u0026#34; src=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;\u0026lt;/audio\u0026gt; {{ end }} 那們我們就可以在 markdown 文件中使用 {{ \u0026lt; audio src=\u0026quot;audio/test.mp3\u0026quot; \u0026gt;}} 來插入一個音樂播放器。\nhugo 會把它渲染成\u0026lt;audio controls preload=\u0026quot;auto\u0026quot; src=\u0026quot;/audio/test.mp3\u0026quot;\u0026gt;\u0026lt;/audio\u0026gt;。\n結論 # Hugo 的模板機制非常強大，它讓我們可以很方便的去定義網頁的結構、樣式，並且可以很方便的去擴展主題的功能。但是需要去熟悉一下 golang 的 Template 模組，還有 hugo 的執行規則，才能夠更好的去開發我們的部落格。\n","date":"January 19 2025","externalUrl":null,"permalink":"/posts/hugo-template-mechanism/","section":"文章","summary":"","title":"深入了解 Hugo 的 Template 機制：結構與渲染規則","type":"posts"},{"content":"","date":"January 18 2025","externalUrl":null,"permalink":"/tags/s3/","section":"標籤","summary":"","title":"S3","type":"tags"},{"content":" 介紹 # S3 是 AWS 提供的物件儲存服務，簡單來說也就是一個檔案儲存服務。 這篇文章將會讓你了解如何使用 AWS 的 Python SDK boto3 操作 S3 API，實現檔案上傳功能，並深入探討 S3 提供的 Multipart Upload 機制，包括其概念、優勢與使用方式。\nS3 的儲存結構主要由 Bucket 和 Object 組成：\nBucket：儲存空間的管理單位，類似硬碟，可設定存取權限與儲存策略。 Object：實際儲存的檔案，每個 Object 由一個唯一的 Key 標識，本質上類似於檔案路徑，例如 folder1/folder2/file.txt。 開始之前，先確保已安裝 boto3，這是操作 AWS API 的 Python 工具：\npip install boto3 在 boto3 中，可以使用高階（high-level）和低階（low-level）API。\n低階 API：每個 function call 直接對應 AWS 服務的 HTTP API，提供更細緻的操作控制。 高階 API：在低階 API 基礎上封裝，使用更簡潔的介面完成常見任務。 使用高階 API 上傳檔案 # 以下範例展示如何利用 boto3 API 將檔案上傳到 S3。\n首先，我們需要建立一個 S3 的客戶端（client），並主要設定 endpoint 和 access key/secret key。\nimport boto3 from boto3.s3.transfer import TransferConfig, KB import os endpoint = os.getenv(\u0026#39;ENDPOINT\u0026#39;) bucket_name = os.getenv(\u0026#39;BUCKET_NAME\u0026#39;) access_key_id = os.getenv(\u0026#39;ACCESS_KEY_ID\u0026#39;) secret_key = os.getenv(\u0026#39;SECRET_KEY\u0026#39;) # Create a session with the specified credentials session = boto3.session.Session( aws_access_key_id=access_key_id, aws_secret_access_key=secret_key ) # Create an S3 client with the specified endpoint s3_client = session.client(\u0026#39;s3\u0026#39;, endpoint_url=endpoint) Endpoint 是指向 S3 伺服器的地址。由於 S3 API 已成為物件儲存的廣泛標準，不僅 AWS 提供的 S3 服務使用此 API，許多其他雲端儲存解決方案也提供兼容的 S3 API 服務。這意味著，我們可以透過設定 endpoint 來指定目標伺服器的位置，進而使用 boto3 操作這些兼容的服務。\nconfig = TransferConfig( multipart_threshold=KB * 25, max_concurrency=10, multipart_chunksize=1024 * 25, use_threads=True) objectKey = \u0026#34;file.txt\u0026#34; # Perform the multipart upload with open(\u0026#34;file.txt\u0026#34;, \u0026#39;rb\u0026#39;) as file: s3_client.upload_fileobj(file, bucket_name, objectKey, Config=config) 使用 High-Level API 上傳檔案非常簡單，只需一行程式碼即可完成。我們只需提供 Bucket 名稱 和 Object Key，即可將檔案順利上傳至 S3。\nMultipart Upload # 在這裡，我們額外套用了 transfer config，以啟用 Multipart Upload 機制。\nMultipart Upload 是 S3 API 提供的一種上傳機制，簡單來說，它將一個大檔案分割成多個小區塊（parts），並分段上傳。\n根據 AWS 文件的說明，這種機制具有以下優勢：\n提升效率：透過將每個 part 平行化上傳，顯著提高大檔案的上傳速度。 提高可靠性：避免因網路或伺服器問題導致整個檔案上傳失敗，因為每個區塊可以獨立重傳。 即時處理：當應用中的使用者上傳大檔案時，可直接處理接收到的分段檔案，而無需等待整個檔案完成上傳。 Multipart Upload 的操作流程 # 從 S3 API 的角度來看，使用 Multipart Upload 進行檔案上傳，需要以下三個步驟：\n建立上傳任務：使用 CreateMultipartUpload 指定上傳路徑（Object Key），建立上傳任務。此時，S3 會回傳一個唯一的 uploadId，用於識別該上傳任務。\n分段上傳：程式將檔案分割為多個小區塊，並使用 UploadPart 上傳每個區塊：\n每個區塊有一個唯一的 partNumber，由程式決定。 partNumber 不需要連續，但最終必須按順序排列。 各區塊可平行上傳，無需等待前一個區塊完成。 完成上傳：最後，呼叫 CompleteMultipartUpload，將 S3 會將所有區塊合併成完整檔案，並完成上傳。\n中斷與清理機制 # 若上傳任務中斷或失敗，部分已上傳的區塊會被保存在 Object Storage 中，這些稱為 incomplete multipart uploads。它們會占用存儲空間，因此需定期清理：\n我們可以主動呼叫 AbortMultipartUpload 取消上傳任務並刪除無效區塊。 AWS 也提供 Lifecycle Policies 來設定定時清除 incomplete multipart uploads。\n當然，boto3 的高階 API 已經將整個 multipart upload 的過程封裝起來，使用者不需要自己去管理 multipart upload 的過程，只需要把檔案傳給 boto3 並設定好 transfer config，他就會自動透過 multithreading 來完成多個區塊的上傳。\n透過低階 API 實現 Multipart Upload # 當然，我們也可以透過 boto3 提供的低階 API 來，自行完成分片跟上傳。\n在低階 API 中，每個 S3 API 請求對應到一個 boto3 的 function call。\nresponse = s3_client.create_multipart_upload(Bucket=bucket_name, Key=objectKey) upload_id = response[\u0026#39;UploadId\u0026#39;] 首先，我們使用 create_multipart_upload 建立一個上傳任務，並取得對應的 uploadId。\nparts = [] with open(filename, \u0026#39;rb\u0026#39;) as file: for i in range(1, num_parts + 1): part_data = file.read(part_size) response = s3_client.upload_part( Bucket=bucket_name, Key=objectKey, PartNumber=i, UploadId=upload_id, Body=part_data ) parts.append({\u0026#39;PartNumber\u0026#39;: i, \u0026#39;ETag\u0026#39;: response[\u0026#39;ETag\u0026#39;]}) 接著我們讀取要上船的檔案，並將檔案分割成多個區塊，透過 upload_part 逐一上傳。\nresponse = s3_client.complete_multipart_upload( Bucket=bucket_name, Key=objectKey, UploadId=upload_id, MultipartUpload={\u0026#39;Parts\u0026#39;: parts} ) 最後，我們使用 complete_multipart_upload 完成上傳，我們可以同時帶上每個 Part 上傳後的 ETag，以確保所有區塊都已成功上傳，在MultipartUpload={'Parts': parts}中我們還可以帶上每個 Part 的 Checksum 或著提供整個檔案的 Checksum，以確保檔案的完整性。\n如果中途上船失敗了，我們可以呼叫 s3 的 AbortMultipartUpload 來取消上傳任務，並刪除已經上傳的區塊。\n# Abort the multipart upload response = s3_client.abort_multipart_upload( Bucket=bucket_name, Key=object_key, UploadId=upload_id ) 總結 # 今天我們介紹了 S3 的 python SDK boto3，並且深入探討了 S3 的 Multipart Upload 機制，透過 Multipart Upload，我們可以更有效率地上傳大檔案，提高可靠性，並且即時處理檔案。 當然更加詳細的使用說明，可以參考 AWS S3 和 boto3 的文件。\n參考資料 # boto3 AWS S3 API ","date":"January 18 2025","externalUrl":null,"permalink":"/posts/aws-s3-python-boto3-tutorial-with-multipart-upload/","section":"文章","summary":"","title":"用 Python boto3 操作 AWS S3：檔案上傳與 Multipart Upload 機制","type":"posts"},{"content":"從完成口試到完成畢業離校是一個很長、很複雜的流程，因此我將我畢業經歷的流程記錄下來，希望能夠幫助到交大資工所的各位。以下是我在 2024 年畢業的流程及心得，希望能讓未來的同學對畢業流程有個清晰的概念。\n不過要特別注意，不同系所的規範可能會有所不同，同實驗室但是掛在不同學院下也會差很多，此外規定也可能隨時間調整，因此這份是 2024 資科工碩甲組的畢業流程，僅供參考。\n主要流程是根據交大資工網站的論文口試須知暨畢業離校說明文件，這篇文章主要是針對一些操作細節補充說明，所有需要訪問的系統連結和文件都有在交大資工網站跟說明文件中。\n口試前準備 # 性別教育與學術倫理 # 除了完成系所的修課要求外，學術倫理和性別平等教育課程建議碩一有空就盡早完成。特別是學術倫理課程，內容較多，還是需要花些時間。建議不要拖到畢業前，避免畢業時間流程緊迫還要處理這種東西。完成後，可在學籍成績管理系統的成績管理中確認有兩個「已通過」的標示。\n畢業時間規劃：學雜費退費 # 此外，為了大家的錢包找想，建議留意一下學費退費的時段。學校以學期三分之一和三分之二為節點可退部分學費，因此如果有機會的話，盡量把畢業離校的時間在退費截止前。具體時間看學校行事曆。\n口試前三個月：提交碩士論文計畫書 # 第一步是口試三個月前繳交碩士論文計畫書。計畫書內容包括論文名稱，還有一個區域填寫論文背景說明、文獻綜覽、研究方法、實驗設計和時程安排等資訊。理想上，這時候應該已經有了論文的基本架構，這份文件無字數限制，也不會影響到後面口試及最後繳交的論文，填上現有的內容即可。另外，計劃書需要**教授簽名。**基本上，與論文相關的文件可用電子簽名的方式線上簽好，再印下來繳交，而有些文件也可用 email 寄給系辦。\n如果沒有在三個月前繳交，助理就會在你申請口試後，寄信叫你去系辦做三小時的義務工讀服務。工讀需連續三小時，需要在上午九點前或下午一點後開始，不然會撞到系辦午休時間。\n口試前一個月：確認口試細節 # 口試前一個月非常重要。依資工所規定，需於口試21 天前提交口試申請。申請內容包括確定口試委員、口試時間與地點、論文中英文題目、論文初稿抄襲檢測結果等。因此大概一個多月前就要稍微留意一下預計的口試時間和進度安排，不然會來不及申請口試。\n口試申請系統 # 資工系申請口試的方式比較特殊，需透過系計中維護的碩士班畢業論文口試申請系統完成，該系統需使用資工系計中帳號登入。因此，需要提前先完成帳號註冊。如果是交大資工系畢業的同學可以申請沿用舊有大學帳號，不過沿用也要申請，所以還是建議早點處理。\n口試委員名單與聯絡 # 口試委員通常是是 3~5 位，其中一位需要是院外委員，而指導教授也可以擔任口委。建議是詢問實驗室學長姐以往口委是怎麼決定的，一般來說會跟指導教授討論確認，然後透過 email 邀請口試委員。\n另外，需指定一位口委擔任召集人，召集人的工作的是在口試當天主持口試流程。指導教授不能擔任召集人。因此，建議是跟指導教授同時確認口委名單和召集人，然後透過 Email 邀請教授擔任口試委員及召集人。\n確認教授們是否方便擔任口委的同時，也需要確認一些事項，建議是可以開一個 Google 表單或著一個表格文件，附在 Email 內去詢問口委們。\n除了要確認教授是否擔任口委外，還要調查口試時間及口委的交通方式。建議是跟指導教授確認，抓出大概一到兩個星期的區間，然後調查各個口委可以的時段。一般來說口試時間是一個小時，因此調查時，可以以每個小時為區間個來調查。完成調查後，確認一個最終的口試時間，然後通知各個口委。很重要的是最終時間也要再次跟指導教授確認教授是否有空。\n交通方式則是用於系所報帳。\n慘痛的教訓是，如果是用學校信箱帳號建立 Google 表單，要小心預設情況下只有交大信箱的帳號可以回復，要記得把限制關掉，不然口委來說不能填會很尷尬。\n口委可能比較忙或信多沒看到，如果一直沒回的話，可以再寄一封，標題寫【再次來信】XXX，當然如果還是沒回或很急迫，就可能詢問指導教授有沒有聯繫方式，或著透過其他管道聯繫口委。\n論文原創性比對 # 提交口試申請時，還需進行論文初稿的原創性比對。將初稿上傳至 E3 系統後，獲得比對結果並填寫於口試申請系統中。初稿交出去後，論文還是可以繼續修改，這個初稿只用於產出這個比對的結果，後續不影響。如果使用中文檔名，可能會出現比對執行失敗的狀況，建議用英文檔名。\n口試時間及地點 # 口試地點是借工程三館的研討室，原則上是到研討室預約系統去借，但是預約系統只會開放當月的借用，所以如果你要借下個月的時段，必須到 EC341 去人工登記或者打電話，建議是前後都多借一些時間，以免其他實驗室需要使用，方便提早進去準備，也避免擔心口試時間延誤。\n論文題目 # 特別說明一下論文題目，首先口試當天需要用到的、給口委簽名的書面文件，系辦助理都會幫你印好。其中包含碩士學位論文審定同意書，上面會有中英文論文題目。如果申請口試後有要修改題目，就需要麻煩系辦修改審定書文件。如果是在口試後，就需要重簽審定書或著印修改紙條貼上去。\n這些都調查好之後，就可以在口試申請系統上面填寫，完成口試申請，然後把印出來的文件給教授簽名，然後送給系辦。\n口試前 # 大約口試前一周左右記得寄信提醒口試委員口試時間、地點，然後也提供一下論文電子檔給口委們參考。另外，如果有要準備茶點或午餐，要順便調查、訂餐。\n建議口試當天提供 PPT 紙本給口委參考和筆記用，可以問問實驗室有沒有這個慣例。有的話，口試前一天記得先印 PPT 紙本。\n口試當天就先去系辦拿文件，系辦助理會說明甚麼東西要簽名。口委來的時候就先把文件交給口委。口試後確認有簽名，然後交回給系辦，系辦助理會複印審定同意書，然後把原版還給你。\n口試後畢業離校 # 到這邊，先恭喜各位口試結束，接下來就可以準備畢業了。\n首先是幾項畢業前的前置動作。\n修訂論文與圖書館上傳 # 根據口試委員與教授建議修訂論文後，將最終版上傳至**圖書館論文上傳系統。**這個系統同樣有很多資料要填寫。這邊會再做一次原創性比對，不過是直接在上傳系統完成，不用到 E3。\n接下來，有幾個文件需要簽名，學術倫理暨原創性比對聲明書只要印出來自己簽名就好了。學位論文發表形式確認書要與教授確認是否選擇**「論文採用著作彙編形式」。我的理解是之前發了好幾篇論文，要作為畢業論文的基礎，要避免自我抄襲還有其他問題，這個跟指導教授確認，應該沒有發過東西的話就不採用。不過不採用也還是要提交學位論文發表形式確認書，這張要教授和系辦簽名蓋章**。\n如果要採用著作彙編形式，則需另外填寫**「著作彙編之學位論文資訊及彙編學術著作之共同作者貢獻聲明書」，如果 paper 還未發表但已被接受，仍要填寫。貢獻度說明和百分比可和指導教授（通訊作者）討論，是否納入下述共同作者的學位論文那欄基本上除了第一作者為是外，其他為否，但詳細情況還是與指導教授討論結果為主。簽名可以是數位簽章、電子簽章、電子簽名等形式，而填寫完成後，要給教授和系辦簽名蓋章**。如果有學術倫理相關問題或不會填寫的文件，可打電話找「學術倫理與研究誠信辦公室」詢問。\n接著是書目資料建檔，就是把你論文的資料填進去，就把論文內對應區塊的文字直接複製貼上過來即可，另外畢業去向記得寫。\n第三個是論文公開授權的設定，分成電子版和紙本兩個部分，一樣跟學長姐和教授確認形式。這邊也會有論文電子檔著作權授權書和延後公開申請書兩個文件，前者自己簽名就好，後者是有不公開或延期公開才會需要填寫，一樣要教授和系辦簽名蓋章。\n雖然系統上面寫第一步第二步第三步，但是其實可以同時進行，所以記得把發表形式確認書和延後公開申請書一起給教授和系辦簽名蓋章，另外這些文件下載下來會有中英兩個版本，我們簽中文的就好了。\n文件都簽名完成後都掃描上傳到系統，最後就會先後提交給系辦和教授去確認，有問題系辦會 Email 告訴你哪裡要修改，如果教授沒看到要提醒他收信按確認。\n紙本論文製作 # 完成論文上傳後，就可以印製紙本論文了。把論文本文、電子檔著作權授權書、學位論文審定同意書、學位論文發表形式確認書、延後公開申請書（有的話）、著作彙編之學位論文資訊及彙編學術著作之共同作者貢獻聲明書（有的話）等檔案都放到一個 USB 裡面（不用合併檔案）。到工程三館一樓影印室印製，或是前一天寄檔案 email 過去，隔天直接去領，而論文最少是兩本要交給學校，額外就看實驗室和自己要不要留一本。印製約需半小時至一小時，部分彩色頁面一本費用約為 NT$200-250。印出來後，要等到離校程序啟動後才繳交給學校。\n成績提交 # 接著要去把離系單印出來，然後給教授和系辦簽名蓋章，系辦才會將口試成績提交給註冊組。可以跟論文上傳相關的文件一起拿給教授和系辦簽名。\n論文海報 PPT # 另外還有一個論文海報 PPT 要記得做好，並寄送給系辦助理。\n啟動離校流程 # 完成上述步驟後，即可啟動線上離校流程。需滿足以下條件才能啟動系統：\n圖書館論文上傳並經過系辦與教授審核（隔天） 成績送達註冊組且已登記（根據系所文件說明，至少等半天。） 沒有未完成的差勤登記項目 如果實驗室有報計畫和工作費或著在學校有打工，那離校程序，要等所有差勤工作項目的登記時間過後，才能夠啟動。理論上，可以請助理協調修改簽核時間，但是我畢業時，剛好在更新差勤系統，規則有變，所以如果還有差勤工作項目的話，記得提早詢問助理，確認是否可以調整。 可以前一天晚上啟動線上離校成續，然後照著學校的流程走去跑校園 RPG，理論上可以一天內完成拿到畢業證書離開學校。記得要帶兩本論文、學生證和退選單。畢業流程分成離系和離校兩個部分。\n離系的部分比較簡單，要拿離系單分別給資工系計中、實驗室助教簽名，最後交回系辦 (記得先把海報 PPT 寄過去)。同時，因為我們需要退掉個別研究，所以可以順便把退選單拿給系辦蓋章。\n因為離系單要先給系辦簽名才能送成績到註冊組啟動離校，所以可能可以在送成績時順便完成離系和退選單，不一定要跟離校在同一天內弄。\n接下來就是離校流程，這邊提供一個建議的 RPG 移動路線：\n圖書館：繳交論文紙本 資訊服務中心三樓職涯發展組：有一個線上表單要從線上離校系統的介面先填，去這邊領畢業證書的書套和紀念品 資訊服務中心三樓住宿服務組：如果有住宿舍的同學才需要來處理退宿，我沒有所以不太清楚流程 科學一館課務組：完成個別研究退選，如果是在寒暑假期間則不用。 科學一館註冊組：領畢業證書、繳交另外一本論文。到這邊就正式完成畢業啦！ 大禮堂二樓出納組：如果有要退學雜費，就需要再來這邊一趟。 到這邊就正式畢業離開交大了！\n","date":"November 16 2024","externalUrl":null,"permalink":"/posts/graduation-process-tips-for-nycu-cs-masters/","section":"文章","summary":"","title":"交大資工研究所畢業流程心得","type":"posts"},{"content":"","date":"November 16 2024","externalUrl":null,"permalink":"/categories/%E7%A0%94%E7%A9%B6%E6%89%80/","section":"分類","summary":"","title":"研究所","type":"categories"},{"content":" 前言 # 部落格廢棄了一年多的時間，終於有空來好好整理整理我的部落格了。原本是使用 VuePress 來產生我的部落格，但是在這次整理的過程中發現了 VuePress 的一些缺點，想了想，研究了一下現在幾款的 Markdown 靜態網站生成框架，最後決定從 VuePress 搬遷到 Hugo 並使用了 Blowfish 這個主題。\n起因 # 大概在兩年前，開始使用 Markdown 生成器來建立靜態部落格。最早有短暫使用 Hexo。後來就接觸到 VuePress 這個框架，發現 Vuepress 能夠直接在頁面裡面寫 Vue.js 來擴充功能，未來能夠比較方便在部落格裡面加一堆功能，所以就跳到 VuePress 了。\n但是一直以來這個部落格就有 SEO 的問題，很多頁面沒有辦法被 Google Index。所以這次嘗試要去解決 SEO 相關的問題，為了解決這些問題，需要去修改 VuePress 的 Theme。但是就發現修改 VuePress 的 theme 是一件有點痛苦的事情。因為相對於 Hexo 這種純粹生成靜態網頁的工具，VuePress 使用的是直接建立一個基於 Vue.js 前端框架的網頁，然後每篇文章使用 Server Side Rendering 的方式去產生靜態內容，並且 Theme 也是直接以 Vue module 的方式插入的。這導致 VuePress 框架本身、theme 還有頁面的程式碼全部都會混在一起，導致開發上會有點難除錯。\n為了省心，就決定準備從 VuePress 搬遷回 Hexo 這種單純模板式的靜態網頁生成器。在找新主題的過程中就發現了 Hugo 這個基於 Golang 開發的靜態網頁生器，並被它的幾個特性給吸引。\n超快的編譯速度：Hugo 是一個使用 Golang 開發的生成器，Hugo 的執行速度非常之快，以我目前 19 篇文章來說，hugo 編譯的時間是 100 ms，基本上是可以順開。原本在 VuePress 就要好幾秒，超級久。 Golang 模板：每個框架都提供了模板引擎，來提供對網頁主題及內容客製化的功能。Hexo 使用 ejs 模板引擎來渲染，VuePress 基於前端框架的特性，使用 Vue.js 的程式碼來建立模板。Hugo 則是使用 Golang 本身提供的模板引擎。相較於從來沒用過的 ejs，我對 Golang 的模板引擎比較熟悉。 容易擴展主題：靜態部落格相對來說不是一個很複雜的網站，所以相對於 VuePress 直接提供前端框架的擴充彈性，擴充的容易度對目前的我來說比較重要。Hugo 可以在不修改主題檔案的情況下，很容易地透過額外的檔案來覆蓋主題，修改主題行為和增加功能。 省心的內建功能：Hugo 本身就提供了對 Google Analytics 和 Open Graph 等在 Hexo 和 VuePress 都需要額外模組才能提供的功能。相對來說非常省心。 比較穩定的編譯結果：這個是 VuePress 修改主題的時候的一個小缺點，因為 VuePress 的所有主題和插件本質上都是 JS 程式碼，最後會被 VuePress 編譯成一些 js bundle，通常會是 assets/js/23.501c2967.js 這樣的檔案，並且會經過 js uglify 跟 minify。重新編譯後整個檔案就可能亂掉，對於用 Github Page 來管理部落格的我來說，不是很喜歡。 主題選擇 # 跟大部分的靜態部落格生成器一樣，Hugo 也有廣大的開元社群提供了各式各樣的主題，除了從 hugo 官網，以外也從 Github 和 Google 去找了幾個感興趣主題。這邊也順便把這幾個分享給大家。\nToha: Toha 是我找到第一個感興趣的主題，他的首頁很好看，也提供了一些如 Skills、Code Notes 之類我感興趣的功能。可惜的是他只提供了基於 Tag 的分類方式，不支援 Category，所以與我原有的部落格不太相容，文章頁面也設計沒那麼吸引我。 blog.coolapso.sh: 這個作者最酷的地方是提供了一個假 shell 的形式的履歷網頁。 hexo-theme-next: 本來是打算從 VuePress 搬到 Hexo + Next，所以就找到這個把 Hexo Next 主題遷移到 Hugo 的版本。 hexo-theme-aomori: 這個是 Hexo 的主題但是很好看。 archie: 經典的極簡主題。 hugo-theme-tailwind: 基於 tailwind CSS 的主題。想選這個是考慮到如果要修改主題、增加頁面，有 tailwind CSS 可能會比較好改。 hugo-theme-bootstrap: 另外一個基於 BootStrap。 最後，我選擇的是 Blowfish這個主題，blowfish 很好看，而且功能完善，基本上可以滿足我所有功能需求，所以後期要額外加的功能也比較少，不用花太多時間。\n專案初始化 # 安裝 Hugo # 因為 Hugo 是基於 Golang 開發的，比較好的安裝方式是安裝 Golang 來編譯最新版本。\nsudo su wget https://go.dev/dl/go1.23.2.linux-amd64.tar.gz rm -rf /usr/local/go \u0026amp;\u0026amp; tar -C /usr/local -xzf go1.23.2.linux-amd64.tar.gz # Add to .bashrc/.zshrc export PATH=$PATH:/usr/local/go/bin 安裝完 Golang 後就可以安裝 hugo。\ngo install -tags extended github.com/gohugoio/hugo@latest # 如果用 non root 安裝會在 $HOME/go/bin/ # 也需要加到環境變數 # export PATH=$PATH:$HOME/go/bin/ 建立部落格及安裝主題 # 接著初始化 hugo 專案。\nhugo new site \u0026lt;folder\u0026gt; cd \u0026lt;folder\u0026gt; 在 hugo 架構下，主題要放在 themes 目錄下，所以要將主題 clone 到 themes/blowfish。\ngit init git submodule add -- https://github.com/nunocoracao/blowfish.git themes/blowfish 接著要將主題提供的設定檔複製到部落格設定檔目錄之下，也就是config/_default。\nrm hugo.toml mkdir -p config/_default/ cp themes/blowfish/config/_default/* config/_default/ 接著要修改 config/_default/hugo.toml，把theme的註解#拿掉，主題才能生效。\n# theme = \u0026#34;blowfish\u0026#34; # UNCOMMENT THIS LINE 然後啟動 hugo的測試伺服器。\nhugo server 就能訪問到空白的主題網頁了！\n設定與搬遷 # 接著就要把網站變成我們想要的樣子，然後把文章搬過來了。修改設定的時候，可以把網站用 hugo server 跑起來，hugo 偵測到檔案變更會自己重編譯，而且因為 Hugo 編譯可很快，可以即時看到修改結果。\n設定主題 # 網頁的自訂化主要還是由主題提供的設定，前面已經把主題的設定檔複製到 config/_default 目錄下，接下來就只要一個接著一個修改。\n# ls config/_default/ hugo.toml languages.en.toml markup.toml menus.en.toml module.toml params.toml 因為 blowfish 支援多語系網站，所以在設定部分 languages 跟 memus 這兩個檔案是語系獨立的，因為我要主要是繁體中文。所以把原有的英文語系設定檔案刪掉，改成中文。\n# ls config/_default/ hugo.toml languages.zh-tw.toml markup.toml menus.zh-tw.toml module.toml params.toml 接下來就是打開每個設定檔，參考主題的說明文件，把網站調整成自己想要的樣子。\nGetting Started · Blowfish Configuration · Blowfish 因為大部分的設定內容都只需要看著設定檔和說明文件設定，這邊就不贅述。\n文章搬遷 # 文章搬遷反而是整個過程中最簡單的步驟，Hugo 完全兼容我文章原有的 front matter。所以只要把文章複製到 content/posts 目錄下，就可以完成搬遷了。\n主題修改 # 調整程式碼區塊配色 # 整體來說，原本 Blowfish 主題的配色就讓我很滿意了，我唯一覺得有問題的地方是程式碼區塊的配色，Blowfish 客製化的語法高量主題好看，但是就不夠清楚，不太好閱讀。\nHugo 利用 chroma 這個工具來產生程式碼區塊，Blowfish 做的事情是透過 CSS 去調整不同關鍵字的配色和區塊底色。因此，可以透過調整 CSS 來修改整個配色。Chroma 內建了許多的配色主題，我挑了其中的 onedark 這個主題。\n可以直接只用 hugo 指令生成主題的 CSS 設定。\nmkdir -p assets/css/ hugo gen hugo gen chromastyles --style=onedark \u0026gt; assets/css/custom.css 跟據 Blowfish 文件 ，寫入到 assets/css/custom.css的 css 因為檔案順序的關係，優先度比較高，可以直接覆蓋掉主題的 CSS 設定，這樣就不需要直接修改主題的檔案。\n不過這樣只會修改到淺色模式下的主題，如果要對暗色模式生效，就需要複製一份整個 chroma 的 CSS，然後每個後面添加 is(.dark *) 選擇器。\n# Light mode .prose .chroma { color: #abb2bf; background-color: #232631; } # Dark mode .prose .chroma:is(.dark *) { color: #abb2bf; background-color: #232631; } 中文化 # Blowfish 本來就已經有完整的繁體中文支援，唯一沒支援到的地方是 /posts, /tags, /categories這三個目錄的標題沒有修改。\n要修改也比較簡單，建立 content/posts/_index.md, content/tags/_index.md, content/categories/_index.md 這三個檔案，然後打上標題即可。\n--- title: 標籤 --- 修改 404 頁面 # 接著要調整的是 404 頁面，這是原本的 404 頁面：\n主要是我想要將我畫的 404 圖片放進來，原本的主題沒有提供在 404 頁面置入圖片的功能，所以需要去修改模板。\n這邊就要介紹 hugo 很強大的主題覆蓋功能，當 hugo 需要搜尋一個模板時，會依序在 layouts, themes/xxxx/layouts去找到對應的檔案。所以當我們想要修改主題模板時，只要將模板複製到 layouts 中修改，就可以在完全不更動主題檔案的情況下完成修改。\ncp themes/blowfish/layouts/404.html layouts 然後就可以簡單的置入 404.png。\n{{ define \u0026#34;main\u0026#34; }} \u0026lt;h1 class=\u0026#34;mb-3 text-4xl font-extrabold\u0026#34;\u0026gt; {{ i18n \u0026#34;error.404_title\u0026#34; | emojify }} \u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;prose dark:prose-invert\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{ i18n \u0026#34;error.404_description\u0026#34; | emojify }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;mt-8 mb-12 border border-2 border-neutral-200 dark:border-neutral-700 rounded shadow-2xl\u0026#34; \u0026gt; \u0026lt;img alt=\u0026#34;404\u0026#34; src=\u0026#34;/404.png\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; {{ end }} 這個圖檔要放在 static/404.png ，hugo 在編譯時會自動把 static 目錄中的檔案搬遷到編譯結果\nLogo 大小 # 另外網站 Logo 的部分也做了調整，原本主題是將 Logo 大小設定成圖檔大小的一半，但是我覺得還是太大了。\n所以就一樣，把 Header 對應的模板拿出來然後去修改。在 hugo 的資料夾組織中，組成網頁的元素都會放在 partials 這個目錄下。\nmkdir layouts/partials/headers cp themes/blowfish/layouts/partials/headers/basic 直接改成 32X32 即可。\n\u0026lt;img src=\u0026#34;{{ $logo.RelPermalink }}\u0026#34; width=\u0026#34;32\u0026#34; height=\u0026#34;32\u0026#34; class=\u0026#34;logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom\u0026#34; alt=\u0026#34;{{ .Site.Title }}\u0026#34; /\u0026gt; Open Graph Image # 另外一個比較大的改動是 open graph image。\nOpen graph 的功能是提供社群網站顯示的資訊，比如把連結貼到 Facebook 會顯示出一張圖片跟一些簡單的描述。這些是透過特殊的 html header tag 達成的。其中 og:image 是提供 facebook 分享時顯示的大圖。\n\u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;http://localhost:1313/posts/proxmox-hard-disk-replacement-and-expansion/\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;Louis Li\u0026amp;#39;s Blog\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Proxmox硬碟更換及擴容\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;在這篇文章中，我們將探討如何在Proxmox上將舊的2TB SSD硬碟升級至4TB，並無需重新安裝系統或移動虛擬機的操作步驟和方法。\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;zh_tw\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;article\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;http://localhost:1313/og_image.png\u0026#34; /\u0026gt; 在 hugo 提供的模板中，會使用文章的第一張內嵌圖片連結作為 og:image，但是如果文章沒有圖片那就不會自動產生 og:image，圖片也限定於放在 contents/pages/文章目錄下的圖片。\n所以我希望能夠修改，讓他使用我之前設計好的圖片，當成所有沒有 og image 網頁的預設 og image。\n去看主題模板會發現，他使用 Hugo 內建的 opengraph 模板。\n# themes/blowfish/layouts/partials/head.html {{ template \u0026#34;_internal/opengraph.html\u0026#34; . }} 因此這裡修改比較麻煩，首先我們需要把這個內置的模板複製一份到 layouts/partials 目錄，這個檔案我找不到要怎麼取得，最後是直接到 hugo 的 repository 去抓。\n另外我們也需要複製一份 head.html 模板，因為要把原本渲染內建 opengraph.html 變成渲染我們改過的版本。\n# layouts/partials/head.html {{ partial \u0026#34;opengraph.html\u0026#34; . }} 最後就是修改 opengraph 模板，當模板找不到圖片的時候使用一個預設的圖片連結。\n{{- with partial \u0026#34;_funcs/get-page-images\u0026#34; . }} {{- range . | first 6 }} \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt; {{- end }} {{- else }} \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;{{ site.Params.defaultOpenGraphImage | absURL }}\u0026#34;\u0026gt; {{- end }} 然後再設定裡面加入這個圖片的連結，並把圖片放到 static 目錄。\n# config/_default/hugo.toml [params] defaultOpenGraphImage = \u0026#34;/og_image.png\u0026#34; 成果 # 到這邊我的整個 Hugo 靜態部落格就建立完成拉。\nPageSpeed Insights 行動裝置能夠拿到 97 分，真舒適。\n","date":"October 17 2024","externalUrl":null,"permalink":"/posts/from-vuepress-to-hugo/","section":"文章","summary":"","title":"從 VuePress 跑到 Hugo","type":"posts"},{"content":"","date":"August 3 2023","externalUrl":null,"permalink":"/tags/lvm/","section":"標籤","summary":"","title":"LVM","type":"tags"},{"content":"","date":"August 3 2023","externalUrl":null,"permalink":"/categories/proxmox/","section":"分類","summary":"","title":"Proxmox","type":"categories"},{"content":"在手邊的 Proxmox 上使用了一年多的時間，最初是使用一顆 2TB 的 SSD 硬碟，但隨著時間推移，硬碟的容量已經不足以應付需求。為了解決這個問題，決定購買一顆 4TB 的 SSD 來替換。然而，為了節省時間和方便起見，希望能夠直接將舊硬碟的內容遷移到新硬碟上，而不必重新安裝 Proxmox 系統，也不需要搬遷虛擬機。因此，進行了一些研究和操作來實現這個目標，這裡記錄下相關的步驟和方法。\n硬碟內容搬遷 # 首先，我們需要將原始硬碟內的資料搬移到新的硬碟上。我們可以使用 dd 命令直接將硬碟內容完全複製過去。這可以在 Proxmox 的 Shell 中執行，無需使用額外的 USB 開機碟操作。儘管這樣做可能會複製 swap 等內容，但重開機後不會影響系統運作。\n請先使用 lsblk 命令找到原始硬碟和目標硬碟：\n# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 1.7T 0 disk ├─nvme0n1p1 259:1 0 1007K 0 part ├─nvme0n1p2 259:2 0 512M 0 part /boot/efi └─nvme0n1p3 259:3 0 1.7T 0 part ├─pve-swap 253:0 0 8G 0 lvm [SWAP] ├─pve-root 253:1 0 96G 0 lvm / ├─pve-data_tmeta 253:2 0 15.8G 0 lvm │ └─pve-data-tpool 253:4 0 1.7T 0 lvm │ ├─pve-data 253:5 0 1.7T 1 lvm │ ├─pve-vm--100--disk--0 253:6 0 1T 0 lvm .... nvme1n1 259:4 0 3.6T 0 disk 從上述輸出中，我們可以很容易地辨識出 nvme0n1 為原始硬碟，nvme1n1 為新的硬碟。\n接下來，使用 dd 命令進行硬碟內容的複製。為了在處理過程中顯示進度，我們加上 status=progress 選項。這樣在複製 2TB 資料時，我們可以知道進度，而不會等太久不知道是不是當掉了 (搬了超過半小時)。bs 參數代表 dd 一次搬動的資料塊大小，預設值是 512bytes，但這可能會導致速度變慢，因此我們可以選擇較大的值，例如 bs=2G。\n# dd if=/dev/nvme0n1 of=/dev/nvme1n1 status=progress bs=2G 完成後，關機並將舊的硬碟移除，用新的硬碟開機。\n調整硬碟分割區大小 # 理論上，重開機後，應該可以正常進入系統，但從 Proxmox 的 Web GUI 可能會發現存儲空間的大小沒有變化。這是因為雖然資料已經完全複製到容量更大的硬碟，但系統設定仍然只認識原本的硬碟大小。因此，接下來我們需要調整分割區和 LVM 的配置，讓 Proxmox 能夠使用新的空間。\n# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 3.6T 0 disk ├─nvme0n1p1 259:1 0 1007K 0 part ├─nvme0n1p2 259:2 0 512M 0 part /boot/efi └─nvme0n1p3 259:3 0 1.7T 0 part ├─pve-swap 253:0 0 8G 0 lvm [SWAP] ├─pve-root 253:1 0 96G 0 lvm / ├─pve-data_tmeta 253:2 0 15.8G 0 lvm │ └─pve-data-tpool 253:4 0 1.7T 0 lvm │ ├─pve-data 253:5 0 1.7T 1 lvm │ ├─pve-vm--100--disk--0 253:6 0 1T 0 lvm 透過 lsblk，可以看到硬碟的大小為 3.6T。由於 pve 的資料都放在 nvem0n1p3 這個分割區下面，所以我們要先調整分割區的大小。\n這邊我們使用 growpart 這個工具來調整分割區大小。\napt install cloud-guest-utils growpart /dev/nvme0n1 3 growpart 的第一個參數是要調整的硬碟，第二個參數是第幾個分割區。growpart 預設會將硬碟的剩餘未分配空間都分配給分割區，因此不需要指定大小。如果你擔心出問題，可以先加上 -N 選項查看分配的結果是否符合預期。\n分配完成後，再次使用 lsblk 命令來確認。現在，nvme0n1p3 分割區的大小應該已經變成 3.6T 了：\n# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 3.6T 0 disk ├─nvme0n1p1 259:1 0 1007K 0 part ├─nvme0n1p2 259:2 0 512M 0 part /boot/efi └─nvme0n1p3 259:3 0 3.6T 0 part ├─pve-swap 253:0 0 8G 0 lvm [SWAP] ├─pve-root 253:1 0 96G 0 lvm / ├─pve-data_tmeta 253:2 0 15.8G 0 lvm │ └─pve-data-tpool 253:4 0 1.7T 0 lvm │ ├─pve-data 253:5 0 1.7T 1 lvm │ ├─pve-vm--100--disk--0 253:6 0 1T 0 lvm 但是從 Proxmox 的 Web GUI，你可能會發現存儲空間大小還是沒有改變。這是因為 Proxmox 在 partition 上透過 LVM 做了一層分割，所以我們需要調整 LVM 的大小。\n調整 LVM # 首先，我們要調整硬碟分割區對應的 PV（物理卷）大小：\n# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p3 pve lvm2 a-- \u0026lt;1.74t \u0026lt;0.2t 使用 pvs 命令可以查看 PV 的資訊。\n接下來，使用 pvresize 命令將分割區剩餘的所有空間都加入到 PV 內：\npvresize /dev/nvme0n1p3 這樣就能將分割區未使用的空間加入到 PV，無需進行其他配置。\n再次執行 pvs 命令，現在你應該會看到 PV 的大小已經增加了：\n# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p3 pve lvm2 a-- \u0026lt;3.64t \u0026lt;1.84t 最後，我們需要調整 storage 對應的 LV（邏輯卷）的大小。首先，使用 lvs 命令來查看 LV 的資訊\n# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert data pve twi-aotz-- 1.67t 73.80 3.82 root pve -wi-ao---- 96.00g swap pve -wi-ao---- 8.00g vm-100-disk-0 pve Vwi-aotz-- 1.00t data 57.73 vm-100-disk-1 pve Vwi-aotz-- 4.00m data 14.06 vm-101-disk-0 pve Vwi-a-tz-- 100.00g data 71.78 vm-102-disk-0 pve Vwi-aotz-- 4.00m data 14.06 ... 從上述輸出中，可以看到所有 VM 的硬碟都掛載在 pve/data 這個 pool 下面。因此，我們需要調整的是 pve/data 的大小。\n使用 lvresize 命令來調整 LV 分配的空間， -l 表示擴大， +100%FREE 表示將所有閒置空間分配給該 LV，最後指定 pve/data：\nlvresize -l +100%FREE pve/data 執行完後再次使用 lvs 命令確認，現在 data 的大小應該已經變大了：\n# lvs lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert data pve twi-aotz-- \u0026lt;3.51t 35.16 3.86 root pve -wi-ao---- 96.00g swap pve -wi-ao---- 8.00g vm-100-disk-0 pve Vwi-aotz-- 1.00t data 57.73 vm-100-disk-1 pve Vwi-aotz-- 4.00m data 14.06 現在，從 Proxmox Web GUI，你也應該可以看到存儲空間大小已成功調整。\n","date":"August 3 2023","externalUrl":null,"permalink":"/posts/proxmox-hard-disk-replacement-and-expansion/","section":"文章","summary":"","title":"Proxmox 硬碟更換及擴容","type":"posts"},{"content":"","date":"August 3 2023","externalUrl":null,"permalink":"/tags/%E7%A1%AC%E7%A2%9F%E6%90%AC%E9%81%B7/","section":"標籤","summary":"","title":"硬碟搬遷","type":"tags"},{"content":"","date":"November 3 2022","externalUrl":null,"permalink":"/tags/debug/","section":"標籤","summary":"","title":"Debug","type":"tags"},{"content":"","date":"November 3 2022","externalUrl":null,"permalink":"/categories/openstack/","section":"分類","summary":"","title":"OpenStack","type":"categories"},{"content":"在前面的文章中，我們介紹了 Openstack 的幾種網路架構，並使用 Openstack-Ansible (OSA) 完成了 Openstack 的部屬，今天則會簡單紀錄一下在完成 OSA 部屬之後怎麼操作 openstack。\n使用 openstack CLI # 完成 OSA 部屬之後，在控制節點上可以使用 lxc-ls -f 指令查看 LXC 資訊。\n# lxc-ls -f NAME STATE AUTOSTART GROUPS IPV4 IPV6 UNPRIVILEGED infra1_galera_container-763c1458 RUNNING 1 onboot, openstack 10.0.3.44, 192.168.56.132 - false infra1_glance_container-ecb3abc2 RUNNING 1 onboot, openstack 10.0.3.174, 192.168.56.4 - false infra1_horizon_container-3c77d785 RUNNING 1 onboot, openstack 10.0.3.30, 192.168.56.238 - false infra1_keystone_container-40e90a3e RUNNING 1 onboot, openstack 10.0.3.19, 192.168.56.108 - false infra1_memcached_container-7a783869 RUNNING 1 onboot, openstack 10.0.3.242, 192.168.56.196 - false infra1_neutron_server_container-35c98df6 RUNNING 1 onboot, openstack 10.0.3.36, 192.168.56.74 - false infra1_nova_api_container-775a3593 RUNNING 1 onboot, openstack 10.0.3.225, 192.168.56.244 - false infra1_placement_container-dac38473 RUNNING 1 onboot, openstack 10.0.3.226, 192.168.56.33 - false infra1_rabbit_mq_container-c2bb7a2e RUNNING 1 onboot, openstack 10.0.3.20, 192.168.56.115 - false infra1_repo_container-23639b71 RUNNING 1 onboot, openstack 10.0.3.129, 192.168.56.94 - false infra1_rsyslog_container-509d2d2f RUNNING 1 onboot, openstack 10.0.3.164, 192.168.56.141 - false infra1_utility_container-4b62f537 RUNNING 1 onboot, openstack 10.0.3.198, 192.168.56.75 - false 其中可以看到一個 utility_container，是 OSA 用來提供 openstack cli 的 LXC。\n透過 lxc-attach infra1_utility_container-4b62f537 進入 LXC。\n在使用 openstack cli 之前要將 admin 登入資訊載入環境變數，可以透過指令 . ~/openrc。\n接著就可以透過 openstack 這個指令跟 cluster 互動。\n# openstack user list --os-cloud=default +----------------------------------+-----------+ | ID | Name | +----------------------------------+-----------+ | 8291fddd2a80414d8f1b3cb40ff720c0 | admin | | 3761aw47e9204a8090843b00705b0ea8 | placement | | 4526a5q64f9a48dc9752b577e26559b9 | glance | | 2dbf52994f7f49a1b6eb5afdc6590781 | nova | | b7866083ve63469f93bd26e72a141a13 | neutron | +----------------------------------+-----------+ 訪問 horizon web GUI # 透過之前在 /etc/openstack_deploy/openstack_user_config.yml 設置的 external_lb_vip_address，透過瀏覽器 https 訪問該網址。\n帳號是 admin，密碼可以直接在前面提到的 openrc 檔案裡面找到 OS_PASSWORD，或著到 deployment host 的 /etc/openstack_deploy/user_secrets.yml 中找到 keystone_auth_admin_password。\n虛擬機建立流程 # 假設前面使用 OSA 部署完成，並使用 flat 網路模式，預留 192.168.56.100-200/24 作為虛擬機 IP，gateway 為 195.168.56.1。\n首先我們要下載一個 VM 的映象檔，這邊使用 openstack 示範使用的 cirros，這是一個只有 13Mb 大小的 img，適合拿來測試。並將其上傳到 glance 上。\nwget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img glance image-create --name \u0026#34;cirros\u0026#34; \\ --file cirros-0.4.0-x86_64-disk.img \\ --disk-format qcow2 --container-format bare \\ --visibility=public 接著我們要將 flat 網路加入 openstack，假設前面部屬的時候將 net_name 設置為 flat_net\nopenstack network create --external --share \\ --provider-physical-network flat_net --provider-network-type flat \\ flat_network neutron 會為 VM 分配 IP，因此要設置 ip pool\nopenstack subnet create --network flat_network \\ --allocation-pool start=192.168.56.100,end=192.168.56.200 \\ --dns-nameserver 8.8.4.4 --gateway 192.168.56.1 \\ --subnet-range 192.168.56.0/24 flat_ip_pool 接著我們要建立一個 flavor，作為虛擬機的模板，定義 VM 的規格，包含 CPU, memory, disk 的大小\nopenstack flavor create --vcpus 1 --ram 512 --disk 1 m1.tiny 接著我們就可以真的將這個 VM 建立出來了\nopenstack server create --flavor m1.tiny --image cirros --security-group default test 因為我們現在只建立了一個 network，因此可以直接省略，如果存在多個 network，就需要下 –network 參數\n為了要測試，虛擬機是否正常，我們可以透過 ssh 連進去虛擬機裡面進行操作\n首先我們要修改 vm 的 security group rule，security group 是 openstack 提供防火牆管理的機制，將 VM 劃分成多個 security group，統一套用特定的防火牆規則，預設的 defualt group 是不允許任何 ingress 流量的，因此我們要讓他放行 ssh 和 icmp 方便測試。\nopenstack security group rule create --protocol tcp --ingress --dst-port 22 --remote-ip 0.0.0.0/0 \u0026lt;group id\u0026gt; openstack security group rule create --protocol icmp --remote-ip 0.0.0.0/0 \u0026lt;group id\u0026gt; 這邊要填入 security group id，可以透過剛剛建立 VM 的結果資訊裡面找到，或著透過 openstack security group list 查看。\n接著我們就可以透過 ssh 連入 VM 內部\nopenstack server ssh -4 --private --login cirros test cirros 的帳號密碼是 cirros/gocubsgo\n","date":"November 3 2022","externalUrl":null,"permalink":"/posts/openstack-deployment-serial-3-basic-management-commands/","section":"文章","summary":"","title":"OpenStack 架設系列文章 (3) - 基本指令操作","type":"posts"},{"content":"","date":"November 3 2022","externalUrl":null,"permalink":"/tags/%E7%B3%BB%E7%B5%B1%E5%BB%BA%E7%BD%AE%E6%95%99%E5%AD%B8/","section":"標籤","summary":"","title":"系統建置教學","type":"tags"},{"content":"這篇記錄一下，在使用 openstack 時，遇到的幾種建立失敗情況和解決方法\nQuota exceeded # openstack 透過 Quota 來限制每個 project 可以使用的資源量，當超過限制時，就會出現這個錯誤訊息。預設 admin project 也會有這個限制，而且預設值其實很小，如 instances、CPU 和 memory 的限制\n10 instances 20 vCPU 51200 MB RAM 透過指令 openstack quota show 可以查看目前的限制並透過 quota set --cores/ram/instance... 200 \u0026lt;project\u0026gt; 來修改各種限制 資源不足 # 也有可能是因為 openstack cluster 已經沒有足夠多的 cpu, memory 或硬碟可以分配給虛擬機了。\n最簡單方法是增加新的機器來應付需求。\n但是通通來說，分配 8GB 的 RAM 給虛擬機不代表虛擬機會吃滿 8GB 的 RAM，因此在一台有 100GB RAM 的機器上，開 50 台 8GB RAM 但是實際上只會用到 1GB RAM 的虛擬機，是不會有問題的。\n因此我們就需要調整 allocation ratio，allocation ratio 表示 openstack 可以分配倍物理設備擁有的資源給虛擬機，例如前面的例子中，50 台 8GB RAM 的機器需要 400GB 的 ram。但是實際上 server 只有 100GB 的 RAM 還要扣掉 2GB 是預留給 host 本身運行使用，因此只剩下 98GB 的 RAM 可以分配給虛擬機，因此 allocation ratio 最少需要是 400/98=4.1。\n在每個 compute node，可修改 /etc/nova/nova.conf 這個檔案中的三個參數，對應到 cpu, disk, ram 的 allocation ratio。\ncpu_allocation_ratio disk_allocation_ratio ram_allocation_ratio 修改完後要重啟 nova。service nova-compute restart 可以透過指令查看各個設備上 resource 的使用情況\n# openstack allocation candidate list --os-placement-api-version 1.12 --resource VCPU=1 --resource MEMORY_MB=1024 --resource DISK_GB=10 +---+----------------------------------+--------------------------------------+------------------------------------------+ | # | allocation | resource provider | inventory used/capacity | +---+----------------------------------+--------------------------------------+------------------------------------------+ | 1 | VCPU=1,MEMORY_MB=1024,DISK_GB=10 | 000ab365-74ea-4c16-9aaa-3a2bdea6238c | VCPU=1/4,MEMORY_MB=512/3893,DISK_GB=1/30 | +---+----------------------------------+--------------------------------------+------------------------------------------+ 其中每一個 resource provider 都是一個 compute node，可以透過 openstack resource provider list 查看對應到哪來設備 8944。\n","date":"November 3 2022","externalUrl":null,"permalink":"/posts/solve-openstack-vm-create-fail/","section":"文章","summary":"","title":"解決 OpenStack  VM 建立失敗問題","type":"posts"},{"content":"","date":"November 1 2022","externalUrl":null,"permalink":"/tags/lxc/","section":"標籤","summary":"","title":"LXC","type":"tags"},{"content":"在前一篇文章中，我們介紹了 Openstack 的幾種 網路架構，今天我們要介紹如何使用 Openstack 官方提供的 Openstack-Ansible (OSA) 來完成 Opentack 的部署。\n本篇文章以 Openstack Victoria 版本為例，安裝在作業系統為 ubuntu 的設備上，其他版本可能會有所不同。另外本篇文章主要專注在使用 nova 建立虛擬機服務，因此會部分省略掉 cinder 等 opentstack 儲存功能的部分。\n介紹 # Openstack 是由 keystone、glance 等多個組件組成，組件下又需要 dabase base 和 message queue 等服務，再加上需要在每個運算節點上部屬的 nova、neutron，如果要手動部署，會需要花費大量的時間、容易出錯，也不利於擴展。因此 Openstack 官方提供了 Openstack-Ansible (OSA)，使用 Ansible 來自動化部署及擴展 Openstack cluster。\n首先必須要知道透過 OSA 部屬的 openstack clauster 在架構上會與手動安裝文件教的方式有些差異。\n所有 Controller node 上的 openstack 組件被部屬在獨立的 linux contaienr (LXC) 內，LXC 透過 linux bridge 直接暴露在外部網路中，與節點們處在同一個子網段內，可以直接互相訪問 controller node 上的組建之間不直接互相訪問，而是透過一個 haproxy 做作為進入點，提供附載均衡和高可用。haproxy 會被直接部屬在控制節點上，而不會被部屬在 LXC 內。 另外在在 OSA 的文件內，用來執行 ansible playbook 的節點稱之為 deployment host，而被部屬 openstack 的 controller node 和 compute node 被統稱為 taget host。不過在實際操作的時候 deployment host 不一定需要是一台獨立的設備，可以直接挑一台 target host 來跑。\n安裝流程 # 前置作業 # 首先我們要在 deployment host 上安裝 python 還有 OSA。\n# system apt update apt dist-upgrade apt install build-essential git chrony \\ openssh-server python3-dev sudo # OSA git clone -b victoria-em https://opendev.org/openstack/openstack-ansible/opt/openstack-ansible cd /opt/openstack-ansible scripts/bootstrap-ansible.sh 接著在 target hosts 上我們也需要先安裝一些必備的套件\napt update apt dist-upgrade apt install bridge-utils debootstrap openssh-server \\ tcpdump vlan python3 apt install linux-modules-extra-$(uname -r) Ansible 是透過 ssh 連線到每一台主機上面完成安裝，且 OSA 要求在 deployment 和 target hosts 上都使用 root。OSA 要求的作法是在 deployment 的 root 上帳號下，使用 ssh-keygen 生成 ssh key 並將 public key 放到每個 target host 的 /root/.ssh/authorized_keys 中。另外 OSA 還會將 deployment host 上的 /root/.ssh/id_rsa.pub 檔案複製到每一個 LXC 的 authorized_keys，方便後續訪問。這個可以透過 lxc_container_ssh_key 選項修改，如何設定 OSA 會在後面提到。\n最後是網路相關的設定，在前一篇文章中我們提到 openstack 的網路模式有 flat、vlan、vxlan 三種再加上 OSA 的 LXC 也需要特別的網路，因此這個部分會變得有點複雜，openstack 的虛擬機網路設定會在後面設定 OSA 的章節再來說明。\n在網路的部分，OSA 要求網路基礎架構是預先提供好的，在所有節點上，我們需要有一個 br-mgmt 的 linux bridge，OSA 會為每一個 LXC 建立 veth，連結 br-mgmt 和 LXC 的 eth0 介面。前面有提到 LXC 和節點是在一個 flat 的網路架構，因此 br-mgmt 也需要連接到主機的實體網卡，並給予一個 IP。\n# /etc/netplan/99-openstack.yaml network: version: 2 renderer: networkd bridges: br-mgmt: addresses: - 192.168.56.1/24 interfaces: [eth1] 在 ubuntu 上我們可以透過 netplan 來設置。\n如果是需要在單節點下啟用 flat 網路做簡單的測試，可以使用下面 netplan 設定\n--- network: version: 2 renderer: networkd ethernets: eth1: {} bridges: br-mgmt: addresses: - 192.168.56.101/24 - 192.168.56.1/24 interfaces: [eth1] vlans: eth1.10: id: 10 link: eth1 br-mgmt 多一個 IP 用於 external_lb_vip_address (後面會再提到) flat 網路設定時，綁定到 eth1.10 這張網卡 如果要使用 block storage 的服務的話還需要配置 LVM，這邊我們就省略掉\nOSA 設定檔設置 # 接著我們要調整 OSA 的設定檔。首先我們要把基礎設定複製到設定檔目錄\ncp -r /opt/openstack-ansible/etc/openstack_deploy/etc/openstack_deploy 這邊我們主要要修改兩個檔案 openstack_user_config.yml 還有 user_variables.yml。\nopenstack_user_config 是各 node 功能和 IP 等基本資訊的設置，OSA 會讀取 openstack_user_config 來生成 ansible 的 inventory file。 user_variables.yml 則是 OSA 的主要設定檔。 在 /etc/openstack_deploy 內有各種後綴為 example 的範例文件可以參考\nopenstack_user_config # --- # openstack_user_config.yml cidr_networks: container: 192.168.56.0/24 tunnel: 172.29.240.0/22 storage: 172.29.244.0/22 used_ips: - \u0026#34;192.168.56.1,192.168.56.50\u0026#34; - \u0026#34;172.29.236.1,172.29.236.50\u0026#34; - \u0026#34;172.29.240.1,172.29.240.50\u0026#34; - \u0026#34;172.29.244.1,172.29.244.50\u0026#34; - \u0026#34;172.29.248.1,172.29.248.50\u0026#34; global_overrides: # The internal and external VIP should be different IPs, however they # do not need to be on separate networks. external_lb_vip_address: 172.29.236.10 internal_lb_vip_address: 172.29.236.11 management_bridge: \u0026#34;br-mgmt\u0026#34; provider_networks: ... 首先我們來設定 /etc/openstack_deploy/openstack_user_config.yml，可以複製 /etc/openstack_deploy/openstack_user_config.yml.example 來做修改。cidr_networks 定義了各種網路架構的 IP 區段，最主要的是 containers，是前面提到節點本身還有 controller service LXC 們要分配的 IP 段，這邊指定為 192.168.56.0/24。另外兩個 tunnel 和 storage 分別是給 vxlan 虛擬機網路架構和 block storage 用的，在 OSA 的架構內，control plane、storage 和虛擬機網路希望是分開的獨立網路，因此如果有使用 block storage 或 vxlan 的話這邊要設置對應的 IP 段。\n接著 used_ips 是不允許被分配給 LXC 容器的 IP，\u0026quot;172.29.236.1,172.29.236.50\u0026quot; 表示.1 到.50 這 50 個 IP 都不允許被使用，當 OSA 生成 inventory file 時會在 container IP 段內隨機為每一個 LXC 分配 IP，因此需要透過此設定規避掉 gateway router 還有其他非 Openstack 使用的 IP。在單 controller node 的部屬環境下 LXC 會用掉 13 個 IP。\n接著 external_lb_vip_address 和 internal_lb_vip_address 是 load balancer 的地址，這邊可以直接指定一個 controller node 的 IP，直接使用 controller node 上的 haproxy，或著在多 controller node 的情況下，使用一個獨立的 load balancer (不在 OSA 自動部屬的範圍)，提供控制平面的高可用 (HA)。\n這邊 external 和 internal 的差別對應到 keystone 設定時需要為 endpoint 設置 public、private 和 admin 三種的 url，public 就會使用 external 的 address，後倆著則會使用 internal 的 address。\n另外 address 也可以設置為 domain name，如 openstack.example.com，不過要特別注意的是 domain 要能夠被 dns 解析，而且要在 user_variables.yml 設置 haproxy_keepalived_external_vip_cidr:\u0026quot;\u0026lt;external_vip_address\u0026gt;/\u0026lt;netmask\u0026gt;\u0026quot;。根據文件，不建議將 external address 和 internal address 設置相同，特別是當兩者分別使用 http 和 https 時，一定要把兩個 address 分開。\n接著要設置的是 provider_networks，這邊包含 LXC 網路和虛擬機網路，provider_networks 是一個 list，列出了所有主機上可用的網路。\nprovider_networks: - network: container_bridge: \u0026#34;br-mgmt\u0026#34; container_type: \u0026#34;veth\u0026#34; container_interface: \u0026#34;eth1\u0026#34; ip_from_q: \u0026#34;container\u0026#34; type: \u0026#34;raw\u0026#34; group_binds: - all_containers - hosts is_container_address: true 首先我們要提供前面提到給 LXC 和節點使用的控制網路，這邊主要要注意的是 container_bridge 要指定成節點上建立好的 bridge 名稱，可以直接沿用慣例的 br-mgmt。\n另外 type 這邊要設置為 raw，type 總共有四種可能 raw, flat, vlan, vxlan，raw 是提供給 mgmt 網路和 storage 網路使用的，後三著則對應到虛擬機網路的三種可能架構。\n接著就來到了 OSA 文件裡面最有問題的地方了，就是關於虛擬機網路架構的設定，在幾乎所有的 OSA 設定文件裡面都會要求在 flat 和 vlan type 的網路中，設置 container_bridge 為 br-vlan。然而在少數 文件 內，可以找到這樣一段話。\nThe “br-vlan” bridge is no longer necessary for deployments unless Neutron agents are deployed in a container. Instead, a direct interface such as bond1 can be specified via the “host_bind_override” override when defining provider networks.\n原來在現有的版本內，已經不需要設置 br-vlan 這個東西了。參考 OSA 容器網路的架構圖，我們可以得知需要 br-vlan 是因為 compute node 上的 neutron 是跑在一個 LXC 內，因此需要多一個 br-vlan 還有 vath pair 將網卡接到 LXC 內。\n而在現行的架構下 neutron agent 是直接跑在 compute node 的 host 上的。\n然後回顧本系列文章前一篇就會知道，不論是 vlan 還是 flat 架構下，我們都只需要指定一張 interface 作為對外出口，bridge 的部分是歸屬於 neutron agent 管理的。\n這邊是正確設置 flat 網路需要的設定檔。\nprovider_networks: - network: host_bind_override: \u0026#34;eth1.10\u0026#34; type: \u0026#34;flat\u0026#34; net_name: \u0026#34;flat\u0026#34; group_binds: - neutron_linuxbridge_agent host_bind_override 指定 flat 網路連到節點外的網卡名稱。 type: flat 網路模式下設置為 flat net_name: neutron physical_network mapping 的 name 同理於 vlan 網路設定，和 flat 的差別是要指定可用的 vlan id range (range)\nprovider_networks: - network: host_bind_override: \u0026#34;eth2\u0026#34; type: \u0026#34;vlan\u0026#34; net_name: \u0026#34;vlan\u0026#34; range: \u0026#34;101:200,301:400\u0026#34; group_binds: - neutron_linuxbridge_agent 最後是 vxlan 的部分，vxlan 的設定與 vlan 和 flat 有比較大的差異，在前一篇文章我們提到，vxlan 網路架構要指定一個 local ip 作為 vxlan 封包對外的 IP，並自動綁定到該 IP 對應的網路介面。\n在 OSA 裡面 vxlan 則是需要設定 container_bridge，指定一個 bridge，OSA 會將該 bridge 的 IP 作為 local ip。\nprovider_networks: - network: container_bridge: \u0026#34;br-vxlan\u0026#34; ip_from_q: \u0026#34;tunnel\u0026#34; type: \u0026#34;vxlan\u0026#34; range: \u0026#34;1:1000\u0026#34; net_name: \u0026#34;vxlan\u0026#34; group_binds: - neutron_linuxbridge_agent container_bridge: 指定為 vxlan 對外綁定的 bridge ip_from_q: 指定為 vxlan 封包傳輸的子網域，雖然 OSA 會從 bridge 提取 local IP，但是還是從該欄位取得 local address 的子網域遮罩 range: 指定可用的 vxlan id (vni) 到這邊就完成 global_overrides 部分的設定。接下來是 openstack_user_config 對每個不同 service 還有節點的設定。\n### ### Infrastructure ### shared-infra_hosts: infra1: ip: 192.168.56.101 --- ### OpenStack ### # keystone identity_hosts: infra1: ip: 192.168.56.101 infra2: ip: 192.168.56.102 # glance image_hosts: infra1: ip: 192.168.56.101 --- # nova hypervisors compute_hosts: compute1: ip: 192.168.56.103 host_vars: ... compute2: ip: 192.168.56.104 storage_hosts: infra1: ip: 172.29.236.11 container_vars: ... 這個部分的格式是\nSERVICE_NAME: HOST1_NAME: ip: HOST1_MGMT_IP HOST2_NAME: ip: HOST1_MGMT_IP ... 對於 openstack 的不同組件，我們可以自由控制要在那些節點上部屬，並在多個節點上部屬單個服務來達到高可用。如果希望 OSA 不要部屬，則可以直接省略對應的 service，OSA 在部屬的時候就會跳過該服務。\n特別要注意的是 compute_hosts 這個 service，compute_hosts 定義了要在哪些節點上部屬 nova_compute 和 neutron agent，也就是作為 openstack 的運算節點。\n另外還可以透過 host_vars 和 container_vars 覆蓋 OSA 的預設值，來對每個節點做單獨調整。\n完整設定範例可以參考 OSA 的 部屬範例，及 openstack_user_config 的 reference 文件。\nuser_variables # 另外一個要設定的設定檔是 /etc/openstack_deploy/user_variables.yml。\n透過 openstack-ansible 指令執行 ansible 時，會將 /etc/openstackdeploy 目錄下的 user*.yml 檔案會作為 ansible 的 varible file，在執行 playbook 的時候被自動加入。\n在 user_variables.yml 中，可以對 OSA 所有組件的部屬和設定進行調整。參照 OSA 的 官方文件，這邊每一個 role 對應到 OSA 部屬的每個 service，裡面有列出每個 service 每個設定的預設值。通常設定會使用 service name 當作 prefix，例如 glance_etc_dir 可以修改 glance 的設定檔位置，預設是“/etc/glance”，可能可以修改為“/usr/local/etc/glance”。另外一份是進階設定的 reference，有比較詳細對 user_varialbes 設定的說明。\n這邊提幾個可能比較需要注意到的設定項目\ninstall_method: 選項有 source 和 distro，這個設定項會控制 openstack 組件的來源 預設是 source，表示 OSA 會直接從 openstack git 拉取原始碼在本地進行安裝，根據官方文件優點是這樣的部屬比較有彈性還有可客製化的程度會比較高，甚至可以將 repo 切成自己的 repostory 來直接修改原始碼，不過缺點是安裝時間比較久 distro 則是比較像 openstack 手動安裝教學文件的方式，從 linux 發行版的 repository 直接安裝，優點是可能會有針對發行版的優化與修復還有安裝速度，但是缺點就是更新不會像官方這麼及時，而且會缺乏一些可設定的選項 keystone_service_publicuri_proto 和 haproxy_ssl 在 openstack 上每個 service 的 api endpoint 分為 public, private, admin 三種，通常來說 public 因為是要公開給外部使用的加上 ssl (可以參考這份 文件)。 在 OSA 中，external_lb_vip_address 會用於 public endpoint，並預設會加上自簽 ssl 憑證，然而在 user_variables.yml.example 中 keystone_service_publicuri_proto 設置為 http，因此透過 public endpoint 操作 openstack api 時，會被 keystone 導向到 http，導致 openstack client 沒辦法正確處理 haproxy ssl 加密的數據，因此要注意將 keystone_service_publicuri_proto 設置為 https user_secrets # 最後需要生成 openstack 組件間溝通使用的密碼，可以簡單透過指令生成，路徑在 /etc/openstack/user_secrets.yml\ncd /opt/openstack-ansible \u0026amp;\u0026amp; ./scripts/pw-token-gen.py --file /etc/openstack_deploy/user_secrets.yml 執行 openstack ansible # 完成前置作業和 openstack_user_config.yml、user_variables.yml 兩個檔案的設置後，就可以正式來進行部屬了。首先要移動到 /opt/openstack-ansible/playbooks，所有指令在這個目錄內執行。\n接著為了讓 ansible 可以 ssh 連線到 target hosts，我們要使用 ssh agent，ssh agent 會再進行 ssh 連線時，自動嘗試預先列好的 ssh key file，進行登入。透過 ssh-add 將所有可能的 ssh key file 加入，由於前面我們將 id_rsa.pub 加入到 target host 的 authorized_keys 中，因此這邊只需要加這個 key。\neval $(ssh-agent) ssh-add ~/.ssh/id_rsa 接著要 export 兩個環境變數，這個是要解決 ansible 本身的 bug，如果再執行 OSA 過程中出現 failed to transfer file to... 錯誤可以嘗試加這兩個環境變數，詳情 可以查看 ansible 的 issue。\nexport ANSIBLE_LOCAL_TEMP=$HOME/.ansible/tmp export ANSIBLE_REMOTE_TEMP=/home/vagrant/.ansible/tmp 接著執行第一步\nopenstack-ansible setup-infrastructure.yml --syntax-check OSA 會根據 openstack_user_config.yml 生成 ansible 的 inventory 文件，放置在 /etc/openstack_deploy/openstack_inventory.json，並替每個 LXC 分配 IP。\n接著就是執行正是安裝部屬的腳本。\nopenstack-ansible setup-hosts.yml openstack-ansible setup-infrastructure.yml openstack-ansible setup-openstack.yml 這邊分成三個步驟，分別是\n在 target hosts 上安裝 LXC 等必要套件 安裝 database, msg queue, repo server 等 intra 安裝設置 openstack 組件理論上三個指令都完整執行成功的話，就代表 openstack 正確建立起來了 更簡單的指令是執行 setup-everything，他會依據執行上面三個指令。\nopenstack-ansible setup-everything.yml 另外當執行後，ansible 的 facts 會被保存在 /etc/openstack_deploy/ansible_facts/。\n除錯 # 在我的安裝經驗中最容易出問題的是 haproxy 的部分。在 OSA 的部屬環境下，所有元件的 log 會被收到 systemd 的 log 內。因為 haproxy 是安裝在 host 上所以可直接在 target host 上，下 journalctl -xe 查看 haproxy 的 log。各個 openstack 組件的 log 則要到各個 LXC 內去查看 systemd log。\n在 Rocky 版本後預設使用 systemd log 取代 rsyslog，因此如果在 openstack_user_config 中指定部屬 log_hosts 是沒有意義的，需要額外將 rsyslog_server_enabled 和 rsyslog_client_enabled 給 enable\n如果是 LXC 有問題，可以在 target host 的 root 權限下\n使用 lxc-ls -f 查看 LXC 的狀態、IP 使用 lxc-attach -n \u0026lt;container_name\u0026gt; 進入 LXC 內查看 結語 # 到這邊就完成整個 OSA 基本的安裝部屬流程了，下一篇我們會簡單看一下在部屬完成 openstack 後，如何測試和使用 openstack 的基本功能。\n","date":"November 1 2022","externalUrl":null,"permalink":"/posts/openstack-deployment-serial-2-deployment-with-openstack-ansible/","section":"文章","summary":"","title":"OpenStack 架設系列文章 (2) - 使用 OpenStack Ansible 部署 OpenStack","type":"posts"},{"content":"","date":"November 1 2022","externalUrl":null,"permalink":"/tags/openvpn/","section":"標籤","summary":"","title":"OpenVPN","type":"tags"},{"content":"最近需要在 proxmox 上面裝 openvpn 的 client 當作本地所有裝置的跳板 (gateway) 來連到受管理網域，因此這邊紀錄一下怎麼在 proxmox 上面開 LXC 和設定 openvpn client。\nPromox LXC 設置 # 首先從 proxmox 的 WebGUI 建立一個 VM 後，要修改 /etc/pve/lxc/${LXC_ID}.conf，直接加入下面幾行。讓 LXC 可以存取 openvpn 需要使用的 tun driver。\nlxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file lxc.mount.entry: /dev/net dev/net none bind,create=dir lxc.cgroup.devices.allow: c 10:200 rwm lxc.apparmor.profile: generated lxc.apparmor.allow_nesting: 1 接著透過 web shell 進入到 LXC 內進行操作。\nOpenvpn client 設置 # 安裝 Openvpn\napk add oepnvpn 將從 server 端拿到的 ovpn client file (.ovpn) 修改為 /etc/openvpn/openvpn.conf\n如果使用帳號密碼進行登入，為了要讓 ovpn 可以開機自動工作，我們要修改 openvpn.conf，加入或修改為 auth-user-pass login.conf。然後增加 /etc/openvpn/login.conf，第一行打帳號，第二行打密碼。\n可以透過 /etc/init.d/openvpn start 指令啟動 openvpn，可能會出現 WARNING: openvpn has started, but is inactive 但是不影響。\n可以透過 ip link 檢查是不是有 tun0 這張介面出現 透過 rc-update add openvpn 讓 openvpn 開機自啟動\n防火牆、gateway 設置 # 為了要讓 LXC 當作 VPN gateway，我們要修改防火牆設置。\n開啟 ipv4 轉發\necho \u0026#34;net.ipv4.ip_forward=1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf 添加防火牆規則\napk add iptables rc-update add iptables iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE /etc/init.d/iptables save 這邊是讓本地的封包經過 VPN 時，加上一層 NAT，不然 vpn server 那邊不會認得本地的 IP。\n路由設定 # 這邊主要是要修改 openvpn.conf，讓 openvpn 只轉發特定的流量。\nroute-nopull route 10.0.20.0 255.255.255.0 vpn_gateway 第一行 route-nopull 讓 openvpn 不會去跟 vpn server 要求路由表，而是只轉發我們希望他轉發的流量。 第二行設定將 10.0.20.0/24 這個網段往 vpn server 送。 重啟 openvpn `/etc/init.d/openvpn restart 這樣就能限制只有連線到 10.0.20.0/24 這個網段的時候，經過 VPN 不過這樣只完成 LXC 的路由設定，接著要在需要通過 VPN 的電腦或直接在路由器上將 10.0.20.0/24 這個網段送到 VPN gateway 的 IP\nip route add 10.0.20.0/24 via \u0026lt;VPN gateway LXC\u0026#39;s ip\u0026gt; 如果是在路由器上設置好，本地所有的裝置都可以直接透過 VPN 訪問 10.0.20.0/24 這個網段而不用在每台機器上設置防火牆了。\n","date":"November 1 2022","externalUrl":null,"permalink":"/posts/install-openvpn-client-on-proxmox/","section":"文章","summary":"","title":"Proxmox 安裝 OpenVPN client 紀錄","type":"posts"},{"content":"","date":"October 31 2022","externalUrl":null,"permalink":"/tags/2022-ithome-%E9%90%B5%E4%BA%BA%E8%B3%BD---%E5%AD%B8%E7%BF%92-ebpf-%E7%B3%BB%E5%88%97/","section":"標籤","summary":"","title":"2022 IThome 鐵人賽 - 學習 EBPF 系列","type":"tags"},{"content":"","date":"October 31 2022","externalUrl":null,"permalink":"/categories/ebpf/","section":"分類","summary":"","title":"EBPF","type":"categories"},{"content":" 前言 # 這個系列是 2022 年參加 ithome 鐵人 30w 競賽的產物，參賽主題是“教練我想玩 eBPF“，因為近年來 eBPF 成為 cloud native 的一個熱門話題，在 COSCUP2022 還有一些 meetup 的活動都在討論這個議題，因此希望能夠找個機會來學習這項技術，借助這個比賽的機會，來學習和整理 eBPF 的知識，比賽結束後將這三十天的產出重新整理後重新發表，成為本“學習 eBPF 系列”。\n本系列文章主要是兩個部分，首先是針對 eBPF 的一些基本語法、架構和用法的基本介紹。接著透過 trace bcc 這個 eBPF 的開發框架的許多範例原始碼，來了解 eBPF 的用途和實例，最後簡單介紹一下所有的 eBPF 裡面很重要的 helper functions。\neBPF 的前身 # 要介紹 eBPF 勢必得先聊聊 eBPF 的前身 Berkeley Packet Filter (BPF)，BPF 最早是在 1993 年 USENIX 上發表的一個在類 Unix 系統上封包擷取的架構。\n由於封包會持續不斷的產生，因此在擷取封包時，單一個封包的處理時間可能只有幾豪秒的時間，又封包擷取工具的使用者通常只關注某部分特定的封包，而不會是所有的封包，因此將每個封包都丟到 User space 來處理是極為低效的行為，因此 BPF 提出了一種在 kernal 內完成的封包過濾的方法。\n簡單來說，BPF 在 kernal 內加入了一個簡易的虛擬機環境，可以執行 BPF 定義的指令集，當封包從網卡內進入的時候，就會進到 BPF 的虛擬機，根據虛擬機的執行結果來決定是否要解取該封包，要的話再送到 user space，因此可以直接在 kernal 過濾掉所有不必要的封包。\n大家最常使用的封包過濾工具應該是 tcpdump ，tcpdump 底下就是基於 BPF 來完成封包過濾的，tcpdmp 使用了 libpcap 這個 library 來與 kernal 的 BPF 溝通，當我們下達 tcpdump tcp port 23 host 127.0.0.1 這樣的過濾規則時，過濾規則會被 libpcap 編譯成 BPC 虛擬機可以執行的 bpf program，然後載入到 kernal 的 BPF 虛擬機，BPF 擷取出來的封包也會被 libpcap 給接收，然後回傳給 tcpdump 顯示\ntcpdump -d ip 透過 -d 這個參數，我們可以看到 ip 這個過濾規則會被編譯成怎樣的 BPF program\n(000) ldh [12] (001) jeq #0x800 jt 2 jf 3 (002) ret #262144 (003) ret #0 Line 0 ldh 指令複製從位移 12 字節開始的 half word (16 bits) 到暫存器，對應到 ethernet header 的 ether type 欄位。 Line 1 jeq 檢查暫存器數值是否為 0x0800 (對應到 IP 的 ether type) 是的話，走到 Line 2 return 262144 不是的話，跳到 Line 3 return 0 ret 指令結束 BPF 並根據回傳值決定是不是要擷取該封包，回傳值為 0 的話表示不要，非 0 的話則帶表要擷取的封包長度，tcpdump 預設指定的擷取長度是 262144 bytes。 BPF 提供了一個高效、可動態修改的 kernal 執行環境的概念，這個功能不僅只能用在封包過濾還能夠用在更多地方，因此在 Linux kernal 3.18 加入了 eBPF 的功能，提供了一個“通用的”in-kernal 虛擬機。承接了 BPF 的概念，改進了虛擬機的功能與架構，支援了更多的虛擬機啟動位置，使 eBPF 可以用在更多功能上。\n也因為 eBPF 做為一個現行更通用更強大的技術，因此現在提及 BPF 常常指的是 eBPF，而傳統的 BPF 則用 classic BPF (cBPF) 來代指。\neBPF 的應用 # 在介紹 BPF 的時候，有提到 BPF 本身就是一個在 kernal 內的虛擬機。eBPF 在 kernal 的許多功能內埋入了虛擬機的啟動點 (hook point)。例如當 kernal 執行 clone 這個 system call 的時候，就會去檢查有沒有 eBPF 程式的啟動條件是等待 clone 這個 system call，如果有的話就會調用 BPF 虛擬機執行 eBPF 程式，同時把 clone 相關的資訊帶入到虛擬機。同時虛擬機的執行結果可以控制 kernal 的後續行為，因此可以透過 eBPF 做到改變 kernal 程式進程、數據，擷取 kernal 執行狀態等功能。\n使用 eBPF 我們可以在不用修改 kernal 或開發 kernal module 的情況下，增加 kernal 的功能，大大了降低了 kernal 功能開發的難度還有降低對 kernal 環境版本的依賴。\n這邊舉立一些 eBPF 的用途\nin kernal 的網路處理：以往在 linux 上要實作網路封包的處理，通常都會經過整個 kernal 的 network stack，通過 iptables (netfilter), ip route 等組件的處理。透過 eBPF，我們可以在封包進入 kernal 的早期去直接丟棄非法封包，這樣就不用讓每個封包都要跑完整個 network stack，達到提高效能的作用\n最知名的應該是 Cilium 這個 CNI 專案，基於 eBFP 提供了整套完整網路、安全、監控的 Kubernetees CNI 方案。 kernal tracing: 前面提到 eBPF 在 kernal 內的許多地方都埋入了啟動點，因此透過 eBPF 可以再不用對 kernal 做任何修改的情況下，很有彈性的監聽分析 kernal 的執行狀況\n下圖是 bcc 專案使用 eBPF 開發的一系列 Linux 監看工具，基本涵蓋了 kernal 的各個面向。\n另外一個專安 bpftrace 也提供了一個非常簡單的語法，來產生對應的 eBFP tracing code。\nuser level tracing: 透過 eBFP，我們可以做 user level 的 dynamic tracing，來監看 user space 應用程式的行為。\n一個很有趣的案例是我們可以使用 eBPF 來做 ssl 加密連線的監看。SSL/TSL 的連線加密通常是在 user space 應用程式內完成加密的，因此即便我們監看應用程式送入 kernal socket 的內容，內容也已經是被加密的了。但是要拆解應用程式來查看又相對比較複雜困難，使用 eBPF 就可以用一個相對簡單的方法來監看加密訊息。 在 Linux 上，應用程式的加密經常會使用 libssl 這個 library 來完成，並使用 libssl 提供的 SSL_read 和 SSL_write 取代 socket 的 read 和 write，透過 eBPF 的功能，我們可以比較簡單的直接監聽應用程式對這兩個函數的呼叫，並直接提取出未加密的連線內容。 Security: 前面有講到透過 eBFP，我們可以監控 system call 的呼叫、kernal 的執行、user space 程式的函數呼叫等等，因此我們也就可以透過 eBFP 來監控這些事件，並以此檢測程式的安全，拒絕非法的 system call 呼叫，或異常行為等等。 + 詳細可以參考 Tetragon 和 tracee 之類的專案。\n上面大概介紹了一些 eBFP 的應用場景，BPF 經過擴展之後，不再侷限於封包過濾這個場景，而在網路處理、内核追蹤、安全監控，等各個方面有了更多可以開發的潛能。\n參考資料 # The BSD Packet filter paper tcpdump man page tcpdump 与 libpcap 原理分析 Debugging with eBPF Part 3: Tracing SSL/TLS connections eBPF applications ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-1-abstract-and-background/","section":"文章","summary":"","title":"學習 eBPF 系列 1 - 摘要與簡介","type":"posts"},{"content":"本篇文章將會介紹包含 program type, map 等重要概念還有 eBPF 的載入流程\nProgram Type # 我們可以把 eBPF 程式區分成不同的 BPF program type，不同的 program type 代表實現不同功能的 eBPF 程式。通常不同的 program type 也會有不同 hook point，eBPF 程式的輸入和輸出格式也不同，也就影響到不同的 kernal 組件。\n到目前 Linux kernal 5.19 版，linux 總共定義了 32 種的 program type。在 linux kernal source code 的 include/uapi/linux/bpf.h 中定義了 bpf_prog_type 列舉，列舉了所有的 program type。\nenum bpf_prog_type { BPF_PROG_TYPE_UNSPEC, BPF_PROG_TYPE_SOCKET_FILTER, BPF_PROG_TYPE_KPROBE, BPF_PROG_TYPE_SCHED_CLS, BPF_PROG_TYPE_SCHED_ACT, BPF_PROG_TYPE_TRACEPOINT, BPF_PROG_TYPE_XDP, BPF_PROG_TYPE_PERF_EVENT, BPF_PROG_TYPE_CGROUP_SKB, BPF_PROG_TYPE_CGROUP_SOCK, ... }; 以 BPF_PROG_TYPE_XDP 為例，XDP 是 Express Data Path 的縮寫，XDP 程式會在封包從網路卡進入到 kernal 的最早期被觸發。\n一個 eBPF 程式大致上可以看成一個 c 的 function，在 XDP program type 下，kernal 會帶入 xdp_md 資料結構作為 eBPF 程式的輸入，包含了封包的內容、封包的來源介面等資訊。\n//include/uapi/linux/bpf.h /* user accessible metadata for XDP packet hook */ struct xdp_md { __u32 data; __u32 data_end; __u32 data_meta; /* Below access go through struct xdp_rxq_info */ __u32 ingress_ifindex; /* rxq-\u0026gt;dev-\u0026gt;ifindex */ __u32 rx_queue_index; /* rxq-\u0026gt;queue_index */ __u32 egress_ifindex; /* txq-\u0026gt;dev-\u0026gt;ifindex */ }; eBPF 程式必須回傳一個 xdp_action 的 enum，其中 XDP_PASS 表示封包可以繼續通過到 kernal network stack，XDP_DROP 表示直接丟棄該封包。\n//include/uapi/linux/bpf.h enum xdp_action { XDP_ABORTED = 0, XDP_DROP, XDP_PASS, XDP_TX, XDP_REDIRECT, }; 透過這樣的 eBPF 程式，我們就可以在封包剛進入 kernal 的時候直接丟棄非法封包，能夠比較高效的處理 DDos 攻擊等問題。\n以此可以寫出一個極簡單的 eBPF 程式範例 (只包含最主要的部份，完整的程式寫法會在後面提到)\nint xdp_prog_simple (struct xdp_md *ctx) { return XDP_DROP; } 這個 eBPF 程式可以被 attach 到某一個 interface 上，當封包進來時會被呼叫。由於無條件回傳 XDP_DROP，因此會丟棄所有的封包。\n使用流程 # eBPF 程式碼要被編譯成 eBPF 虛擬機的 bytecode 才能夠執行。 以 XDP 為例，最底層的做法是直接使用 LLVM 編譯這段 eBPF 程式碼。 首先需要補齊使用 LLVM 編譯時，需要的 header file 和資訊。\n#include \u0026lt;uapi/linux/bpf.h\u0026gt; SEC (\u0026#34;xdp_prog\u0026#34;) int xdp_program(struct xdp_md *ctx) { return XDP_DROP; } char _license [] SEC (\u0026#34;license\u0026#34;) = \u0026#34;GPL\u0026#34;; 接著使用 LLVM 編譯成 ELF 格式文件\nclang -c -target bpf xdp.c -o xdp.o 然後使用 bpf system call 將 bytecode 載入到 kernal 的 eBPF 虛擬機內，並取得對應的 file descriptor。最後透過 netlink socket 發送一個 NLA_F_NESTED | 43 訊息來把 interface index 與 ebpf 程式的 file descriptor 綁定。就能夠讓 eBPF 程式在對應的 interface 封包處理過程中被呼叫。\niproute2 有實作載入 eBPF 的功能，因此可以透過下指令\nip link set dev eth1 xdp xdp.o 可能會注意在程式碼的最後一行。特別標註了 GPL licence。由於 eBPF 程式會嵌入到 kernel，與 kernel 緊密的一起執行 (共用 address space、權限等)，在法律判斷獨立程式的邊界時，eBPF 程式和相關的 kernel 組件會被視為一體，因此 eBPF 程式會受到相關的 licence 限制。\n而這邊提到的內核組件指的是 eBPF helper function。helper function 是 eBPF 程式與 kernel 溝通的橋梁，由於 eBPF 程式是在 eBPF 虛擬機內執行，因此如果要取得 kernel 的額外資訊或改變 kernel 的行為，必須透過虛擬機提供的 helper function 接口。\n一部份的 helper function 基於 GPL 授權，因此當 eBPF 程式使用了 GPL 授權的 helper function 就必須標示為 GPL 授權，否則將 eBPF 程式載入到 kernel 時，會直接被 kernel 拒絕。\n直接使用最底層的方法開發相對來說是不方便和困難的，不同 program type 的載入方式可能還完全不一樣，因此許多抽象的框架和 SDK 被發出來。雖然還是需要編寫 eBPF 的 c code，但是編譯、載入、溝通等工作被包在 SDK 裡面，可以方便的直接使用。\n這邊舉例 BPF Compiler Collection (BCC) 這套工具，BCC 將 eBPF 的編譯和載入動作包裝成了 python 的 API，因此能夠簡單的完成 eBPF 的編譯和執行。\nfrom bcc import BPF import time b = BPF (text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/bpf.h\u0026gt; int xdp_prog1 (struct xdp_md *ctx) { return XDP_DROP; } \u0026#34;\u0026#34;\u0026#34;) fn = b.load_func (\u0026#34;xdp_prog1\u0026#34;, BPF.XDP) b.attach_xdp (\u0026#34;wlp2s0\u0026#34;, fn, 0) try: while True: time.sleep (1) except KeyboardInterrupt: pass b.remove_xdp (\u0026#34;wlp2s0\u0026#34;, 0) 使用條件 # 在開始玩 eBPF 之前，我們要先確定一下我們的環境能夠使用 eBPF。最早在 kernal 3.15 版加入了 eBPF 功能。後續在 3.15 到現在的 5.19 版間，eBPF 陸陸續續加入了許多新的功能，因此開發的時候，如果不是使用最新版的作業系統，就可能會需要確認一下版本是否支援，各個功能支援的版本可以在 這邊 參考\n另外就是 eBPF 的功能需要在編譯 kernel 的時候啟用，大部分的發行版應該都直接啟用了，不過如果使用時出現問題可能還是到 /proc/config.gz 或 /boot/config-\u0026lt;kernel-version\u0026gt; 檢查內核編譯的設定，是否有開啟 CONFIG_BPF, CONFIG_BPF_SYSCALL, CONFIG_BPF_JIT 還有其他 BPF 相關 Kernal 選項。 設定可以參考 bcc 的 安裝需求\n載入流程 # 當 eBPF 程式編譯完成後，就需要透過 bpf system call (原始碼)，將編譯後的 bytecode 載入 kernel 內執行。\n為了安全性考量，要掛載 eBPF 程式需要 root 權限或 CAP_BPF capability，不過目前也有在設計讓非 root 權限帳號能載入 eBPF 程式，因此將 kernel.unprivileged_bpf_disabled sysctl 設置為 false 的情況下，非 root 帳號是有能力能夠使用 BPF_PROG_TYPE_SOCKET_FILTER 的 eBPF 程式。\neBPF 程式需要嵌入到 kernel 執行，因此 eBPF 程式的安全性是極為重要的，也要避免 eBPF 程式的錯誤有可能會導致 kernel 崩潰或卡死，因此每個載入 kernel 的 eBPF 程式都要先經過接著 verifier 檢查。\n首先 eBPF 程式必須要在有限的時間內執行完成，不然就會造成 kernel 卡死，因此在早期的版本中 verifier 是拒絕任何 loop 的存在的，整個程式碼必須是一張 DAG (有向無環圖)。不過在 kernel 5.3 版本開始，verifier 允許了有限次數的循環，verifier 會透過模擬執行檢查 eBPF 是不是會在有限次數內在所有可能的分支上走到 bpf_exit。\n接著 eBPF 程式的大小也存在限制，早期只一個 eBPF 程式只允許 4096 個 ebpf vm 的 instruction s，在設計比較複雜的 eBPF 程式上有些捉襟見肘，因此後來在 5.2 版 這個限制被放寬成 1 million 個指令，基本上是十分夠用了，也還是能確保 ebpf 程式在 1/10 秒內執行完成。\n然後程式的 stack 也存在大小限制，目前限制是 512。\n當然 verifier 檢查的項目不只如此，前面提到 non-GPL licence eBPF 程式使用 GPL licence 的 helper function，也會被 verifier 拒絕，並收到一個 cannot call GPL-restricted function from non-GPL compatible program 的錯誤。\n此外 verifier 也會針對 helper function 的函數呼叫參數合法性，暫存器數值合法性，或其他無效的使用方式、無效的回傳數值、特定必須的資料結構是否定義、是否非法存取修改數據、無效的 instruction 參數等等做出檢查以及拒絕存在無法執行到的程式碼。\n具體的 verifier 可以參考 1 萬 5 千行的 原始碼\nJIT # 當 eBPF 程式通過 verifier 的驗證之後會進行 JIT (Just In Time) 的二次編譯。之前一直提到 eBPF 是一個執行在 kernel 內的虛擬機，因此編譯出來的 bytecode 需要再執行的過程中在轉換成 machine code，才能夠真正在 CPU 上面執行，然而這樣的虛擬化和轉換過程，會造成 eBPF 程式的執行效率，比直接執行 machine code 要低上很多。\n因此 eBPF 加入了 JIT 的功能，簡單來說就是把 eBPF 的 bytecode 預先在載入的時候，直接編譯成 CPU 可執行的 machine code，在執行 eBPF 程式的時候就可以直接執行，而不用再經過 eBPF 虛擬機的轉換，使 eBPF 可以達到原生程式的執行效率。\n由於 JIT 需要編譯出 machine code，因此針對不同的 CPU 平台他的支援是分開的，不過當然到了現在，基本上大部分主流的 CPU 架構 (x86, ARM, RISC, MIPS…) 都已經支援了，具體的支援情況可以參考這張 表。\n同樣 JIT 是 eBPF 的一個可開關的獨立功能，透過設置 bpf_jit_enable 來啟用 JIT 的功能\nsystcl -w net.core.bpf_jit_enable=1 (設置為 2 的話，可以在 kernel log 看到相關日誌)\n到此 eBPF 程式就就完成載入了，雖然在 eBPF 程式的載入過程中還會完成一些資料結構的建立和維護，但是這個部分就不再本文的範圍內了。\n當然到此 eBPF 程式只是載入到了內核之中，並未連接到任何的 hook point，因此到此為 eBPF 程式還未能真正被執行，不過這就是後面的故事了。\n從 kernel source code 來看，在 eBPF 程式載入的過程中會呼叫 bpf_prog_select_runtime 來判斷是否要呼叫 JIT compiler 去編譯，有興趣可以去 trace 這部分的 code。\n生命週期 # 在透過 bpf (BPF_PROG_LOAD, ...) system call 將 eBPF 程式載入內核的過程 (可以參考 原始碼)，會替該 eBPF 程式建立 struct bpf_prog 結構，其中 prog-\u0026gt;aux-\u0026gt;refcnt 計數器記錄了該 eBPF 程式的參考數量，載入的時候會透過 atomic64_set (\u0026amp;prog-\u0026gt;aux-\u0026gt;refcnt, 1); 將 refcnt 設置為一，並返為對應的 file descriptor。\n當 refcnt 降為 0 的時候，就會觸發 unload，將 eBPF 程式資源給釋放掉。(原始碼) 因此如果呼叫 BPF_PROG_LOAD 的程式沒有進一步操作，並直接結束的話，當 file descriptor 被 release，就會觸發 refcnt–，而變成 0 並移除 eBPF 程式。\n要增加 eBPF 程式 refcnf 大致上有幾種方式\n透過 bpf systemcall 的 BPF_BTF_GET_FD_BY_ID 等方式取得 eBPF 程式對應的 file descriptor 將 eBPF 程式 attach 到事件、Link 上，使 eBPF 程式能真的開始工作。 因此當 eBPF 被 attach 到 hook points 上之後，即便原始載入程式結束也不會導致 eBPF 程式被回收，而可以正常繼續工作。 Link 是 eBPF 後來提供的新特性，因此暫時超出了本文的討論範圍 透過 bpf systemcall 的 BPF_OBJ_PIN，將 eBPF 程式釘到 BPFFS 上。 BPFFS 是 BPF file system，本質上是一個虛擬的檔案系統，一樣透過 bpf system call 的 BPF_OBJ_PIN，我們可以把 eBPF 程式放到 /sys/fs/bpf/ 路徑下的指定位置，並透過 open 的方式直接取得 file descriptor。PIN 同樣會增加 refcnt，因此 PIN 住的程式不會被回收 要釋放 PIN 住的程式，可以使用 unlink 指令移除虛擬檔案，即可取消 PIN。 透過以上的操作都會增加 refcnt，相反的，對應的資源釋放則會減少 refcnt。因此只要確保有任何一個 eBPF 程式的參考存在，即可保證 eBPF 程式一直存在 kernel 內。\nHelper Funtions # 之前在介紹 eBPF 的 GPL 授權的時候，有提到 eBPF helper function 這個東西，接下來我們來比較仔細的介紹一下。\n之前提到 eBPF 程式是在 eBPF 虛擬機內執行，由於 eBPF 程式會嵌入 kernel 內，在 kernel space 執行，所以為了安全性考量我們不能讓 eBPF 程式任意的存取和修改 kernel 記憶體和呼叫 kernel 函數，因此 eBPF 的解決方案是提供了一系列的 API，讓 eBPF 程式只能夠過限定的 API 去與 kernel 溝通，因此可以讓 eBPF 程式對 kernel 的操作限制在一個可控的範圍，也可以透過 verifier 和 API 後面的實作去確保 API 呼叫的有效和安全性。\n在 eBPF 裡這一系列的 API 就稱之為 eBPF helper funtions。\n另外不同的對於 eBPF program type 的 eBPF 程式，由於他們執行的時機點和在 kernel 的位置不同，因此他們能夠取得的 kernel 資訊也就不同，他們可以呼叫執行的 helper funtions 也就不同。具體每個不同 program type 可以執行的 helper function 可以參考 bcc 的 文件\n下面列幾舉個所有 program type 都可以呼叫的 helper function\nu64 bpf_ktime_get_ns (void) 取得從開機開始到現在的時間，單位是奈秒 u32 bpf_get_prandom_u32 (void) 取得一個 random number 接著我們舉例在 BPF_PROG_TYPE_SOCKET_FILTER 下才能使用的 helper function\nlong bpf_skb_load_bytes (const void *skb, u32 offset, void *to, u32 len) 由於 socket filter 的功能就是對 socket 的流量做過濾，因此我們可以透過 skb_load_bytes 來取得 socket 傳輸的封包內容 完整的 helper function 列表還有每個函數具體的定義以及使用說明描述可以在 bpf.h 查找到。\n另外特別要注意的是受限於 eBPF 虛擬機的限制，eBPF helper function 的參數數量最多只可以有五個，在使用不定參數長度的參數時，最多也只能有 5 個參數 (如之後會提到的 trace_printk)。\n因此雖然 eBPF 非常強大能夠非常方便的動態對 kernel 做修改，但為了安全，他可以執行的操作是訂定在一個非常嚴格的框架上的，在開發時需要熟習整個框架的限制和可利用的 API 資源。\nDebug Tracing # 在將 eBPF 程式載入 kernel 工作後，我們勢必需要一些手段來與 eBPF 程式做溝通，一方面我們需要輸出偵錯訊息，來對 eBPF 程式 debug，一方面我們可能會希望能夠實時透過 eBPF 程式取得 kernel 的某些資訊又或著動態調整 eBPF 程式的執行規則。\n如果只是需要 eBPF 程式單方面的輸出訊息，讓我們可以偵錯，可以使用比較簡單的手段。eBPF 有提供一個 helper function long bpf_trace_printk (const char *fmt, u32 fmt_size, ...)，可以輸入一個格式化字串 fmt，及最多三個變數 (參數個數的限制)。輸出結果會被輸出到 /sys/kernel/debug/tracing/trace_pipe 中。.\n可以透過指令查看輸出結果：\nsudo cat /sys/kernel/debug/tracing/trace_pipe 輸出的格式如下：\ntelnet-470 [001] .N.. 419421.045894: 0x00000001: \u0026lt;formatted msg\u0026gt; 首先是 process name telnet，然後是 PID 470。 接續的 001 是指當前執行的 CPU 編號。 .N.. 的每個字元對應到一個參數 irqs 中斷是否啟用 TIF_NEED_RESCHED 和 PREEMPT_NEED_RESCHED 是否設置 (用於 kernel process scheduling) 硬中断 / 軟中断是否發生中 level of preempt_disabled 419421.045894 時間 0x00000001: eBPF 內部的指令暫存器數值 雖然 trace_printk 可以接收格式化字串，但是支援的格式字元比較少，只支援 % d, % i, % u, % x, % ld, % li, % lu, % lx, % lld, % lli, % llu, % llx, % p, % s。\n另外有一個 bpf_printk 巨集，會使用 sizeof (fmt) 幫忙填上第二個 fmt_size。因此使用 bpf_printk 可以省略 fmt_size。\n在比較新的版本提供了 bpf_snprintf 和 bpf_seq_printf 兩個新的 print 函數，前者是把資料寫入預先建立好的 buffer 內，後者可以寫入在特定 program type 下可以取得的 seg_file，兩者皆用陣列存放後面的參數列，因此可以打破 helper funtion 5 個參數的限制。\n最後要特別注意的是使用 trace_printk 會大幅拖慢 eBPF 程式的執行效率，所以 trace_printk 只適用於開發時用來 debug 使用，不適用於正式環境當中。\nMap # 接著介紹 eBPF 的另外一個重要組件 map，前面提到 trace_printk 只適合用在除錯階段，輸出 eBPF 的執行資訊到 user space，然而我們需要一個可以在正式環境內，提供 user space 程式和 eBPF 程式之間雙向數據交換的能力，另外每次觸發 eBPF 程式都可看作獨立執行 eBPF 程式，所以也需要在多次呼叫 eBPF 程式時共享資料的功能。因此 eBPF 程式引入了 map。\neBPF map 定義了一系列不同的不同的資料結構類型，包含了 hash, array, LRU hash, ring buffer, queue 等等，另外也提供 per-cpu hash, per-cpu array 等資料結構，由於每顆 CPU 可以獲得獨立的 map，因此可以減少 lock 的需求，提高執行效能。所有的 map type 一樣可以參考 bpf.h 的 enum bpf_map_type。\nstruct bpf_map_def SEC (\u0026#34;maps\u0026#34;) map = { .type = BPF_MAP_TYPE_ARRAY, .key_size = sizeof (int), .value_size = sizeof (__u32), .max_entries = 4096, }; 首先要先在 eBPF 程式內定義 map 的資料結構，在 eBPF 程式內定義一個 map 時，基本需要定義四個東西分別是該資料結構的 map type, key 和 value 的大小以及資料結構內最多有含多少 entry，如果超出 max_entries 上限則會發生錯誤回傳 (-E2BIG)。\neBPF 提供了 bpf_map_lookup_elem, bpf_map_update_elem, bpf_map_delete_elem 等 helper functions 來對 map 資料做操作。lookup 的完整定義是 void *bpf_map_lookup_elem (struct bpf_map *map, const void *key)，透過 key 去尋找 map 裡面對應的 value，並返回其指標，由於返回的是指標，所以會指向 map 真實儲存的記憶體，可以直接對其值進行更新。\n當然除了幾個基本的 helper function 外，不同的 map type 可能會支援更多的操作或功能，例如 bpf_skb_under_cgroup 是給 BPF_MAP_TYPE_CGROUP_ARRAY 專用的。\n原始碼解析 # linux kernel 定義了 struct bpf_map_ops，來描述 map 可能會支援的所有功能。\nstruct bpf_map_ops { /* funcs callable from userspace (via syscall) */ int (*map_alloc_check)(union bpf_attr *attr); struct bpf_map *(*map_alloc)(union bpf_attr *attr); void (*map_release)(struct bpf_map *map, struct file *map_file); void (*map_free)(struct bpf_map *map); int (*map_get_next_key)(struct bpf_map *map, void *key, void *next_key); void (*map_release_uref)(struct bpf_map *map); void *(*map_lookup_elem_sys_only)(struct bpf_map *map, void *key); ... } 不同的 map 再根據需要去實作對應的操作，在 include/linux/bpf_types.h 定義。以 BPF_MAP_TYPE_QUEUE 這個 map type 來說對應到 queue_map_ops。\n//kernel/bpf/queue_stack_maps.c const struct bpf_map_ops queue_map_ops = { .map_meta_equal = bpf_map_meta_equal, .map_alloc_check = queue_stack_map_alloc_check, .map_alloc = queue_stack_map_alloc, .map_free = queue_stack_map_free, .map_lookup_elem = queue_stack_map_lookup_elem, .map_update_elem = queue_stack_map_update_elem, .map_delete_elem = queue_stack_map_delete_elem, .map_push_elem = queue_stack_map_push_elem, .map_pop_elem = queue_map_pop_elem, .map_peek_elem = queue_map_peek_elem, .map_get_next_key = queue_stack_map_get_next_key, .map_btf_name = \u0026#34;bpf_queue_stack\u0026#34;, .map_btf_id = \u0026amp;queue_map_btf_id, }; 當呼叫 bpf_map_push_elem 時，就會呼叫 bpf_map_ops.map_push_elem 來調用 queue 的 queue_stack_map_push_elem 完成。\n而具體每個 map 支援什麼 help function 可能就要參考 helper function 文件描述\n使用範例 # 這邊我們一個特別的使用實例來看\nstruct elem { int cnt; struct bpf_spin_lock lock; }; struct bpf_map_def SEC(\u0026#34;maps\u0026#34;) counter = { .type = BPF_MAP_TYPE_ARRAY, .key_size = sizeof(int), .value_size = sizeof(elem), .max_entries = 1, }; 首先我們定義了一個特別的 ARRAY map，它的 array size 只有 1，然後 value 是一個包含 u32 整數和一個 lock 的資料結構。\nSEC (\u0026#34;kprobe/sys_clone\u0026#34;) int hello_world(void *ctx) { u32 key = 0; elem *val; val = bpf_map_lookup_elem (\u0026amp;counter, \u0026amp;key); bpf_spin_lock (\u0026amp;val-\u0026gt;lock); val-\u0026gt;cnt++; bpf_spin_unlock (\u0026amp;val-\u0026gt;lock); bpf_trace_printk (\u0026#34;sys_clone count: % d\u0026#34;, val-\u0026gt;cnt); return 0; } 由於 key 我們固定是 0，透過 bpf_map_lookup_elem 我們永遠會取得同一筆資料，因此可以簡單看成我們把 counter 當作一個單一的容器來存放 cnt 變數，並使用 lock 避免 cnt 更新時的 race condition。\n我們將這個程式附加到 kprobe/sys_clone，就可以用來統計 sys_clone 呼叫的次數。\n和其他 eBPF 的操作一樣，我們透過 bpf 的 system call 去與 kernel 進行溝通。跟 helper fuction 類似，bpf systemcall 提供了 BPF_MAP_LOOKUP_ELEM, BPF_MAP_UPDATE_ELEM, BPF_MAP_DELETE_ELEM 等參數來提供搜尋、更新、刪除 map 數值的方法。另外為了減少 system call 的開銷，也提供 BPF_MAP_LOOKUP_BATCH, BPF_MAP_LOOKUP_AND_DELETE_BATCH, BPF_MAP_UPDATE_BATCH, BPF_MAP_DELETE_BATCH 等方法來在單次 system call 內完成多次 map 操作。\n必要要注意的是 map 並不是 eBPF program 的附屬品，在 eBPF 虛擬機內，map 和 program 一樣是獨立的物件，每個 map 有自己的 refcnt 和生命週期，eBPF 程式的生命週期和 map 不一定是統一的。\nmap 載入流程 # 在透過函式庫將 eBPF 程式載入 kernel 時，先做的其實是建立 map，對每張 map 會呼叫 bpf system call 的 BPF_MAP_CREATE，並帶入 map type, key size, value size, max entries, flags 等資訊來建立 map，建立完成後會返回 map 對應的 fire descripter。\n接著函數庫會修改編譯過的 ebpf bytecode 裡面參考到 map 變數的地方 (例如 lookup 等 helper function 的參數部分)，將原先流空的 map 地址修改成 map 對應的 file descripter。\n接著一樣呼叫 bpf BPF_PROG_LOAD 來載入 eBPF bytecode，在載入過程中，verifier 會呼叫到 replace_map_fd_with_map_ptr 函數，將 bytecode 裡面 map 的 file descripter 在替換成 map 的實際地址。\nMap 持久化 # 如前面所述，map 在 eBPF 虛擬機內和 prog 同等是獨立的存在，並且具有自己的 refcnt，因此和 prog 一樣，我們可以透過 bpf BPF_OBJ_PIN 將 map 釘到 BPFFS 的 /sys/fs/bpf/ 路徑下，其他程式就一樣能透過 open file 的方式取得 map 的 file descripter，將 map 載入到其他的 eBPF 程式內，達成了多個 eBPF 程式 share 同一個 map 的效果。\nTail call # 最後我們要來聊的是 tail call 的功能。\ntail call 簡單來說就是在 eBPF 程式內執行另外一個 eBPF 程式，不過和一般的函數呼叫不一樣，eBPF 虛擬機在跳轉到另外一個 eBPF 程式後就不會再回到前一個程式了，所以他是一個單向的呼叫。\n另外雖然他會直接複用前一個 eBPF 程式的 stack frame，但是被呼叫的 eBPF 程式不能夠存取前呼叫者的暫存器和 stack，只能透取得在呼叫 tail call 時，透過參數傳遞的 ctx。\n使用 tail call 可以透過拆解簡化一個 eBPF 程式，打破單個 eBPF 程式只能有 512bytes 的 stack、1 million 個指令的限制。\n一個使用範例是先使用一個 eBPF 程式作為 packet dispatcher，然後根據不同的 packet ether type 之類的欄位，將 packet 轉發給對應處理的 eBPF 程式。\n另外一個就是將 eBPF 程式視為多個模組，透過 map 和 tail call 去動態的任意重整排序執行結構。\n為了避免 eBPF 程式交替呼叫彼此導致卡死的狀況，kernel 定義了 MAX_TAIL_CALL_CNT 表示在單個 context 下最多可呼叫的 tail call 次數，目前是 32。如果 tail call 因為任何原因而執行失敗，則會繼續執行原本的 eBPF 程式。\n如何使用 # tail call 的 helper function 定義如下 long bpf_tail_call (void *ctx, struct bpf_map *prog_array_map, u32 index)。在使用的時候我們要一個 BPF_MAP_TYPE_PROG_ARRAY type 的 map，用來保存一個 eBPF program file descriptor 的陣列。在呼叫 tail call 的時候傳遞進去執行。\n參考資料 # BPF: A Tour of Program Types BPF 程序（BPF Prog）类型详解 Lifetime of BPF objects difference between loading, attaching, and linking? eBPF map BPF Map 内核实现 BPF 环形缓冲区 BPF 技术介绍及学习分享 揭秘 BPF map 前生今世 BPF 数据传递的桥梁 ——BPF MAP（一） bpf-helpers man page introduce bpf_tail_call () helper 從 BPF to BPF Calls 到 Tail Calls ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-2-basic-concept/","section":"文章","summary":"","title":"學習 eBPF 系列 2 - 基本概念","type":"posts"},{"content":"BPF Compiler Collection (BCC) 是一套用於 eBPF，用來有效開發 kernel 追蹤修改程式的工具集。\n介紹 # BCC 我覺得可以看成兩個部分：\neBPF 的 python 和 lua 的前端，透過 BCC 我們可以使用 python 和 lua 比較簡單的開發 eBPF 的應用程式，BCC 將 bpf system call 還有 eBPC 程式編譯封裝成了 API，並提供一系列預先定義好的巨集和語法來簡化 eBPF 程式。 from bcc import BPF b = BPF (text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/bpf.h\u0026gt; int xdp_prog1 (struct xdp_md *ctx) { return XDP_DROP; } \u0026#34;\u0026#34;\u0026#34; fn = b.load_func (\u0026#34;xdp_prog1\u0026#34;, BPF.XDP) b.attach_xdp (\u0026#34;eth0\u0026#34;, fn, 0) 以上面的範例來說，透過 BPF 物件實立化時會完成 eBPC bytecode 的編譯，然後透過 load_func 和 attach_xdp 就可以很簡單的將上面的 eBPF 程式碼編譯載入到 kernel 然後 attach 到 xdp 的 hook point 上。\n一系列使用自身框架開發的工具\nBCC 使用自己的 API 開發了一系列可以直接使用的現成 bcc eBPF 程式，本身就幾乎涵蓋了 eBPF 的所有 program type，可以開箱即用，直接跳過 eBPF 的開發。\n下圖包含了 BCC 對 linux kernel 各個模組實現的工具名稱\neBPC 本身和 bcc 相關的開發文件以及範例程式\n可以看到前面很多天有參考到 BCC 的文件，資料非常地豐富\n安裝 # 首先 bcc 的安裝大概有幾種方式\n透過各大發行板的套件管理工具安裝 直接使用原始碼編譯安裝 透過 docker image 執行對於前兩著，bcc 官方的文件列舉了需多發行版的 安裝方式，所以可以很容易地照著官方文件安裝。以 ubuntu 來說，可以透過 Universe 或 iovisor 的 repo 安裝。然而必須要注意的是，目前 iovisor 和 universe 上面的 bcc 套件本的都比較陳舊，甚至沒有 20.04 和 22.04 對應的安裝源，因此透過 apt 安裝可能會出現版本不支援或安裝後連範例都跑不起來的問題。 # use Universe # add-apt-repository universe # iovisor sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \u0026#34;deb [trusted=yes] https://repo.iovisor.org/apt/xenial xenial-nightly main\u0026#34; | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r) 因此特別建議透過原始碼來安裝會是比較穩妥的方式。一樣在 bcc 的的 安裝文檔 詳細列舉了在各個發行版本的各個版本下，要怎麼去安裝相依套件，然後編譯安裝 bcc。\nsudo apt install -y bison build-essential cmake flex git libedit-dev \\ libllvm12 llvm-12-dev libclang-12-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils git clone https://github.com/iovisor/bcc.git mkdir bcc/build; cd bcc/build cmake .. make sudo make install cmake -DPYTHON_CMD=python3 .. # build python3 binding pushd src/python/ make sudo make install popd 這邊同樣以 ubuntu 舉例，首先因為 BCC 後端還是使用 LLVM，因此需要先安裝 llvm 以及 bcc 編譯需要的 cmake 等工具，然後後過 cmake 編譯安裝。\n安裝完成後，昨天提到的 bcc 自己寫好的 kernel trace tools 會被安裝到 /usr/share/bcc/tools，因此可以直接 cd 到該目錄來玩，由於這些 tools 其實就是 python script，所以其實也可以直接透過 python3 執行 bcc repo 下 tools 目錄內的 python 檔，其結果其實是一樣的。\n同樣的還有 examples 這個資料夾下的範例也會被安裝到 /usr/share/bcc/examples 目錄下。\n最後是透過 docker 的方式執行 bcc。同樣參考 bcc 的 quickstart 文件，不過加上 --pid=host\ndocker run -it --rm \\ --pid=host \\ --privileged \\ -v /lib/modules:/lib/modules:ro \\ -v /usr/src:/usr/src:ro \\ -v /etc/localtime:/etc/localtime:ro \\ --workdir /usr/share/bcc/tools \\ zlim/bcc 但是不論是直接使用 zlim/bcc 還是透過 bcc repo 內的 dockerfile 自行編譯，目前測試起來還是有許多問題，使用 zlim/bcc 在執行部分的 eBPF 程式時會編譯失敗，直接透過 dockerfile 編譯初步測試也沒辦法 build 成功，因此目前自行編譯使用可能還是相對比較穩定簡單快速的方式。\n","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-3-introduction-to-bcc/","section":"文章","summary":"","title":"學習 eBPF 系列 3 - BCC 介紹","type":"posts"},{"content":"我們要來看 BCC 的 tools/tcpconnect.py 這支程式。原始碼在 這邊。\ntcpconnect 介紹 # 這隻程式會追蹤紀錄 kernel 發起的 TCP 連線，可以看到發起連線的 pid, 指令名稱，ip version, IP 地址和目標 port 等資訊。\n執行結果如下：\npython3 tools/tcpconnect Tracing connect ... Hit Ctrl-C to end PID COMM IP SADDR DADDR DPORT 2553 ssh 4 10.0.2.15 10.0.2.1 22 2555 wget 4 10.0.2.15 172.217.160.100 80 tcpconnect 實作 # 首先透過 argparse 定義了指令的參數輸入，主要是提供 filter 的選項，讓使用者可以透過 pid, uid, namespace 等參數去 filter 連線紀錄。\nparser = argparse.ArgumentParser ( description=\u0026#34;Trace TCP connects\u0026#34;, formatter_class=argparse.RawDescriptionHelpFormatter, epilog=examples) parser.add_argument (\u0026#34;-p\u0026#34;, \u0026#34;--pid\u0026#34;, help=\u0026#34;trace this PID only\u0026#34;) ... args = parser.parse_args () 接著就來到主要的 eBPF 程式碼的定義\nbpf_text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;net/sock.h\u0026gt; #include \u0026lt;bcc/proto.h\u0026gt; BPF_HASH (currsock, u32, struct sock *); ... 首先可以看到 BPF_HASH，這是 BCC 提供的一個巨集，用來定一個 hash type 的 map，對於不同 map type BCC 都定義了對應的巨集來建立 map。具體列表可以參考 這邊。第一個參數是 map 的名稱，這邊叫做 currsock，同時這個變數也用於後續程式碼中對 map 的參考和 API 呼叫，例如 currsock.lookup (\u0026amp;tid); 就是對 currsock 這個 map 進行 lookup 操作。接著兩個欄位分別對應 key 和 value 的 type，key 是一個 32 位元整數，value 則對應到 sock struct 指標。sock 結構在 net/sock.h 內定義，是 linux kernel 用來維護 socket 的資料結構。\nstruct ipv4_data_t { u64 ts_us; u32 pid; u32 uid; u32 saddr; u32 daddr; u64 ip; u16 lport; u16 dport; char task [TASK_COMM_LEN]; }; BPF_PERF_OUTPUT (ipv4_events); struct ipv6_data_t { ... 接著分別針對 ipv4 和 ipv6 定義了一個 data_t 的資料結構，用於 bpf 和 userspace client 之間傳輸 tcp connect 的資訊用。\n這邊可以看到另外一個特別的巨集 BPF_PERF_OUTPUT。這邊用到了 eBPF 提供的 perf event 機制，定義了一個 per-CPU 的 event ring buffer，並提供了對應的 bpf_perf_event_output helper function 來把資料推進 ring buffer 給 userspace 存取。在 bcc 這邊則使用 ipv4_events.perf_submit (ctx, \u0026amp;data, sizeof (data)); 的 API 來傳輸資料。\n//separate flow keys per address family struct ipv4_flow_key_t { u32 saddr; u32 daddr; u16 dport; }; BPF_HASH (ipv4_count, struct ipv4_flow_key_t); 接著又是一個 HASH map，tcpdconnect 提供一個功能選項是統計各種 connection 的次數，所以這邊定義了一個 ipv4_flow_key_t 當作 key 來作為統計依據，BPF_HASH 在預設情況下 value 的 type 是 u64，一個 64 位元無號整數，因此可以直接拿來統計。\n接著就來到了 bpf 函數主體，這個函數會被 attach 到 tcp_v4_connect 和 tcp_v6_connect 的 kprobe 上，當呼叫 tcp_v4_connect 和 tcp_v6_connect 時被觸發。\nint trace_connect_entry(struct pt_regs *ctx, struct sock *sk) { if (container_should_be_filtered ()) { return 0; } u64 pid_tgid = bpf_get_current_pid_tgid (); u32 pid = pid_tgid \u0026gt;\u0026gt; 32; u32 tid = pid_tgid; FILTER_PID u32 uid = bpf_get_current_uid_gid (); FILTER_UID //stash the sock ptr for lookup on return currsock.update (\u0026amp;tid, \u0026amp;sk); return 0; }; 首先它接收的參數是 pt_regs 結構和 tcp_v4_connect 的參數，pt_regs 包含了 CPU 佔存器的數值資訊，作為 eBPF 的上下文。後面 tcp_v4_connect 的第一個參數 sock 結構對應到當次連線的 socket 資訊，由於後面幾個參數不會使用到所以可以省略掉。\n./tcpconnect --cgroupmap mappath # only trace cgroups in this BPF map ./tcpconnect --mntnsmap mappath # only trace mount namespaces in the map 首先呼叫的是 container_should_be_filtered。在 argparser 中定義了兩個參數 cgroupmap 和 mntnsmap 用來針對特定的 cgroups 或 mount namespace。container_should_be_filtered 則會負責這兩項的檢查。\n一開始看可能會發現在 eBPF 程式裡面找不到這個函數定的定義，由於這兩個 filter 非常常用因此 bcc 定義了 bcc.containers.filter_by_containers函數，在 python 程式碼裡面會看到，bpf_text = filter_by_containers (args) + bpf_text。以 cgroup 來說，如果使用者有提供 cgroupmap 這個參數，filter_by_containers 會在 mappath 透過 BPF_TABLE_PINNED 在 BPFFS 建立一個 hash type 的 map，根據這個 map 的 key 來 filter cgroup id，透過 bpf_get_current_cgroup_id () 取得當前上下文的 cgroup_id 並只保留有在 map 內的上下文。\n接著 FILTER_PID 和 FILTER_UID 分別是針對 pid 和 uid 去 filter，在後面的 python 程式碼中會根據是否有啟用這個選項來把字串替代成對應的程式碼或空字串\nif args.pid: bpf_text = bpf_text.replace (\u0026#39;FILTER_PID\u0026#39;, \u0026#39;if (pid != % s) { return 0; }\u0026#39; % args.pid) bpf_text = bpf_text.replace (\u0026#39;FILTER_PID\u0026#39;, \u0026#39;\u0026#39;) 如果一切都滿足，就會使用 tid 當 key，將 sock 結構更新到 currsock map 當中。\n後半部分的 eBPG 程式碼定義了 trace_connect_return，這個函數會被 attach 到 tcp_v4_connect 和 tcp_v6_connect 的 kretprobe 上。kprobe 是在函數被呼叫時被觸發，kretprobe 則是在函數回傳時被觸發，因此可以取得函數的回傳值和執行結果。\nint trace_connect_v4_return(struct pt_regs *ctx) { return trace_connect_return (ctx, 4); } 真正的進入點分成 ip v4 和 v6 的版本來傳入 ipver 變數。\nstatic int trace_connect_return(struct pt_regs *ctx, short ipver) { int ret = PT_REGS_RC (ctx); u64 pid_tgid = bpf_get_current_pid_tgid (); u32 pid = pid_tgid \u0026gt;\u0026gt; 32; u32 tid = pid_tgid; struct sock **skpp; skpp = currsock.lookup (\u0026amp;tid); if (skpp == 0) { return 0; //missed entry } if (ret != 0) { //failed to send SYNC packet, may not have populated //socket __sk_common.{skc_rcv_saddr, ...} currsock.delete (\u0026amp;tid); return 0; } //pull in details struct sock *skp = *skpp; u16 lport = skp-\u0026gt;__sk_common.skc_num; u16 dport = skp-\u0026gt;__sk_common.skc_dport; FILTER_PORT FILTER_FAMILY if (ipver == 4) { IPV4_CODE } else /* 6 */ { IPV6_CODE } currsock.delete (\u0026amp;tid); return 0; } 透過 PT_REGS_RC 可以取得函數的回傳值，根據函數的定義，如果執行成功應該要回傳 0 所以如果 ret 不為零，表示執行錯誤，直接忽略。透過 currsock.lookup 我們可以取回對應 tid 的 sock 指標，然後取得 dst port 和 src port (lport)，由於這時候 tcp_connect 已經執行完成，所以 src port 已經被 kernel 分配。\n這邊可以看到 eBPF 程式設計上比較複雜的地方，sock 結構體要在 kprobe 取得，但是我們又需要 kretprobe 後的一些資訊，因此整個架構要被拆成兩個部分，然後透過 map 來進行傳輸。\n接著 FILTER_PORT 和 FILTER_FAMILY 一樣會被替換，然後根據 dst port 和 family 來 filter。\n由於 tcpconnect 有紀錄和統計連線次數兩種模式，因此最後一段的 code 一樣先被標記成 IPV4_CODE。然後根據模式的不同來取代成不同的 code。\nif args.count: bpf_text = bpf_text.replace (\u0026#34;IPV4_CODE\u0026#34;, struct_init [\u0026#39;ipv4\u0026#39;][\u0026#39;count\u0026#39;]) bpf_text = bpf_text.replace (\u0026#34;IPV6_CODE\u0026#34;, struct_init [\u0026#39;ipv6\u0026#39;][\u0026#39;count\u0026#39;]) else: bpf_text = bpf_text.replace (\u0026#34;IPV4_CODE\u0026#34;, struct_init [\u0026#39;ipv4\u0026#39;][\u0026#39;trace\u0026#39;]) bpf_text = bpf_text.replace (\u0026#34;IPV6_CODE\u0026#34;, struct_init [\u0026#39;ipv6\u0026#39;][\u0026#39;trace\u0026#39;]) 我們這邊就只看 ipv4 trace 的版本。\nstruct ipv4_data_t data4 = {.pid = pid, .ip = ipver}; data4.uid = bpf_get_current_uid_gid (); data4.ts_us = bpf_ktime_get_ns () / 1000; data4.saddr = skp-\u0026gt;__sk_common.skc_rcv_saddr; data4.daddr = skp-\u0026gt;__sk_common.skc_daddr; data4.lport = lport; data4.dport = ntohs (dport); bpf_get_current_comm (\u0026amp;data4.task, sizeof(data4.task)); ipv4_events.perf_submit (ctx, \u0026amp;data4, sizeof(data4)); 這邊其實就是去填充 ipv4_data_t 結構、透過 bpf_get_current_comm 取得當前程式的名稱，最後透過前面透過 BPP_PERF_OUT 定義的 ipv4_events，呼叫 perf_submit (ctx, \u0026amp;data4, sizeof (data4)) 將資料送到 user space。\n到這邊就完成了整個的 eBPF 程式碼 bpf_text 的定義，後面就會先經過前面講的，將 IPV4_CODE 等字段，根據 tcpconnect 的參數進行取代。\nb = BPF (text=bpf_text) b.attach_kprobe (event=\u0026#34;tcp_v4_connect\u0026#34;, fn_name=\u0026#34;trace_connect_entry\u0026#34;) b.attach_kprobe (event=\u0026#34;tcp_v6_connect\u0026#34;, fn_name=\u0026#34;trace_connect_entry\u0026#34;) b.attach_kretprobe (event=\u0026#34;tcp_v4_connect\u0026#34;, fn_name=\u0026#34;trace_connect_v4_return\u0026#34;) b.attach_kretprobe (event=\u0026#34;tcp_v6_connect\u0026#34;, fn_name=\u0026#34;trace_connect_v6_return\u0026#34;) 接著透過 BCC 的 library 完成 eBPF 程式碼的編譯、載入和 attach。\n最後是輸出的部分，前面會先輸出一些下列的欄位資訊，但是由於這不是很重要所以就省略掉。\nTracing connect ... Hit Ctrl-C to end PID COMM IP SADDR DADDR DPORT b = BPF (text=bpf_text) ... # read events b [\u0026#34;ipv4_events\u0026#34;].open_perf_buffer (print_ipv4_event) b [\u0026#34;ipv6_events\u0026#34;].open_perf_buffer (print_ipv6_event) while True: try: b.perf_buffer_poll () except KeyboardInterrupt: exit () 完成載入後，我們可以拿到一個對應的 BPF 物件，透過 b[MAP_NAME]，我們可以調用 map 對應的 open_perf_bufferAPI，透過 open_perf_buffer，我們可以定義一個 callback function 當有資料從 kernel 透過 perf_submit 被傳輸的時候被呼叫來處理 eBPF 程式送過來的資料。\n最後會呼叫 b.perf_buffer_poll 來持續檢查 perf map 是不是有新的 perf event，以及呼叫對應的 callback function。\ndef print_ipv4_event(cpu, data, size): event = b [\u0026#34;ipv4_events\u0026#34;].event (data) global start_ts if args.timestamp: if start_ts == 0: start_ts = event.ts_us printb (b\u0026#34;%-9.3f\u0026#34; % ((float(event.ts_us) - start_ts) / 1000000), nl=\u0026#34;\u0026#34;) if args.print_uid: printb (b\u0026#34;%-6d\u0026#34; % event.uid, nl=\u0026#34;\u0026#34;) dest_ip = inet_ntop (AF_INET, pack (\u0026#34;I\u0026#34;, event.daddr)).encode () if args.lport: printb (b\u0026#34;%-7d %-12.12s %-2d %-16s %-6d %-16s %-6d % s\u0026#34; % (event.pid, event.task, event.ip, inet_ntop (AF_INET, pack (\u0026#34;I\u0026#34;, event.saddr)).encode (), event.lport, dest_ip, event.dport, print_dns (dest_ip))) else: printb (b\u0026#34;%-7d %-12.12s %-2d %-16s %-16s %-6d % s\u0026#34; % (event.pid, event.task, event.ip, inet_ntop (AF_INET, pack (\u0026#34;I\u0026#34;, event.saddr)).encode (), dest_ip, event.dport, print_dns (dest_ip))) x 透過 b [\u0026quot;ipv4_events\u0026quot;].event 可以直接將 data 數據轉換成 BPF 程式內定義的資料結構，方便存取。取得的資料再經過一些清洗和轉譯就能夠直接輸出了。\n雖然我們跳過了 count 功能還有一個紀錄 dst ip 的 DNS 查詢，但到此我們大致上看完了整個 tcpconnect 的主要的實作內容。\n","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-4-bcc-tcpconnect/","section":"文章","summary":"","title":"學習 eBPF 系列 4 - BCC tcpconnect","type":"posts"},{"content":"這篇文章會介紹 eBPF socket filter 的概念，並使用 bcc 的 http-parse-simple.py 作為範例，來說明如何使用 eBPF socket filter 來過濾 HTTP 請求，並且會深入底下 eBPF 的 socket filter 底層部分的實作。\nSocket filter 介紹 # 前一篇文章介紹的 tcpconnect 是使用 BPF_PROG_TYPE_KPROBE 這個 program type，透過 kprobe/kretprobe 機制在 kernel function 被呼叫和回傳的時候執行。\n這次使用的是 BPF_PROG_TYPE_SOCKET_FILTER，socket filter 可以對進出 socket 的封包進行截斷或過濾。特別注意這邊如果會需要擷取封包 (長度不等於原始封包長度) 則會觸發對封包進行複製，然後修改封包大小。\nsocket filter program 會在 socket 層被呼叫 (在 net/core/sock.c 的 sock_queue_rcv_skb 被呼叫)，並傳入 _sk_buff 結構 取得 socket 上下文及封包的內容。\n透過回傳的數值來決定如何處理該封包，如果回傳的數值大於等於封包長度，等價於保留完整封包，如果長度小於封包長度，則截斷只保留回傳數值長度的封包。其中兩個特例是回傳 0 和 - 1。回傳 0 等價解取一個長度為 0 的封包，也就是直接丟棄該封包。回傳 - 1 時，由於封包長度是無號整數，-1 等價於整數的最大數值，因此保證保留整個完整的封包。\n另外一個關鍵技術是 raw socket，我們可以將 raw socket 監聽某個網路介面上所有進出封包。\n因此整個程式的執行方式是這樣的，在目標網路卡上開啟一個 raw socket，透過 eBPF 程式過濾掉所有非 http 的封包，只保留 http 封包送出到 raw socket，userspace client 接收到封包時，可以直接解析封包欄位提取出 http 封包資訊。\nhttp-parse-simple 介紹 # 首先一樣先了解一下這支程式的功能，http-parse 能夠綁定到一張網路卡上面執行，然後提取經過 http 流量，將 http version, method, uri 和 status 輸出顯示。(當然如果經過 tls 加密的話是沒辦法的)\n執行結果如下\npython http-parse-complete.py GET /pipermail/iovisor-dev/ HTTP/1.1 HTTP/1.1 200 OK GET /favicon.ico HTTP/1.1 HTTP/1.1 404 Not Found GET /pipermail/iovisor-dev/2016-January/thread.html HTTP/1.1 HTTP/1.1 200 OK GET /pipermail/iovisor-dev/2016-January/000046.html HTTP/1.1 HTTP/1.1 200 OK http-parse-simple 實作 # eBPF 實作 # 在這次的程式中 eBPF c code 直接寫在一個獨立的 http-parse-simple.c 檔案中。\n這次的 ebpf 程式很簡單只有單一個函數 http_filter，作為 socket filter 的進度點。\nint http_filter(struct __sk_buff *skb) { u8 *cursor = 0; struct ethernet_t *ethernet = cursor_advance (cursor, sizeof(*ethernet)); //filter IP packets (ethernet type = 0x0800) if (!(ethernet-\u0026gt;type == 0x0800)) { goto DROP; } struct ip_t *ip = cursor_advance (cursor, sizeof(*ip)); //drop the packet returning 0 DROP: return 0; ... 相信很多人跟我一樣第一眼看到這個程式會覺得非常疑惑，首先看到的是 cursor 和 cursor_advance 這兩個東西，從 ip 那行大概可以猜的出來，cursor 是對封包內容存取位置的指標，cursor_advance 會輸出當前 cursor 的位置，然後將 cursor 向後移動第二個參數的長度。由於我們要分析的是 http 封包，所以他的 ether type 勢必得是 0x0800 (IP)，所以對於不滿足的封包，我們直接 goto 到 drop，return 0。表示我們要擷取一個長度為 0 的封包等價於丟棄該封包。\n在 bcc 的 helpers.h 輔助函數標頭檔裡面可以看到 cursor_advane 的定義。\n//packet parsing state machine helpers #define cursor_advance (_cursor, _len) \\ ({ void *_tmp = _cursor; _cursor += _len; _tmp; }) 果然符合我們的預期，先將原先 cursor 指標的數值保留起來，將 cursor 向後移動 len 後回傳原始數值。\n後面的程式碼其實就很簡單，首先一路解析封包確保他是一個 ip/tcp/http 封包、封包長度夠長塞的下一個有效的 http 封包內容\npayload_offset = ETH_HLEN + ip_header_length + tcp_header_length; ... unsigned long p [7]; int i = 0; for (i = 0; i \u0026lt; 7; i++) { p [i] = load_byte (skb, payload_offset + i); } 接著將 http packet 的前 7 個 byte 讀出來，load_byte 同樣是定義在 helpers.h\nunsigned long long load_byte (void *skb, unsigned long long off) asm (\u0026#34;llvm.bpf.load.byte\u0026#34;); 他會直接轉譯成 BPF_LD_ABS，從 payload_offset 位置開始讀一個 byte 出來，payload_offset，是前面算出來從 ethernet header 開始到 http payload 的位移。\n//HTTP if ((p [0] == \u0026#39;H\u0026#39;) \u0026amp;\u0026amp; (p [1] == \u0026#39;T\u0026#39;) \u0026amp;\u0026amp; (p [2] == \u0026#39;T\u0026#39;) \u0026amp;\u0026amp; (p [3] == \u0026#39;P\u0026#39;)) { goto KEEP; } //GET if ((p [0] == \u0026#39;G\u0026#39;) \u0026amp;\u0026amp; (p [1] == \u0026#39;E\u0026#39;) \u0026amp;\u0026amp; (p [2] == \u0026#39;T\u0026#39;)) { goto KEEP; } ... //no HTTP match goto DROP; //keep the packet and send it to userspace returning -1 KEEP: return -1; 接著檢查如果封包屬於 HTTP (以 HTTP, GET, POST, PUT, DELETE HEAD… 開頭)，就會跳到 keep，保留整個完整的封包送到 userspace client program。\nGET /favicon.ico HTTP/1.1 HTTP/1.1 200 OK HTTP request 會以 method 開頭、response 會以 HTTP 開頭，所以需要查找這些字樣開頭的封包。\npython 實作 # 接著我們很快速的來看一下 python 程式碼的部分。\nbpf = BPF (src_file = \u0026#34;http-parse-simple.c\u0026#34;,debug = 0) function_http_filter = bpf.load_func (\u0026#34;http_filter\u0026#34;, BPF.SOCKET_FILTER) BPF.attach_raw_socket (function_http_filter, interface) socket_fd = function_http_filter.sock sock = socket.fromfd (socket_fd,socket.PF_PACKET,socket.SOCK_RAW,socket.IPPROTO_IP) sock.setblocking (True) 首先我們一樣透過 BPF 物件完成 bpf 程式碼的編譯，不一樣的是是這邊直接指定 src_file 從檔案讀取。接著透過 load_func，指定 socket filter 這個 program type type 和 http_filter 這個入口函數，並載入 ebpf bytecode 到 kernel 接著透過 bcc 提供的 attach_raw_socket API 在 interface 上建立 row socket 並將 socket filter program attach 上去。接著從 function_http_filter.sock 取得 raw socket 的 file descripter 並封裝成 python 的 socket 物件。由於後面需要 socket 是阻塞的，但是 attach_raw_socket 建立出來的 socket 是非阻塞的，所以這邊透過 sock.setblocking (True) 阻塞 socket\nwhile 1: #retrieve raw packet from socket packet_str = os.read (socket_fd,2048) packet_bytearray = bytearray (packet_str) ... for i in range (payload_offset,len (packet_bytearray)-1): if (packet_bytearray [i]== 0x0A): # \\n if (packet_bytearray [i-1] == 0x0D): \\r break # 遇到 http 的換行 \\r\\n 則結束 \u0026lt; br\u0026gt; print (\u0026#34;% c\u0026#34; % chr (packet_bytearray [i]), end = \u0026#34;\u0026#34;) 後面的程式碼其實就和 ebpf 的部分大同小異，從 socket 讀取封包內容、解析到 http payload 後，將 http payload 的第一行輸出出來。\n到此我們就完成了 http-parse-simple 的解析。\n問題 # cursor 指標數值為 0，但是可以存取到封包的內容。\nu8 *cursor = 0; struct ethernet_t *ethernet = cursor_advance (cursor, sizeof(*ethernet)); if (!(ethernet-\u0026gt;type == 0x0800)) { goto DROP; } 特別的 load_bytes 函數呼叫，來取得封包內容\nload_byte (skb, payload_offset + i); 首先雖然 ebpf 使用 c 來編寫，但是經由 LLVM 編譯後會轉換成 eBPF bytecode，在進入 kernel 後會再經過 verifier 的修改。(經過這次的探索，可以理解 verifier 雖然叫做 verifier，但是他的功能確包羅萬象，對 eBPF 架構來說非常重要)\n深入了解 Socket filter # 為了理解這段 eBPF code 後面發生了什麼事，我們先查看 LLVM 編譯出來的 eBPF bytecode。\nBCC Debug # 在 BCC 編譯時，我們可以透過 debug 這個參數取得編譯過程中的資訊，當然也包含取得 LLVM 編譯出來的 eBPF bytecode，可以使用的 debug 選項如下\n# Debug flags # Debug output compiled LLVM IR. DEBUG_LLVM_IR = 0x1 # Debug output loaded BPF bytecode and register state on branches. DEBUG_BPF = 0x2 # Debug output pre-processor result. DEBUG_PREPROCESSOR = 0x4 # Debug output ASM instructions embedded with source. DEBUG_SOURCE = 0x8 # Debug output register state on all instructions in addition to DEBUG_BPF. DEBUG_BPF_REGISTER_STATE = 0x10 # Debug BTF. DEBUG_BTF = 0x20 解析 simple-http-parse 編譯結果 # 透過 BPF (src='simple-http-parse.c', debug=DEBUG_PREPROCESSOR)，我們可以看到上面的 code 被 LLVM 重新解釋為\nvoid *cursor = 0; struct ethernet_t *ethernet = cursor_advance (cursor, sizeof(*ethernet)); //filter IP packets (ethernet type = 0x0800) if (!(bpf_dext_pkt (skb, (u64) ethernet+12, 0, 16) == 0x0800)) { goto DROP; } 因此 cursor 在這邊的用途真的只是計算 offset。bpf_dext_pkt 在 bcc 的 helper.h 有所定義\nu64 bpf_dext_pkt(void *pkt, u64 off, u64 bofs, u64 bsz) { if (bofs == 0 \u0026amp;\u0026amp; bsz == 8) { return load_byte (pkt, off); } else if (bofs + bsz \u0026lt;= 8) { return load_byte (pkt, off) \u0026gt;\u0026gt; (8 - (bofs + bsz)) \u0026amp; MASK (bsz); } else if (bofs == 0 \u0026amp;\u0026amp; bsz == 16) { return load_half (pkt, off); ... 可以看到他是根據參數大小和類型去正確呼叫 load_byte, load_half, load_dword 系列函數，所以其實他做的事情與我們感興趣的第二段 code load_byte (skb, payload_offset + i); 是一致的。\n接著我們使用 BPF (src='simple-http-parse.c', debug=DEBUG_SOURCE) 查看編譯出來的 eBPF binary code。\n; int http_filter(struct __sk_buff *skb) { // Line 27 0:\tbf 16 00 00 00 00 00 00\tr6 = r1 1:\t28 00 00 00 0c 00 00 00\tr0 = *(u16 *) skb [12] ; if (!(bpf_dext_pkt (skb, (u64) ethernet+12, 0, 16) == 0x0800)) { // Line 34 2:\t55 00 5c 00 00 08 00 00\tif r0 != 2048 goto +92 其中 r0, r1, r6 是 eBPF 的 register，我們這邊只關注第 1 行 r0 = *(u16 *) skb [12]，這行是從 skb 的第 12 個 byte 拿取資料出來，剛好對應到 bpf_dext_pkt。\n根據 ebpf instruction set 的定義，第一個 byte 28 (0010 1000) 是 op code。最後 3 個 bit 000 是 op code 的種類。這邊的 0x00 對應到 BPF_LD (non-standard load operations)\n3 bits (MSB) 2 bits 3 bits (LSB) mode size instruction class 在 BPF_LD 這個分類內，size bits 01 剛好對應到 BPF_H (half word (2 bytes)) 最前面的 3 個 bit 000 代表 BPF_ABS(legacy BPF packet access)。\n到這邊我們就理解它是怎麼運作了了，eBPF 定義了 BPF_ABS 來代表對封包的存取操作，LLVM 在編譯的時會將對 skb 的 load_byte 轉譯成對應的 instruction。\n了解 eBPF BPF_ABS instruction # 我們可以更深入的了解一下 eBPF 對 BPF_ABS 做了什麼事情，在 verifier 這個神奇的地方搜尋 BPF_ABS 這個 instruction，會找到下面這段內容 (簡化版)\n/* Implement LD_ABS and LD_IND with a rewrite, if supported by the program type. */ if (BPF_CLASS (insn-\u0026gt;code) == BPF_LD \u0026amp;\u0026amp; (BPF_MODE (insn-\u0026gt;code) == BPF_ABS || BPF_MODE (insn-\u0026gt;code) == BPF_IND)) { cnt = env-\u0026gt;ops-\u0026gt;gen_ld_abs (insn, insn_buf); new_prog = bpf_patch_insn_data (env, i + delta, insn_buf, cnt); 首先執行條件是 BPF_LD 及 BPF_ABS，我們的 code 剛好符合這個條件，接著會呼叫 env-\u0026gt;ops-\u0026gt;gen_ld_abs，根據原本的 instrunction insn，生成新的 instruction 寫入 insn_buf，接著呼叫 bpf_patch_insn_data 將原本的指令取代為新的指令。\n接著我們要找一下 gen_ld_abs，跟 day 11 介紹 map 的情況類似，verifier 定義了 bpf_verifier_ops 結構，讓不同的 program type 根據需要，實作 bpf_verifier_ops 定義的 function 來提供不同的功能和行為。\nsocket filter 的定義如下\nconst struct bpf_verifier_ops sk_filter_verifier_ops = { .get_func_proto\t= sk_filter_func_proto, .is_valid_access\t= sk_filter_is_valid_access, .convert_ctx_access\t= bpf_convert_ctx_access, .gen_ld_abs\t= bpf_gen_ld_abs, }; 所以讓我們看到 bpf_gen_ld_abs (一樣經過簡化只看我們需要的部分)\nstatic int bpf_gen_ld_abs(const struct bpf_insn *insn, struct bpf_insn *insn_buf) { *insn++ = BPF_MOV64_REG (BPF_REG_2, orig-\u0026gt;src_reg); /* We\u0026#39;re guaranteed here that CTX is in R6. */ *insn++ = BPF_MOV64_REG (BPF_REG_1, BPF_REG_CTX); *insn++ = BPF_EMIT_CALL (bpf_skb_load_helper_16_no_cache); } 看到最後一行就很清晰了，最後其實等於調用了內部使用的 helper function 來存取資料。eBPF 也提提供了類似的 helper function bpf_skb_load_bytes，來提供存取封包內容的功能。\nBPF_CALL_2 (bpf_skb_load_helper_16_no_cache, const struct sk_buff *, skb, int, offset) { return ____bpf_skb_load_helper_16 (skb, skb-\u0026gt;data, skb-\u0026gt;len - skb-\u0026gt;data_len, offset); } 而 bpf_skb_load_helper_16_no_cache 其實就是直接從 sk_buff-\u0026gt;data 的位置取得資料，data 是 sk_buff 用來指到封包開頭的指標。\n__sk_buff 的限制 # 既然整個指令的本質是從 sk_buff-\u0026gt;data 拿取資料，那我們是不是能夠直接從 __sk_buff 裡面拿到資料呢？\n在 socket program type 下 program context 是 __sk_buff，他其實本質是對 sk_buff 的多一層封裝 (原因 參見)，在執行的時候，verifier 換將其取代回 sk_buff，因此__sk_buff 等於是 sk_buff 暴露出來的介面。\nstruct __sk_buff { ... __u32 data; __u32 data_end; __u32 napi_id; ... 參考**sk_buff 的定義，**sk_buff 是有定義將 data 和 data_end，那我們原始的 eBPF 程式是不是可以改成\nvoid *cursor = (void*)(long)(__sk_buff-\u0026gt;data); struct ethernet_t *ethernet = cursor_advance (cursor, sizeof (*ethernet)); if (!(ethernet-\u0026gt;type == 0x0800)) { goto DROP; } 如果完成這樣的修改，重新跑一遍 http-parse-simple.py，你會得到\npython3 http-parse-simple.py -i eno0 binding socket to \u0026#39;enp0s3\u0026#39; bpf: Failed to load program: Permission denied ; int http_filter (struct __sk_buff *skb) { 0: (bf) r6 = r1 ; void *cursor = (void*)(long) skb-\u0026gt;data; 1: (61) r7 = *(u32 *)(r6 +76) invalid bpf_context access off=76 size=4 processed 2 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0 Traceback (most recent call last): File \u0026#34;http-parse-simple.py\u0026#34;, line 69, in \u0026lt;module\u0026gt; function_http_filter = bpf.load_func (\u0026#34;http_filter\u0026#34;, BPF.SOCKET_FILTER) File \u0026#34;/usr/lib/python3/dist-packages/bcc/__init__.py\u0026#34;, line 526, in load_func raise Exception (\u0026#34;Failed to load BPF program % s: % s\u0026#34; % Exception: Failed to load BPF program b\u0026#39;http_filter\u0026#39;: Permission denied 可以看到程式碼被 verifier 拒絕，並拿到了一個 invalid bpf_context access off=76 size=4 的錯誤，表示存取 __sk_buff-\u0026gt;data 是非法的。\n回去追蹤程式碼的話，會看到在 verifier 裡面會用 env-\u0026gt;ops-\u0026gt;is_valid_access 來檢查該存取是否有效，這同樣定義在 bpf_verifier_ops 結構內。\n其中 socket filter program 的實作是\nstatic bool sk_filter_is_valid_access (int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { switch (off) { case bpf_ctx_range (struct __sk_buff, tc_classid): case bpf_ctx_range (struct __sk_buff, data): case bpf_ctx_range (struct __sk_buff, data_meta): case bpf_ctx_range (struct __sk_buff, data_end): case bpf_ctx_range_till (struct __sk_buff, family, local_port): case bpf_ctx_range (struct __sk_buff, tstamp): case bpf_ctx_range (struct __sk_buff, wire_len): case bpf_ctx_range (struct __sk_buff, hwtstamp): return false; } ... 可以很直接看到拒絕了 data 的存取。\n從 linux kernel 的 變更紀錄 來推測，data 欄位好像本來就不是給 socket filter 使用的，只是單純因為 cls_bpf 和 socker filter 可能共用了這部分的程式碼，因此要額外阻擋這部分的 code 不讓使用。\n結語 # 最後還有一個沒解決的問題，u8 *cursor = 0;，為甚麼空指標經過 LLVM 編譯後會編譯成對 skb 的存取還是未知的，看起來像是 BCC 特別的機制，但是找不太到相關資料，只好保留這個問題。\n參考資料 # eBPF Instruction Set Stackoverflow: invalid bpf_context access” bpf-helpers man page ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-5-bcc-http-filter-socket-filter/","section":"文章","summary":"","title":"學習 eBPF 系列 5 - BCC HTTP Filter \u0026 Socket Filter","type":"posts"},{"content":"這篇文章會介紹 eBPF 一個比較知名的用途，express data path (XDP)，並使用 bcc 的 xdp_redirect_map.py 作為範例。\nXDP 介紹 # 會說 linux 的網路慢主要是因為封包在進出 linux 設備時要經過 linux kernel 的 network stack，經過大家熟悉的 iptables, routing table.. 等網路子系統的處理，然而經由這麼多複雜的系統處理就會帶來延遲，降低 linux 網路的效能。\n上圖是封包在經由 linux 網路子系統到進入路由前的截圖，可以看到在封包剛進入到 linux kernel，甚至連前面看過，linux 用來維護每一個封包的 skb 結構都還沒建立前，就會呼叫到 XDP eBPF 程式，因此如果我們能夠在 XDP 階段就先過濾掉大量的封包，或封包轉發、改寫，能夠避免掉進入 linux 網路子系統的整個過程，降低 linux 處理封包的成本、提高性能。\n前面提到 XDP 工作在封包進入 linux kernel 的非常早期，甚至早於 skb 的建立，其實 XDP 的 hook point 直接是在 driver 內，因此 XDP 是需要 driver 特別支援的，為此 XDP 其實有三種工作模式: xdpdrv, xdpgeneric,xdpoffload。xdpdrv 指的是 native XDP，就是標準的 XDP 模式，他的 hook point 在 driver 層，因此是網卡接收到封包送至系統的第一位，可以提供極好的網路性能。xdpgeneric: generic XDP 提供一個在 skb 建立後的 XDP 進入點，因此可以在 driver 不支援的情況下提供 XDP 功能，但也由於該進入點比較晚，所以其實不太能提供好的網路效能，該進入點主要是讓新開發者在缺乏支援網卡的情況下用於測試學習，以及提供 driver 開發者一個標準用。xdpoffload: 在某些網卡下，可以將 XDP offload 到網卡上面執行，由於直接工作在網卡晶片上，因此能夠提供比 native XDP 還要更好的性能，不過缺點就是需要網卡支援而且部分的 map 和 helper function 會無法使用。\nXDP 的 return 數值代表了封包的下場，總共有五種結果，定義在 xdp_action\nenum xdp_action { XDP_ABORTED = 0, XDP_DROP, XDP_PASS, XDP_TX, XDP_REDIRECT, }; XDP_ABORTED, XDP_DROP 都代表丟棄封包，因此使用 XDP 我們可以比較高效的丟棄封包，用於防禦 DDoS 攻擊。不過 XDP_ABORTED 同時會產生一個 eBPF 系統錯誤，可以透過 tracepoint 機制來查看。\necho 1 \u0026gt; /sys/kernel/debug/tracing/events/xdp/xdp_exception/enable cat /sys/kernel/debug/tracing/trace_pipe systemd-resolve-512 [000] .Ns.1 5911.288420: xdp_exception: prog_id=91 action=ABORTED ifindex=2 ... XDP_PASS 就是正常的讓封包通過不處理。\nXDP_TX 是將封包直接從原始網卡送出去，我們可以透過在 XDP 程式內修改封包內容，來修改目的地 IP 和 MAC，一個使用前景是用於 load balancing，可以將封包打到 XDP 主機，在修改封包送去後端主機。\nXDP_REDIRECT 是比較後來新加入的一個功能，它可以將封包\n直接轉送到另外一張網路卡，直接送出去 指定給特定的 CPU 處理 將封包直接送給特定的一個 AF_XDP 的 socket 來達到跳過 kernel stack 直接交由 user space 處理的效過 最後，前面提到 XDP 早於 skb 的建立，因此 XDP eBPF program 的上下文不是skbbuff，而是使用自己的 xdp_md\nstruct xdp_md { __u32 data; __u32 data_end; __u32 data_meta; /* Below access go through struct xdp_rxq_info */ __u32 ingress_ifindex; /* rxq-\u0026gt;dev-\u0026gt;ifindex */ __u32 rx_queue_index; /* rxq-\u0026gt;queue_index */ }; 可以看到 xdp_md 是一個非常精簡的資料結構，因為 linux 還沒對其做解析提取出更多資訊。\nxdp_redirect_map 介紹 # 這次使用的 eBPF program type 理所當然是 BPF_PROG_TYPE_XDP。\n這隻程式的功能很簡單，執行時指定兩個 interface in_intf 和 out_intf，所有從 in_intf 進入的封包會直接從 out_intf 送出去，並且交換 src mac address 和 dst mac address，同時記錄每秒鐘通過該介面的封包個數。\n從 out_intf 進入的封包則正常交給 linux network 系統處理。\n首先我們一樣要先驗證程式的執行，首先建立一個 network namespace net0。然後把兩個網卡 veth_in_intf , veth_out_intf\n放進去，作為 xdp_redirect_map 使用的網卡。為了方便打流量，我們幫 in_intf 指定一個 ip 10.10.10.1，並幫加入一個不存在的遠端 ip 10.10.10.2，接著我們就可以透過 ping 10.10.10.2 來從 in_intf 打流量，透過 tcpdump 捕捉 out_intf 的封包，應該就可以看到從 10.10.10.1 過來的封包，同時 mac address 被交換了，所以可以看到 src mac 變成 ee:11:ee:11:ee:11。\nip netns add net0 ip link add in_intf type veth peer name veth_in_intf ip link add out_intf type veth peer name veth_out_intf ip link set veth_in_intf netns net0 ip link set veth_out_intf netns net0 ip link set in_intf up ip link set out_intf up ip netns exec net0 ip link set veth_in_intf up ip netns exec net0 ip link set veth_out_intf up ip address add 10.10.10.1/24 dev in_intf ip neigh add 10.10.10.2 lladdr ee:11:ee:11:ee:11 dev in_intf 目前這個部分其實沒有驗證成功，雖然根據 xdp redirect 的 log，封包是真的有成功被轉送到 veth_out_intf 的，然後透過 tcpdump 卻沒有在 out_intf 上收到封包，可惜的是具體原因沒能確定。\nxdp_redirect_map 實作 # eBPF 實作 # 這次的程式非常簡短，首先是一個 swap_src_dst_mac 函數，用於交換封包的 src mac address 和 dst mac address。\nstatic inline void swap_src_dst_mac(void *data) { unsigned short *p = data; unsigned short dst [3]; dst [0] = p [0]; dst [1] = p [1]; dst [2] = p [2]; p [0] = p [3]; p [1] = p [4]; p [2] = p [5]; p [3] = dst [0]; p [4] = dst [1]; p [5] = dst [2]; } 由於 mac address 在 ethernet header 的前 12 個 bit 所以可以很簡單地進行交換。\n接著就直接進入到了 attach 在 in interface 上的 XDP 函數\nint xdp_redirect_map(struct xdp_md *ctx) { void* data_end = (void*)(long) ctx-\u0026gt;data_end; void* data = (void*)(long) ctx-\u0026gt;data; struct ethhdr *eth = data; uint32_t key = 0; long *value; uint64_t nh_off; nh_off = sizeof(*eth); if (data + nh_off \u0026gt; data_end) return XDP_DROP; value = rxcnt.lookup (\u0026amp;key); if (value) *value += 1; swap_src_dst_mac (data); return tx_port.redirect_map (0, 0); } 首先 data 及 data_end 是分別指到封包頭尾的指標，由於封包頭都是 ethernet header，因此可以直接將 data 轉成 ethhdr 指標。首先對 ethernet 封包做一個完整性檢查，data + nh_off \u0026gt; data_end 表示封包大小小於一個 ethernet header，表示封包表示不完整，就直接將封包透過 XDP_DROP 丟棄。\n接著 rxcxt 是預先定義的一個 BPF_PERCPU_ARRAY (rxcnt, long, 1);，PER_CPU map 的特性是每顆 CPU 上都會保有一份獨立不同步的資料，因此可以避免 cpu 之間的 race condition，減少 lock 的開銷。這邊指定每個 CPU 上的 array 長度為 1，可以參考 Day11 有介紹過，是一個特別的使用技巧，可以簡單看成一個可以跟 user space share 的全域變數。\nuint32_t key = 0; value = rxcnt.lookup (\u0026amp;key); if (value) *value += 1; 這邊的用途是用來統計經過的封包個數，因此這邊非常簡單，統一使用 0 當作 key 去存取唯一的 value，然後每經過一個封包就將 value 加一，這邊可以注意到 lookup 回傳的是 pointer，因此可以直接對他做修改即可保存。\nswap_src_dst_mac (data); return tx_port.redirect_map (0, 0); 最後會呼叫 swap_src_dst_mac 來交換封包，然後透過 redirect_map 來將封包送到對應的 out interface。\nBPF_MAP_TYPE_DEVMAP 和 BPF_MAP_TYPE_CUPMAP 是用來搭配 XDP_REDIRECT，將封包導向透定的 CPU 或著從其他 interface 送出去的。\n而這邊的 redirect_map 在編譯時會被修改為呼叫 bpf_redirect_map 這個 helper function。其定義為 long bpf_redirect_map (struct bpf_map *map, u32 key, u64 flags)，透過接收 map 可以根據對應到的 value 來將封包導向到 interface 或著 CPU，設置方法會在後面的 python code 介紹。由於我們今天只為有一個 out interface，因此可以很簡單的指定 key 為 0\n後面的 flags 目前只有使用最後兩個 bit，可以當作 key 找不到時 redirect_map 的回傳值，因此以本次的 code 來說，預設的回傳數值是 0，也就對應到 XDP_ABORTED。\nint xdp_dummy(struct xdp_md *ctx) { return XDP_PASS; } 最後一段程式碼 xdp_dummy 是用來皆在 out interface 上的 XDP 程式，但他就只是簡單的 XDP_PASS，讓進入的封包繼續交由 linux kernel 來處理。\npython 實作 # 接下來就進入到 python code 的部分\nin_if = sys.argv [1] out_if = sys.argv [2] ip = pyroute2.IPRoute () out_idx = ip.link_lookup (ifname=out_if)[0] 首先將兩張網卡的名稱讀進來，接著透過 pyroute2 的工具去找到 out interface 的 ifindex\ntx_port = b.get_table (\u0026#34;tx_port\u0026#34;) tx_port [0] = ct.c_int (out_idx) 接著是設定 tx_port 這張 DEVMAP 的 key 0 為 out interface 的 index，因此所有經過 eBPF 程式的封包都會丟到 out interface\nin_fn = b.load_func (\u0026#34;xdp_redirect_map\u0026#34;, BPF.XDP) out_fn = b.load_func (\u0026#34;xdp_dummy\u0026#34;, BPF.XDP) b.attach_xdp (in_if, in_fn, flags) b.attach_xdp (out_if, out_fn, flags) 接著就是將 eBPF 程式 attach 到兩張網卡上\nrxcnt = b.get_table (\u0026#34;rxcnt\u0026#34;) prev = 0 while 1: val = rxcnt.sum(0).value if val: delta = val - prev prev = val print(\u0026#34;{} pkt/s\u0026#34;.format(delta)) time.sleep (1) 將 eBPF 程式 attach 上去之後就完成了封包重導向的工作，剩下的部分是用來統計每秒鐘經過的封包的，這邊的做法很簡單，每秒鐘都去紀錄一次通過封包總量和前一秒鐘的差異就可以算出來這一秒內經過的封包數量。這邊比較特別的是 rxcnt.sum，前面提到 rxcnt 是一個 per cpu 的 map，因此這邊使用 sum 函數將每顆 cpu 的 key 0 直接相加起來，就可以得到經過所有 CPU 的封包總量。\n","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-6-xdp-bcc-xdp-redirect-map/","section":"文章","summary":"","title":"學習 eBPF 系列 6 - XDP \u0026 BCC xdp_redirect_map","type":"posts"},{"content":"接續前一篇主題 XDP，今天我們要繼續來聊聊 eBPF 在 linux netowrk data path 上的另外一個進入點 tc，並同樣以 bcc 的 neighbor_sharing 作為範例。\nLinux tc 介紹 # 首先我們要先聊聊 tc 是什麼東西。Traffic Control (tc) 是 linux kernel 網路系統裡面和 netfilter/iptables 同等重要的一個組件。不過 netfilter 主要著重在 packet mangling (封包修改) 和 filter (過濾)。而 tc 的重點是在調控流量，提供限速、整形等功能。\ntc 的工作時機點分成 ingress tc 和 egress tc，以 ingress tc 來說，他發生在 skb allocation 之後，進入 netfilter 之前。ingress tc 主要用於輸入流量控制，egress tc 則用於流量優先級、QoS 的功能。在傳統使用上，tc 更主要是用在 egress tc，ingress tc 本身有比較大的功能限制。\n在 tc 裡面有三個主要的概念，qdisc、class 和 filter (classifier)。\ntc 的基礎是 queue，封包要進出主機時，會先進入 queue，根據特定的策略重新排序、刪除、延遲後再交給網卡送出，或 netfilter 等系統收入。\nqdisc 是套用在這個 queue 上面的策略規則。下列舉例一部份：\n最基本的策略規則是 pfifo，就是一個簡單的 FIFO queue，只能設定 queue 的可儲存的封包大小和封包個數。 更進階的如 pfifo_fast，會根據 ip 封包內的 ToS 欄位將封包分成三個優先度，每個優先度內是走 FIFO 規則，但是會優先清空高優先度的封包。 sfq 則是會根據 tcp/udp/ip 欄位 hash 的結果區分出不同的連線，將不同連線的封包放入獨立的 bucket 內，然後 bucket 間使用輪尋的方式，來讓不同連線均等的輸出。 ingress 是專門用在 ingress tc 的 qdisc 上面的 qdisc 都歸為 classless QDisc，因為我們不能透過自定義的方式對流量進行分類，提供不同的策略。 與 classless 相反的是 classful qdisc，在 classful qdisc 內，我們可以以定義出多個 class，針對不同的 class 設定不同的限速策略等規則。也可以將多個 class 附屬在另外一個 class 下，讓子 class 共用一個父 class 的最大總限速規則，但是子分類又獨立有限速規則等等。\n而要對流量進行分類就會用到 filter, 對於某個 qdisc (classless/classful 皆可) 或著父 class 上的封包，如果滿足 filter 的條件，就可以把封包歸到某個 class 上。除了歸類到某個 class 上，filter 也可以設置為執行某個 action，包括丟棄封包、複製封包流量到另外一個網路介面上之類的…\n對於 qdisc 和 class 在建立時需指定或自動分配一個在網卡上唯一的 handle 作為識別 id，格式是 \u0026lt;major\u0026gt;:\u0026lt;minor\u0026gt;(數字)，對於 qdisc 來說只有 major 的部分 \u0026lt;major\u0026gt;:，對 class 來說 major 必須與對應 qdisc 相同。\n另外在 egress pipeline 可以有多個 qdisc，其中一個作為 root，其他的藉由 filter 從 root qdisc dispatch 過去，所以需要有 major 這個欄位。\n在 linux 上面主要透過 tc 這個指令來設置 qdisc、class 和 filter。\n# 添加 eth0 egress 的 root qdisc，類型是 htb，後面是 htb 的參數 tc qdisc add dev enp0s3 root handle 1: htb default 30 # 添加 eth 的 ingress qdisc tc qdisc add dev enp0s3 ingress # 設置一個 class，速度上下限都是 20mbps，附屬於 eth0 的 root qdisc (1:) 下 tc class add dev enp0s3 partent 1: classid 1:1 htb rate 20mbit ceil 20mbit # 當封包為 ip, dst port 80 時分類到上述分類 tc filter add dev enp0s3 protocol ip parent 1: prio 1 u32 match ip dport 80 0xffff flowid 1:1 # 查看 egress filter tc filter show dev eth0 # 查看 ingress filter tc filter show dev eth0 ingress eBPF 與 tc # eBPF 在 tc 系統裡面是在 filter 的部分作用，並可分成兩種模式，classifier (BPF_PROG_TYPE_SCHED_CLS) 和 action (BPF_PROG_TYPE_SCHED_ACT)。\nclassifier: 前者分析封包後，決定是否 match，並可以將封包分類給透過 tc 指令預設的 classid 或著重新指定 classid。 0: mismatch 1: match, 使用 filter 預設的 classid 直接回傳一個 classid action: 作為該 filter 的 action，當 tc 設置的 filter 規則 match 後，呼叫 eBPF 程式決定 action 是 drop (2), 執行預設 action (-1) 等。下列是 action 的完整定義 #define TC_ACT_UNSPEC\t(-1) #define TC_ACT_OK\t0 #define TC_ACT_RECLASSIFY\t1 #define TC_ACT_SHOT\t2 #define TC_ACT_PIPE\t3 #define TC_ACT_STOLEN\t4 #define TC_ACT_QUEUED\t5 #define TC_ACT_REPEAT\t6 #define TC_ACT_REDIRECT\t7 #define TC_ACT_JUMP\t0x10000000 BCC neighbor_sharing # 介紹 # 這次要看的是 examples/networking/neighbor_sharing。(原始碼)\n這次的 eBPF 程式會提供 QoS 的服務，對經過某張網卡的針對往特定的 IP 提供不同的限速群組。\n/------------\\ | neigh1 --|-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-| | | neigh2 --|-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-| \u0026lt;-128kb-| /------\\ | neigh3 --|-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-| | wan0 | wan | | | ^ | br100 |-\u0026lt;-\u0026lt;-\u0026lt;--| sim | | | clsfy_neigh () | | ^ \\------/ | lan1 ----|-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-| \u0026lt;--1Mb--| | | lan2 ----|-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-\u0026gt;-| | classify_wan () | ^ \\------------/ | pass () | 上圖是 neighbor_sharing 自帶的網路拓譜圖，neight1-3, lan1-2, wan0 是獨立的 network namespace 擁有獨立的 IP，neighbor_sharing 會在 wansim 到 br100 的介面上建立 ingress tc，針對 neigh1-3 的 IP 提供總共 128kb/s 的網路速度，對其他 IP 提供總共 1024kb/s 的網路速度。\n首先在測試之前要先安裝 pyroute2 和 netperf，前者是 python 接接 tc 指令的 library，後者是用來測試網速的工具。另外要記得設置防火牆規則不然 br100 不會轉發封包\npip3 install pyroute2 apt install netperf iptables -P FORWARD ACCEPT sysctl -w net.ipv4.ip_forward=1 neight1-3 會被分配 172.16.1.100-102 的 IP, lan 則是 172.16.1.150-151。\nsudo ip netns exec wan0 netperf -H 172.16.1.100 -l 2 -k MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.100 () port 0 AF_INET : demo Recv Send Send Socket Socket Message Elapsed Size Size Size Time Throughput bytes bytes bytes secs. 10^6bits/sec 131072 16384 16384 6.00 161.45 透過 netperf 可以測出來到 neight1 的封包流量被限制在約 161.45 kbits/sec。\nip netns exec wan0 netperf -H 172.16.1.150 -l 2 -f k MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.1.150 () port 0 AF_INET : demo Recv Send Send Socket Socket Message Elapsed Size Size Size Time Throughput bytes bytes bytes secs. 10^3bits/sec 131072 16384 16384 2.67 1065.83 而到 lan1 大約是 1065.83kbits/sec，接近預先設置的規則。\npython 實作 # 這次會先看 python 的程式碼，由於這次的程式碼包含大量用來建立測試環境的部分，所以會跳過只看相關的內容。\nb = BPF (src_file=\u0026#34;tc_neighbor_sharing.c\u0026#34;, debug=0) wan_fn = b.load_func (\u0026#34;classify_wan\u0026#34;, BPF.SCHED_CLS) pass_fn = b.load_func (\u0026#34;pass\u0026#34;, BPF.SCHED_CLS) neighbor_fn = b.load_func (\u0026#34;classify_neighbor\u0026#34;, BPF.SCHED_CLS) 首先這次的 eBPF 程式包含三個部分，因此會分別載入，並且全部都是 classifier (BPF_PROG_TYPE_SCHED_CLS)\nipr.tc (\u0026#34;add\u0026#34;, \u0026#34;ingress\u0026#34;, wan_if [\u0026#34;index\u0026#34;], \u0026#34;ffff:\u0026#34;) ipr.tc (\u0026#34;add-filter\u0026#34;, \u0026#34;bpf\u0026#34;, wan_if [\u0026#34;index\u0026#34;], \u0026#34;:1\u0026#34;, fd=wan_fn.fd, prio=1, name=wan_fn.name, parent=\u0026#34;ffff:\u0026#34;, action=\u0026#34;drop\u0026#34;, classid=1, rate=\u0026#34;128kbit\u0026#34;, burst=1024 * 32, mtu=16 * 1024) ipr.tc (\u0026#34;add-filter\u0026#34;, \u0026#34;bpf\u0026#34;, wan_if [\u0026#34;index\u0026#34;], \u0026#34;:2\u0026#34;, fd=pass_fn.fd, prio=2, name=pass_fn.name, parent=\u0026#34;ffff:\u0026#34;, action=\u0026#34;drop\u0026#34;, classid=2, rate=\u0026#34;1024kbit\u0026#34;, burst=1024 * 32, mtu=16 * 1024) 接著會建立 wan_if 的 ingress qdisc (wan_if 是 wan0 接到 br100 的介面)，並且會 ingress qdisc 下建立兩條 filter，首先它的 type 指定為 bpf 並透過 fd=wan_fn.fd 選定 eBPF program，所以會交由 eBPF classifier 來決定是不是要 match。\nclassifier match 後就會執行下屬的 policing action，跟 classid 無關，且在這次的範例中並不存在 class，所以 classid 其實是無意義的，不一定要設置。\n後半段 action=\u0026quot;drop\u0026quot;, rate=\u0026quot;128kbit\u0026quot;, burst=1024 * 32, mtu=16 * 1024 定義了一條 policing action，只有當封包滿足 policy 條件時才會觸發具體的 action，這邊指定是流量超出 128kbit 時執行 drop，也就達到了限制 neigh 流量的效果。\n第二條同理，match pass_fn 並且流量到達 1024kbit 時執行 drop，由於 pass_fn 顧名思義是無條件 match 的意思，所以等價於所有非 neigh 的流量共用這一條的 1024kbit 流量限制。\n因此總結來說，eBPF 程式 wan_fn 透過某種方式判斷封包是否是往 neigh 的 ip，是的話就 match 第一條 filter 執行 policing action 來限流，不然就 match 第二條 filter 來做限流。\nret = self._create_ns (\u0026#34;neighbor% d\u0026#34; % i, ipaddr=ipaddr, fn=neighbor_fn, cmd=cmd) 接著就會看到，在建立 neigh1-3 的 namespace 時，attach 了 neighbor_fn 到網卡上，因此就很好理解了 neighbor_fn 監聽了從 neigh 發出的封包，解析拿到 neigh 的 IP 後，透過 map share 給 wan_fn，讓 wan_fn 可以根據 ip 決定要不要 match 第一條 policing action。\neBPF 實作 # 到這裡其實就分析出整個程式的執行邏輯了，我們接續來看看 neighbor_sharing 的 eBPF 程式，這次的 eBPF 程式分成三個部分，首先是接在每個 neigh ingress 方向的 classify_neighbor，接著是接在 wan0 ingress 方向的 classify_wan 和 pass。\n前面說到出來 classify_neighbor 要做的事情就是紀錄 neigh1-3 的 IP，提供給 classify_wan 判斷是否要 match 封包，執行 128kbits 的流量限制。\nstruct ipkey { u32 client_ip; }; BPF_HASH (learned_ips, struct ipkey, int, 1024); 首先定義了一個 hash map 用 key 來儲存所有 neigh 的 IP\nint classify_neighbor(struct __sk_buff *skb) { u8 *cursor = 0; ethernet: { struct ethernet_t *ethernet = cursor_advance (cursor, sizeof(*ethernet)); switch (ethernet-\u0026gt;type) { case ETH_P_IP: goto ip; default: goto EOP; } } ip: { struct ip_t *ip = cursor_advance (cursor, sizeof(*ip)); u32 sip = ip-\u0026gt;src; struct ipkey key = {.client_ip=sip}; int val = 1; learned_ips.insert (\u0026amp;key, \u0026amp;val); goto EOP; } EOP: return 1; } 接著 classify_neighbor 就會用 cursor 解析出 source ip，將其作為 hash map 的 key 放到 learned_ips 裡面，value 則都設為 1。不論如何都會 return 1 放行封包。雖然其實這是 neighbor ingress 方向上唯一的一條 filter，所以不論回傳值為多少其實都可以，不影響執行結果。\n這邊就要提到第一次學習 tc 還有 classifier 時會感到很困惑的地方了，首先 classifier 的回傳值 0 表示 mismatch, 1 表示 match 並轉移到預設的 class，其餘回傳值表示直接指定 classid 為回傳的數值。接著不論 classid 是多少，都會執行 filter 上面綁定的 action。在這次的範例中，所有的 filter 其實都不存在任何的 class，因此 return 值唯一的意義是控制是否要執行 action。這邊 classify_neighbor 綁定的 action 是 ok，表示放行封包的意思\nint classify_wan(struct __sk_buff *skb) { u8 *cursor = 0; ethernet: { struct ethernet_t *ethernet = cursor_advance (cursor, sizeof(*ethernet)); switch (ethernet-\u0026gt;type) { case ETH_P_IP: goto ip; default: goto EOP; } } ip: { struct ip_t *ip = cursor_advance (cursor, sizeof(*ip)); u32 dip = ip-\u0026gt;dst; struct ipkey key = {.client_ip=dip}; int *val = learned_ips.lookup (\u0026amp;key); if (val) return *val; goto EOP; } EOP: return 0; } 接著看到 classify_wan，他會提取封包的 dst ip address，並嘗試搜尋 learned_ips，如果找的到就表示這個是 neighbor 的 ip，回傳 map 對應的 value，前面提到所有的 value 都會設置為 1，因此表示 match 的意思，不然就跳轉到 EOP 回傳 0，表示 mismatch。同樣由於這邊不存在 class，因此 value 只要是非 0 即可，只是用來 match 執行 policing action。\nint pass(struct __sk_buff *skb) { return 1; } 最後的 pass 其實就是一條無條件回傳 1 表示 match，來執行 wan0 方向第二條 1024kbits/sec 的限流政策用的。\ntc 與 XDP 比較 # 在 eBPF 裡面，XDP 和 TC 兩個功能常常被拿來一起討輪，前面有提到 eBPF 可以做為 tc actions 使用來達到封包過濾之類的效果，雖然實行效果上是比不上 XDP 的，但是 tc ingress 的 eBPF hook point 也在 kernel data path 的最早期，因此也能夠提供不錯的效能，加上 tc ebpf program 的 context 是 sk_buff，相較於 xdp_buff，可以直接透過 __sk_buff 取得和修改更多的 meta data，加上 tc 在 ingress 和 egress 方向都有 hook point，不像 XDP 只能作用在 ingress 方向，且 tc 完全不需要驅動支援即可工作，因此 tc 在使用彈性和靈活度上是比 XDP 更占優的。\n不過 tc 其實也有提供 offload 的功能，將 eBPF 程式 offload 到網卡上面執行。\nDirect action # 然而由於 tc 的 hook point 分成 classifier 和 action 因此無法透過單一個 eBPF 程式做到 match-action 的效果，然而大多數時候 eBPF tc 程式的開發並不是要利用 tc 系統的功能做限速等功能，而是要利用 tc 在 kernel path 極早期這點做 packet mangling 和 filter 等事項，再加上 tc 系統的使用學習難度相對高，因此 eBPC 在 tc 後引入了 direct-action 和 clsact 這兩個功能。\n首先介紹 direct-action (da)，這個是在 classifier (BPF_PROG_TYPE_SCHED_CLS) 可啟用的一個選項，如果啟用 da，classifier 的回傳值就變成是 action，和 BPF_PROG_TYPE_SCHED_ACT 相同，而原本的 classid 改成設置__skb_buff-\u0026gt;tc_classid 來傳輸。\n在 kernel code 內使用 prog-\u0026gt;exts_integrated 標示是否啟用 da 功能\n透過 da 可以透過單一個 eBPF 程式完成 classifier 和 action 的功能，降低了 tc hook point 對原本 tc 系統框架的依賴，能夠透過 eBPF 程式簡潔的完成功能。\n在 da 的使用上可以參考 bcc 的範例 examples/networking/tc_perf_event.py，使用上與普通的 classifer 幾乎無異，只要在載入時 ipr.tc (\u0026quot;add-filter\u0026quot;,\u0026quot;bpf\u0026quot;, me,\u0026quot;:1\u0026quot;, fd=fn.fd, ... ,direct_action=True) 加上 direct_action 選項即可。\n透過 tc 指令查看時也可以看到 direct-action 字樣。\ntc filter show dev t1a filter parent 1: protocol all pref 49152 bpf chain 0 filter parent 1: protocol all pref 49152 bpf chain 0 handle 0x1 flowid :1 hello direct-action not_in_hw id 308 tag 57cd311f2e27366b jited action order 1: gact action pass random type none pass val 0 index 2 ref 1 bind 1 clsact # 後來 tc 加入了 clsact，clsact 是一個專為 eBPF 設計的偽 qdisc。首先 clsact 同時作用在 ingress 和 egress 方向，也進一步簡化了 ebpf 程式的掛載。\ntc qdisc add dev em1 clsact tc filter add dev em1 ingress bpf da obj tc-example.o sec ingress tc filter add dev em1 egress bpf da obj tc-example.o sec egress 同時 clsact 工作在真的 qdisc 本身的 lock 之前，因此可以避免 lock 的開銷，預先完成比較複雜繁重的封包分類，在進入到真的 queue filter 時只根據更簡單的欄位 (如 tc_index) 做分類。另外 da 本來只能使用在 ingress 方向，透過 clsact，da 可以工作在 egress 方向。\n關於 eBPF tc 的部分就大致上介紹到這裡，對於 tc 這個子系統相對來說是真的蠻陌生的，因此介紹這個部分的確是有比較大的難度和說不清楚的地方。\n參考資料 # Classifying packets with filters tc man page 用 tc qdisc 管理 Linux 网络带宽 TC (Traffic Control) 命令 Linux TC (Traffic Control) 简介 ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-7-tc-bcc-neighbor-sharing/","section":"文章","summary":"","title":"學習 eBPF 系列 7 - tc \u0026 BCC neighbor_sharing","type":"posts"},{"content":"本篇是 BCC 學習歷程的最後一篇，這篇文章會介紹 linux cgroups、eBPF socketmap 的功能，並以 sockmap.py 作為範例。\ncgroups 介紹 # cgroups 是 Linux kernel 內建的一個機制，可以以進程為最小單位，對可使用的 CPU、memory、裝置 I/O 等資源進行限制、分割。\ncgroups 目前有 v1 和 v2 兩個版本，在分組策略架構上有所差異，這邊介紹只以 v1 為主\n在 cgroup 的架構內，我們可以針對不同的資源類型進行獨立管理 (稱為不同的 subsystem 或 controller) ，一些可能的資源類型和一部份的功能簡介如下\ncpu: 對一定時間週期內，可使用的 cpu 時間長度限制 memory: 限制記憶體使用上限以及超出上限時的行為 blkio: 控制對硬碟等設備的訪問速度上限 cpuacct: 用來統計目前的 CPU 使用情況 devices: 控制可以訪問那些 device pids: 限制 cgroup 內可建立的 pid 數量，也就是進程數量 接著是 hierarchy，cgroup 使用樹狀結構來管理資源，一個 hierarchy 預設會有一個根結點，所有的 process (pid 都會 attach 在這個節點上)。\n一個 hierarchy 可以對應到零個或多個上述的 subsystem，並在一個節點內設置上述的那些限制，那這些限制就會套用到在這個節點內的所有 process。\n可以在 hierarchy 內建立子節點，那子節點就會預設套用父節點的所有設置，然後可以只針對有興趣的項目作更細緻的調正。\n一個 process 在一棵 hierarchy 只能 attach 在一個節點上，可以對 process 設定所在的節點。從 process fork 出來的 process 會在同一個節點上，但是搬運 process 到不同的節點，並不會影響子 process。\nLinux 透過虛擬檔案系統來提供修改調整 cgroups 的 user space 介面。通常來說介面會被掛載在 /sys/fs/cgroup 這個路徑下。\n我們可以透過 mount 來建立 hierarchy 並把他關連到一個或多個 subsystem\n# 關連到 CPU mkdir /sys/fs/cgroup/cpu mount -t cgroup -o cpu none /sys/fs/cgroup/cpu # 關連到 CPU 和 CPUACCT mkdir /sys/fs/cgroup/cpu,cpuacct mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct # 不過 /sys/fs/cgroup 目錄可能會被系統設置為 \u0026lt; span class=\u0026#34;hljs-built_in\u0026#34;\u0026gt;read only，避免隨意變更，而且通常不需要增減 hierarchy 本身，只是在 hierarchy 內增減節點管理 查看所有目前的 hierarchy\nls /sys/fs/cgroup/-l total 0 dr-xr-xr-x 4 root root 0 十 11 22:50 blkio lrwxrwxrwx 1 root root 11 十 11 22:50 cpu -\u0026gt; cpu,cpuacct lrwxrwxrwx 1 root root 11 十 11 22:50 cpuacct -\u0026gt; cpu,cpuacct dr-xr-xr-x 4 root root 0 十 11 22:50 cpu,cpuacct dr-xr-xr-x 2 root root 0 十 11 22:50 cpuset dr-xr-xr-x 4 root root 0 十 11 22:50 devices dr-xr-xr-x 2 root root 0 十 11 22:50 freezer dr-xr-xr-x 2 root root 0 十 11 22:50 hugetlb dr-xr-xr-x 4 root root 0 十 11 22:50 memory dr-xr-xr-x 2 root root 0 十 11 22:50 misc lrwxrwxrwx 1 root root 16 十 11 22:50 net_cls -\u0026gt; net_cls,net_prio dr-xr-xr-x 2 root root 0 十 11 22:50 net_cls,net_prio lrwxrwxrwx 1 root root 16 十 11 22:50 net_prio -\u0026gt; net_cls,net_prio dr-xr-xr-x 2 root root 0 十 11 22:50 perf_event dr-xr-xr-x 4 root root 0 十 11 22:50 pids dr-xr-xr-x 2 root root 0 十 11 22:50 rdma dr-xr-xr-x 5 root root 0 十 11 22:50 systemd dr-xr-xr-x 5 root root 0 十 11 22:50 unified 接著查看 cpu 的根結點\nls /sys/fs/cgroup/cpu/-l total 0 -rw-r--r-- 1 root root 0 十 11 21:39 cgroup.clone_children -rw-r--r-- 1 root root 0 十 11 21:39 cgroup.procs -r--r--r-- 1 root root 0 十 11 21:39 cgroup.sane_behavior -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.stat -rw-r--r-- 1 root root 0 十 11 21:39 cpuacct.usage -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_all -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_percpu -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_percpu_sys -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_percpu_user -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_sys -r--r--r-- 1 root root 0 十 11 21:39 cpuacct.usage_user -rw-r--r-- 1 root root 0 十 11 21:39 cpu.cfs_period_us -rw-r--r-- 1 root root 0 十 11 21:39 cpu.cfs_quota_us -rw-r--r-- 1 root root 0 十 11 21:39 cpu.shares -r--r--r-- 1 root root 0 十 11 21:39 cpu.stat drwxr-xr-x 4 root root 0 八 24 14:50 docker -rw-r--r-- 1 root root 0 十 11 21:39 notify_on_release -rw-r--r-- 1 root root 0 十 11 21:39 release_agent drwxr-xr-x 96 root root 0 十 11 06:05 system.slice -rw-r--r-- 1 root root 0 十 11 21:39 tasks drwxr-xr-x 2 root root 0 十 11 21:31 user.slice 由於前面可以看到 cpu 被 link 到 cpu,cpuacct，所以可以同時查看到 cpu._ 和 cpuacct._ 的選項。\n透過 cpu.cfs_quota_us 和 cpu.cfs_period_us 我們就能控制這個節點上所有 process 在 period 內可使用的 CPU 時間 (quota)。\n透過 cat tasks 我們可以看到所有 attach 在這個節點上的 pid。\n可以看到有三個資料夾 docker, system.slice, user.slice，是三個 hierarchy 上的子節點，我們可以簡單的透過 mkdir 的方式建立子節點。由於這台設備上有跑 docker，所以 docker 會在 /sys/fs/cgroup/cpu/docker/ 目錄下為每個 container 建立獨立的子節點，透過 cgroup 的方式限制容器的資源使用量。\ndocker ps --format=\u0026#34;{{.ID}}\u0026#34; 90f64cb70ee0 177d1a3920ec ls /sys/fs/cgroup/cpu/docker -l total 0 drwxr-xr-x 2 root root 0 八 24 14:50 177d1a3920ec9.... drwxr-xr-x 2 root root 0 八 24 14:50 90f64cb70ee068... -rw-r--r-- 1 root root 0 十 11 21:39 cgroup.clone_children -rw-r--r-- 1 root root 0 十 11 21:39 cgroup.procs ... 在許多發行版上使用 systemd 來做為核心系統管理程式，也就會透過 systemd 來管理 cgroup，因此在設置 kubelet 時會建議將 cgroup driver 從 cgroupfs 改成 systemd，統一由 systemd 來管理，避免同時有兩個系統在調整 cgroup\ncgroup v2 調整了管理介面的結構，只保留了單一個 hierarchy (/sys/fs/cgroup/unified) 管理所有的 subsystem，因為切出多個 hierarchy 來管理的方式被認為是不必要且增加系統複雜度的。\n到這邊大概介紹完了 cgroup，由於這次 sockmap.py 使用的 program type 的 hook point 會在 cgroup 上，所以趁這個機會詳細了解了一下 cgroup。\nsocketmap 介紹 # 這邊我們拿 Cilium CNI 介紹 的一張圖來說明。\n圖中是一個使用 envoy sidecar 的 kubernetes pod 網路連線示意圖，簡單來說 kubernetes 上面容器 (Pod) 服務 (Service) 的網路流量會透過 iptables 的機制全部重新導向到跑在同一個容器內的 sidecar，透過 sidecar 當作中介完成網路監控、服務發現等功能後才會真正離開容器。進入容器的流量同樣先都重導向到 sidecar 處理。\n這樣的好處是可以完全不對 service 本身修改，完全由獨立的 sidecar 來提供附加的網路功能，但是也有一個很明顯的問題，一個封包在傳輸的過程中，要經過 3 次 Linux kernel 的 network stack 處理，大大降低了封包的傳輸效率。\n其中由於都是在同一台設備的同一個網路空間內傳輸，因此 TPC/IP/ethernet 等底層網路完全可以省略。\n因此我們可以透過 eBPF 的 socket redirect 技術來簡化這個封包的傳輸過程，簡單來說，在同一個設備的兩個 socket 間的傳輸，我們完全可以直接跳過底層的網路堆疊，直接在 socket layer 將封包內容從一個 socket 搬到另外一個 socket，跳過底層 TCP/IP/ethernet 處理。\nsockmap.py 介紹 # bcc 的 sockmap.py 提供的就是 socket redirect 的功能，他會監聽機器上的所有 socket，將 local to local 的 tcp 連線資料封包直接透過 socket redirect 的方式進行搬運。\nsocket redirect 機制好像同時也節省了 packet 在 userspace 和 kernel space 之間複製搬運的過程，不過這件事情沒有完全確定。\n我們一樣先看看執行起來怎麼樣，我們透過 python 建立一個 http server 並透過 curl 來測試\npython3 -m http.server \u0026amp; curl 127.0.0.1:8000 接著是 eBPF 程式的執行結果\npython3 sockmap.py -c /sys/fs/cgroup/unified/ b\u0026#39;curl-3043 [000] d...1 7164.673950: bpf_trace_printk: remote-port: 8000, local-port: 46246\u0026#39; b\u0026#39;curl-3043 [000] dN..1 7164.673973: bpf_trace_printk: Sockhash op: 4, port 46246 --\u0026gt; 8000\u0026#39; b\u0026#39;curl-3043 [000] dNs11 7164.673985: bpf_trace_printk: remote-port: 46246, local-port: 8000\u0026#39; b\u0026#39;curl-3043 [000] dNs11 7164.673988: bpf_trace_printk: Sockhash op: 5, port 8000 --\u0026gt; 46246\u0026#39; b\u0026#39;curl-3043 [000] d...1 7164.674643: bpf_trace_printk: try redirect port 46246 --\u0026gt; 8000\u0026#39; b\u0026#39;python3-3044 [000] d...1 7164.675211: bpf_trace_printk: try redirect port 8000 --\u0026gt; 46246\u0026#39; b\u0026#39;python3-3044 [000] d...1 7164.675492: bpf_trace_printk: try redirect port 8000 --\u0026gt; 46246\u0026#39; 這邊可以看到 sockmap 要指定一個 - c 的參數，後面是指定一個 cgroup，sockmap 只會監控在這個 cgroup 節點上的 socket 連線。這邊 unified 是 cgroup v2 的 hierarchy，在 cgroup v2 只有 unified 一個 hierarchy，所有 subsystem 都在這個 hierarchy 上。\n首先是 curl remote-port: 8000, local-port: 46246' Sockhash op: 4, port 46246 --\u0026gt; 8000'，這兩條是 curl 發起連線時，記錄下來的 socket 連線請求。\n接著 curl remote-port: 46246, local-port: 8000' Sockhash op: 5, port 8000 --\u0026gt; 46246'，是 curl 跟 http server 之間連線建立成功後，返回給 curl 的 socket 通知。\n接著可以看到 3 條 try redirect 是 curl 傳遞 http request 和 http server 返回 http response 的 msg，直接透過 socket redirect 的方式在兩個 socket 之間交互。\n這邊我們使用 tcpdump 去監聽 lo interface 的方式來驗證 socket redirect 有真的運作到。同樣是透過 curl 127.0.0.1:8000 發起連線傳輸資料。在沒有啟用 sockmap 的情況下 tcpdump 捕捉到 12 個封包。而開啟 socketmap 後只會捕捉到 6 個封包。\n透過封包內容會發現，在 socketmap 啟動後，只能夠捕捉到帶 SYN、FIN 等 flag 的 TCP 控制封包，不會捕捉到中間純粹的資料交換封包。\nSOCK_OPS program type # 完成驗證後，我們接著來介紹這次用到的兩種 eBPG program type，分別是 BPF_PROG_TYPE_SOCK_OPS 和 BPF_PROG_TYPE_SK_MSG。\nBPF_PROG_TYPE_SOCK_OPS 可以 attach 在一個 cgroup 節點上，當該節點上任意 process 的 socket 發生特定事件時，該 eBPF program 會被觸發。可能的事件定義在 bpf.h。其中 CB 結尾的表示特定事件完成後觸發，例如 BPF_SOCK_OPS_TCP_LISTEN_CB 表示在 socket tcp 連線轉乘 LISTEN 狀態後觸發。有些則是觸發來透過回傳值設置一些控制項，BPF_SOCK_OPS_TIMEOUT_INIT 是在 TCP Timeout 後觸發，透過 eBPF 的 return value 設置 RTO，-1 表示使用系統預設。\nenum { BPF_SOCK_OPS_VOID, BPF_SOCK_OPS_TIMEOUT_INIT, BPF_SOCK_OPS_RWND_INIT, BPF_SOCK_OPS_TCP_CONNECT_CB, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB, BPF_SOCK_OPS_NEEDS_ECN, BPF_SOCK_OPS_BASE_RTT, BPF_SOCK_OPS_RTO_CB, BPF_SOCK_OPS_RETRANS_CB, BPF_SOCK_OPS_STATE_CB, BPF_SOCK_OPS_TCP_LISTEN_CB, BPF_SOCK_OPS_RTT_CB, BPF_SOCK_OPS_PARSE_HDR_OPT_CB, BPF_SOCK_OPS_HDR_OPT_LEN_CB, BPF_SOCK_OPS_WRITE_HDR_OPT_CB, }; 這邊要特別介紹的是 BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB 和 BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB 分別是在主動建立連線時 (發送 SYN，tcp 三手交握第一手)，和被動建立連線時 (發送 SYN+ACK，tcp 三手交握第二手) 觸發。\n觸發後會拿到 bpf_sock_ops 上下文，並根據事件不同，eBPF 回傳值也代表不同的意義。其中 bpf_sock_ops-\u0026gt;op 對應到上述的事件類型。args 則是不同 op 可能帶的一些特殊參數。\nstruct bpf_sock_ops { __u32 op; union { __u32 args [4];\t/* Optionally passed to bpf program */ __u32 reply;\t/* Returned by bpf program\t*/ __u32 replylong [4];\t/* Optionally returned by bpf prog */ }; __u32 family; __u32 remote_ip4;\t/* Stored in network byte order */ __u32 local_ip4;\t/* Stored in network byte order */ __u32 remote_ip6 [4];\t/* Stored in network byte order */ __u32 local_ip6 [4];\t/* Stored in network byte order */ __u32 remote_port;\t/* Stored in network byte order */ __u32 local_port;\t/* stored in host byte order */ __u32 is_fullsock; ... SK_MSG SK_SKB program type # 接著要介紹另外兩個 program type BPF_PROG_TYPE_SK_SKB 和 BPF_PROG_TYPE_SK_MSG。\n首先他們不 attach linux 本身的某個地方而是 attach 在一個 eBPF map 上，這個 map 必須是 BPF_MAP_TYPE_SOCKMAP 或 BPF_MAP_TYPE_SOCKHASH。兩個 map 都是某個 key 對應到 socket，可以使用 sock_hash_update 更新 sockhash map，將 sock_ops 的上下文 bpf_sock_ops 結構當作 value 去插入。\n當 sockmap 裡面的 socket 有訊息要送出，封包要被放到 socket 的 TXQueue 時會觸發 BPF_PROG_TYPE_SK_MSG，而當封包從外界送入被主機接收，要放到 socket 的 RXQueue 時則會觸發 BPF_PROG_TYPE_SK_SKB。\n以這次會用到的 BPF_PROG_TYPE_SK_MSG 來說，當 userspace 呼叫 sendmsg 時，就會被 eBPF 程式攔截。\n可以透過回傳 __SK_DROP, __SK_PASS, __SK_REDIRECT 來決定是要丟棄、接收或做 socket redirect。\n透過 socket redirect，封包會從發送端 socket 直接被丟到接收端 socket RXQ。\n目前 redirect 的功能只能用於 TCP 連線。\nsockmap 實作 # eBPF 實作 # 大致上的概念介紹完了就讓我們回到 bcc sockmap 的程式碼。\n首先一樣先看 eBPF 的程式碼。\n#define MAX_SOCK_OPS_MAP_ENTRIES 65535 struct sock_key { u32 remote_ip4; u32 local_ip4; u32 remote_port; u32 local_port; u32 family; }; BPF_SOCKHASH (sock_hash, struct sock_key, MAX_SOCK_OPS_MAP_ENTRIES); 這邊定義了一個 sock_key，作為 BPF_SOCKHASH socket map 的 key，透過 five tuple (IP src/dst, sct/dst port 及 TCP/UDP) 來定位一個連線。\n接著我們看到第一種 program type SOCK_OPS 的入口函數。\nint bpf_sockhash (struct bpf_sock_ops *skops) { u32 op = skops-\u0026gt;op; /* ipv4 only */ if (skops-\u0026gt;family != AF_INET) return 0; switch (op) { case BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB: case BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB: bpf_sock_ops_ipv4 (skops); break; default: break; } return 0; } 這邊做的事情很簡單，在 socket 建立連線 (ACTIVE_ESTABLISHED_CB) 和接收連線 (PASSIVE_ESTABLISHED_CB) 時，呼叫 bpf_sock_ops_ipv4 將 socket 放到 sock map 內，讓 socket 被第二個 program type SK_MSG 的程式能夠在 socket 呼叫 sendmsg 等 API 時被攔截處理。由於 socker redirect 只能處裡 TCP 連線，所以非 AF_INET 的連線會被過濾掉。\nstatic __always_inline void bpf_sock_ops_ipv4(struct bpf_sock_ops *skops) { struct sock_key skk = { .remote_ip4 = skops-\u0026gt;remote_ip4, .local_ip4 = skops-\u0026gt;local_ip4, .local_port = skops-\u0026gt;local_port, .remote_port = bpf_ntohl (skops-\u0026gt;remote_port), .family = skops-\u0026gt;family, }; int ret; bpf_trace_printk (...); ret = sock_hash.sock_hash_update (skops, \u0026amp;skk, BPF_NOEXIST); if (ret) { bpf_trace_printk (\u0026#34;bpf_sock_hash_update () failed. % d\\\\n\u0026#34;, -ret); return; } bpf_trace_printk (...); } 這邊的 bpf_sock_ops_ipv4 其實也很簡單，從 sock_opt 裡面提取出 IP 地址 / TCP port 的資訊，填充 sock_key 結構，然後呼叫 sock_hash_update 把 key-value piar 塞進去 sock_hash。後面的 flag 有 BPF_NOEXIST, BPF_EXIST, BPF_ANY。BPF_NOEXIST 表示只有 key 不在 map 裡面的時候可以插入。\n接著是 BPF_PROG_TYPE_SK_MSG 的入口函數。\nint bpf_redir(struct sk_msg_md *msg) { if (msg-\u0026gt;family != AF_INET) return SK_PASS; if (msg-\u0026gt;remote_ip4 != msg-\u0026gt;local_ip4) return SK_PASS; struct sock_key skk = { .remote_ip4 = msg-\u0026gt;local_ip4, .local_ip4 = msg-\u0026gt;remote_ip4, .local_port = bpf_ntohl (msg-\u0026gt;remote_port), .remote_port = msg-\u0026gt;local_port, .family = msg-\u0026gt;family, }; int ret = 0; ret = sock_hash.msg_redirect_hash (msg, \u0026amp;skk, BPF_F_INGRESS); bpf_trace_printk (...); if (ret != SK_PASS) bpf_trace_printk (...); return ret; } 首先一樣我們只能處裡 TCP 連線所有把非 AF_INET 的連線透過 return SK_PASS; 交回 linux kernel 處理。\n接著由於 socket redirect 只在本機起作用，所以這邊簡單判斷 src ip 和 dst ip 相不相同，來判斷是否是 local to local 連線。\n接著由於 socket redirect 時要從發送端的 socket redirect 到接收端的 socket，因此我們要從 socket map 中找到接收端的 socket，對發送端和接收端的 socket 來說 src addres 和 dst address 的是顛倒的，所以這邊在生 sock_key 時會把 local 和 remote 顛倒。\n接著這邊的 msg_redirect_hash 是對 bpf_msg_redirect_hash helper function 的包裝，會嘗試從 socket map 找到對應的 socket，然後完成 redirect 的設置，不過成功是回傳是 SK_PASS 而不是 SK_REDIRECT。\n到這邊就完成 eBPF 程式的部分了，接下來 python 的部分就很簡單，只是把 eBPG 程式掛進去。\npython 實作 # examples = \u0026#34;\u0026#34;\u0026#34;examples: ./sockmap.py -c /root/cgroup # attach to /root/cgroup \u0026#34;\u0026#34;\u0026#34; parser = argparse.ArgumentParser ( description=\u0026#34;pipe data across multiple sockets\u0026#34;, formatter_class=argparse.RawDescriptionHelpFormatter, epilog=examples) parser.add_argument (\u0026#34;-c\u0026#34;, \u0026#34;--cgroup\u0026#34;, required=True, help=\u0026#34;Specify the cgroup address. Note. must be cgroup2\u0026#34;) args = parser.parse_args () 前面有提到 SOCK_OPS 要掛在一個 cgroup 下面，所以先吃一個 cgroup 路徑參數來。\nbpf = BPF (text=bpf_text) func_sock_ops = bpf.load_func (\u0026#34;bpf_sockhash\u0026#34;, bpf.SOCK_OPS) func_sock_redir = bpf.load_func (\u0026#34;bpf_redir\u0026#34;, bpf.SK_MSG) 編譯 eBPF 程式，取得兩個入口函數\n# raise if error fd = os.open(args.cgroup, os.O_RDONLY) map_fd = lib.bpf_table_fd (bpf.module, b\u0026#34;sock_hash\u0026#34;) bpf.attach_func (func_sock_ops, fd, BPFAttachType.CGROUP_SOCK_OPS) bpf.attach_func (func_sock_redir, map_fd, BPFAttachType.SK_MSG_VERDICT) 前面提到 cgroup 介面是一個虛擬檔案系統，所以當然要透過 open 去取得對應的 file descriptor。接著就是 attach func_sock_ops 到 SOCK_OPS。由於 func_sock_redir 要 attach 到 sock map，所以先透過 bcc 的 API 取得 sock_hash map 的 file descripter，然後 attach 上去。\n這樣就完成 sockemap 的設置，可以成功提供 socket redirect 的服務了！\n參考文件 # cgroups man page 第一千零一篇的 cgroups 介紹 Cgroups 中的资源管理 hierarchy 层级树 ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-8-cgroups-socket-map/","section":"文章","summary":"","title":"學習 eBPF 系列 8 - cgroups \u0026 socket map","type":"posts"},{"content":"本篇是本系列文章的最後一篇，在 eBPF 程式裡面要與 kernel 交互很重要的是 helper function，因此在最後的兩天時間，我們要把所有的 helper function 速覽過一遍。這邊介紹以 bpf-helper 的 man 文件 的內容為主，部分的 helper function 可能因為文件更新而有遺漏。\n接下來的介紹會稍微對 helper function 做一定程度的分類，但是具體不同的 eBPF program type 支援那些 helper function 可能還是要根據 bcc 文件、每個 helper function 對應的 commit 資訊等查詢。\neBPF map 操作類 # array, map 類型 map 的操作函數，對應到查詢、插入或更新、刪除 map 內的元素。其中 update 可以透過 flag (BPF_NOEXIST, BPF_EXIST, BPF_ANY) 決定 key 是不是不能先存在或一定要存在於 map 內。 bpf_map_lookup_elem bpf_map_update_elem bpf_map_delete_elem 用於 stack, queue 類型 map 的操作函數。 bpf_map_peek_elem bpf_map_pop_elem bpf_map_push_elem 用於 ringbuff 的操作函數 (改進原本 perf event map 的問題) bpf_ringbuf_output bpf_ringbuf_reserve bpf_ringbuf_submit bpf_ringbuf_discard bpf_ringbuf_query 通用函數 # 生成隨機數 get_prandom_u32 atol, atoul bpf_strtol bpf_strtoul 取得當前執行 eBPF 程式的 (SMP) processor ID。由於 eBPF 是 no preemption 的所以在整個執行過程中 processor id 不會變。 bpf_get_smp_processor_id 取得當前 NUMA (Non-uniform memory access) 的 node id。受於匯流排限制，CPU 核心可以比較快存取同節點上的 memory，透過 node id 區分。通常是當 attach 的 socket 有啟用 SO_ATTACH_REUSEPORT_EBPF 選項時會用到。 bpf_get_numa_node_id 搭配 BPF_MAP_TYPE_PROG_ARRAY map 去執行 tail call。 bpf_tail_call 取得開機到當下經過的時間，單位是 ns，差別在於後者會多包含 suspend (暫停) 的時間 bpf_ktime_get_ns bpf_ktime_get_boot_ns 取得 jiffies64 bpf_jiffies64 將字串訊息發送到 /sys/kernel/debug/tracing/trace，主要用於開發除錯 bpf_trace_printk 寫入 seq_file bpf_seq_write bpf_seq_printf 搭配 struct bpf_spin_lock 提供一個給 BPF_MAP_TYPE_HASH 和 BPF_MAP_TYPE_ARRAY(目前只支援這兩著) 裡面 value 使用的 lock，由於一個 map 裡面只能有一個 spin_lock，所以通常是使用把之前提過，整個 map 固定只有一個元素，把整個 map 當作一個 global variable 的用法 bpf_spin_lock bpf_spin_unlock 搭配 BPF_MAP_TYPE_PERF_EVENT_ARRAY 使用，傳輸資料到 user space bpf_perf_event_output Tracing 相關 (kprobe, tracepoint, perf event) # 取得當前的 tgid, uid, gid, command name, task structure bpf_get_current_pid_tgid bpf_get_current_uid_gid bpf_get_current_comm bpf_get_current_task 發 signal 到當前 (process, thread) bpf_send_signal bpf_send_signal_thread 用於讀取記憶體資料、字串及寫入記憶體。帶 user 的版本用於 user space memory，其餘用於 kernel space memory。 bpf_probe_read (通常使用後倆著) bpf_probe_read_user bpf_probe_read_kernel bpf_probe_read_str (通常使用後倆著) bpf_probe_read_user_str bpf_probe_read_kernel_str bpf_probe_write_user 搭配 BPF_MAP_TYPE_STACK_TRACE 使用，取得一個 stack address hash 過的 stack id bpf_get_stackid 取得 userspace 或 kernel space 的 stack 資料 bpf_get_stack bpf_get_task_stack 搭配 BPF_MAP_TYPE_PERF_EVENT_ARRAY 取得 perf-event counter 的讀數 bpf_perf_event_read bpf_perf_event_read_value (建議使用) 用於 BPF_PROG_TYPE_PERF_EVENT 取得 struct perf_branch_entry bpf_read_branch_records 搭配 BPF_MAP_TYPE_CGROUP_ARRAY 使用，檢查是否在某個 cgroup v2 節點內 bpf_current_task_under_cgroup 查看當前上下文的 cgroup 節點的祖先節點 id bpf_get_current_ancestor_cgroup_id 取得當前上下文對應的 cgroup id bpf_get_current_cgroup_id 用於 kprobe，修改函數回傳值 bpf_override_return Cgroup 相關 # 取得一個當前 network namespace 對應的 cookie (identifer) bpf_get_netns_cookie 取得 local storage 的指標 (cgroup 相關可使用的一個儲存區) bpf_get_local_storage 用於 BPF_PROG_TYPE_CGROUP_SYSCTL 取得、更新 sysctl 資訊 bpf_sysctl_get_name bpf_sysctl_get_current_value bpf_sysctl_get_new_value bpf_sysctl_set_new_value 其他類別 # LIRC 紅外線收發相關 (BPF_PROG_TYPE_LIRC_MODE2) bpf_rc_repeat bpf_rc_keydown bpf_rc_pointer_rel XDP 相關 # 於 XDP 修改封包大小 (可以增大或縮小) bpf_xdp_adjust_head bpf_xdp_adjust_tail XDP_TX redirect 使用 bpf_redirect_map XDP 輸出封包內容到 perf event bpf_xdp_output 調整 xdp_md-\u0026gt;data_meta bpf_xdp_adjust_meta 查詢 fid (Forward Information Base, L2) bpf_fib_lookup (也可用在 TC) LWT 相關 # attach 在 routing table 替 L3 封包進行 tunnel header encap bpf_lwt_push_encap 外封包 (underlay) 內容修改 bpf_lwt_seg6_store_bytes bpf_lwt_seg6_adjust_srh 套用 IPv6 Segment Routing action 決策 bpf_lwt_seg6_action socket, socket buffer 相關 # 用於 BPF_PROG_TYPE_CGROUP_SOCK_ADDR，修改 bind address bpf_bind 讀取封包內容 bpf_skb_load_bytes bpf_skb_load_bytes_relative 修改封包內容，可自動更新 chekcsum bpf_skb_store_bytes 改寫 l3, l4 的 checksum bpf_l3_csum_replace bpf_l4_csum_replace 用於計算 check sum，可搭配前兩個 replace 函數使用 bpf_csum_diff 取得 xfrm (IPsec 相關) bpf_skb_get_xfrm_state 將封包發到其他的 device。後者會複製一分封包。 bpf_redirect bpf_clone_redirect 取得 classid，參考 cgroup 的 net_cls，使用於 TC egress path。 bpf_get_cgroup_classid 增減 vlan header bpf_skb_vlan_push bpf_skb_vlan_pop 取得、修改封包的 tunnel (ex. GRE) 的 tunnel key 資訊 bpf_skb_get_tunnel_key bpf_skb_set_tunnel_key 取得、修改封包的 tunnel 資訊 bpf_skb_get_tunnel_opt bpf_skb_set_tunnel_opt 取得 skb 的 tclassid 欄位，用於 clsact TC egress bpf_get_route_realm 修改封包 prtocol (ipv4, ipv6) bpf_skb_change_proto 修改封包類型 (broadcast, multicast, unitcast..) bpf_skb_change_type 搭配 BPF_MAP_TYPE_CGROUP_ARRAY 使用，檢查 skb 是不是在某個 cgroup v2 節點的子節點內。 bpf_skb_under_cgroup 取得 skb 對應的 cgroup id bpf_skb_cgroup_id 向上查找 skb 對應 cgroup 節點的祖先節點 id bpf_sk_ancestor_cgroup_id 取得、修改 skb-\u0026gt;hash bpf_get_hash_recalc bpf_set_hash 修改封包大小 bpf_skb_change_tail 用於封包 payload 存取，具體內容有點難理解 (non-linear data) bpf_skb_pull_data 修改 skb-\u0026gt;csum bpf_csum_update 修改 skb-\u0026gt;csum_level bpf_csum_level 標註 skb-\u0026gt;hash 為無效，觸發重算 bpf_set_hash_invalid 將 skb-\u0026gt;sk 轉成所有欄位都可以訪問的版本 bpf_sk_fullsock 從 skb-\u0026gt;sk 取得 struct bpf_tcp_sock bpf_tcp_sock 向 tcp-sock 對應的方向發一個 tcp-ack bpf_tcp_send_ack 從 skb-\u0026gt;sk 取得 bpf_sock bpf_get_listener_sock 設置 skb-\u0026gt;sk bpf_sk_assign 取得 bpf_sock 對應的 cgroupv2 id bpf_sk_cgroup_id 取得 bpf_sock 對應 cgroupv2 節點的祖先 id bpf_sk_ancestor_cgroup_id 強制轉型 sk 成特定的 XXX_sock 結構 bpf_skc_to_tcp6_sock bpf_skc_to_tcp_sock bpf_skc_to_tcp_timewait_sock bpf_skc_to_tcp_request_sock bpf_skc_to_udp6_sock 增加 packet header 區 (headroom) 長度，用於為 L3 封包直接加上 L2 的 header bpf_skb_change_head 幫 socket 建立一個 cookie，作為 socket 的 identifier，用於追蹤 bpf_get_socket_cookie (sk_buff) bpf_get_socket_cookie (bpf_sock_addr) bpf_get_socket_cookie (bpf_sock_ops) 取得 socket 的 owner UID bpf_get_socket_uid 模擬呼叫 getsocketopt、setsockopt bpf_getsockopt bpf_setsockopt 存取 skb 的 ebpf local storage bpf_sk_storage_get bpf_sk_storage_delete 將封包內容輸出到 perf event bpf_skb_output 修改封包 payload 大小，可以從 L2 或 L3 的角度來看 bpf_skb_adjust_room 產生、尋找封包對應的 SYN cookie ACK+ bpf_tcp_gen_syncookie+ bpf_tcp_check_syncookie BPF_PROG_TYPE_SK_SKB (ingress 方向) 搭配 BPF_MAP_TYPE_SOCKMAP 做 socket redierct bpf_sk_redirect_map bpf_sk_redirect_hash 搜尋滿足對應 5-tuple 的 socket bpf_sk_lookup_tcp bpf_skc_lookup_tcp bpf_sk_lookup_udp 釋放上面兩個找到的 socket 的 reference bpf_sk_release BPF_PROG_TYPE_SK_MSG (egress 方向) 搭配 BPF_MAP_TYPE_SOCKMAP 做 socket redierct bpf_msg_redirect_map bpf_msg_redirect_hash 替特定長度的內容作 verdict (SK_PASS…)，可用於截短封包，優化處理速度 (tc 相關的東西不太熟…/) bpf_msg_apply_bytes 跳過接下來某長度的內容不做 veridct bpf_msg_cork_bytes 讀寫修改特定長度的資料 bpf_msg_pull_data bpf_msg_pop_data bpf_msg_push_data 更新 BPF_MAP_TYPE_SOCKMAP bpf_sock_map_update 設置 bpf_sock_ops-\u0026gt;bpf_sock_ops_cb_flags 欄位 bpf_sock_ops_cb_flags_set 更新 sockhash bpf_sock_hash_update 用於 BPF_PROG_TYPE_SK_REUSEPORT(將多個程式綁定在同一個 port 上) sk_select_reuseport 用於 BPF_PROG_TYPE_CGROUP_SKB，設置 ECN (Explicit Congestion Notification) bpf_skb_ecn_set_ce ","date":"October 31 2022","externalUrl":null,"permalink":"/posts/learn-ebpf-serial-9-ebpf-helper-functions/","section":"文章","summary":"","title":"學習 eBPF 系列 9 - eBPF helper functions","type":"posts"},{"content":"","date":"October 29 2022","externalUrl":null,"permalink":"/categories/terraform/","section":"分類","summary":"","title":"Terraform","type":"categories"},{"content":"","date":"October 29 2022","externalUrl":null,"permalink":"/tags/%E8%87%AA%E5%8B%95%E5%8C%96/","section":"標籤","summary":"","title":"自動化","type":"tags"},{"content":" 前言 # Proxmox 提供了 web GUI 來方便的建立和管理 LXC 和虛擬機，但是如果有大量的虛擬機需要建立，那麼使用 GUI 就會變得非常繁瑣，而且不利於版本控制。因此我們可以使用 Terraform 來完成自動化建立的過程。然而在 Proxmox 上使用 Terraform，我覺得相對 openstack 來說概念會比較複雜一點，因此花了一點點時間來釐清。這邊記錄下使用 terrform 管理 Proxmox 的基本操作，希望對大家有幫助。\nCloud-init 基本觀念 # 在使用 Terraform 建立 Proxmox VM 的過程中，我們會使用到 cloud-init 這個技術。 在使用 Promox GUI 設置虛擬機的過程中會有兩大麻煩的地方，第一個是需要在 web GUI 介面上一台一台的建立出來，第二個是需要在每台虛擬機上完成 OS 的安裝，設置硬碟、網路、SSH 等。 前者我們透過 terraform 來解決，後者我們則會搭配利用 cloud-init。cloud-init 是一個業界標準，在許多 Linux 發行版還有公 / 私有雲上都有相對應的支援。 各 Linux 發行版會發行特製的 cloud image 來支持 cloud-init。 支援 cloud-init 的作業系統會在開機執行的時候執行透過特定方式去讀取使用者設定檔，自動完成前面提到的網路、帳號等設置，來達到自動化的目的。\nData Source # 在 cloud image 中，cloud-init 會根據設定檔來完成設置，而設定檔的來源 (Data source) 有很多種，不同的 cloud (AWS, Azure, GCP, Openstack, Proxmox) 在 cloud-init 標準下制定了不同的設定檔來源。(可參考 文件)\n在 Proxmox 上支援 NoCloud 和 ConfigDrive 兩種資料源，兩種的執行方式相似，將使用者設定檔轉成一個特製的印象檔掛載到虛擬機上，當 VM 開機時 cloud-init 可以自動搜索到該印象檔，並讀取裡面的設定檔來完成設置。\n前置作業 # 首先我們要先安裝 Terraform 和在 proxmox 上安裝 cloud-init 的工具，這邊簡單直接把 Terraform 也裝在 promox host 上面。\n# cloud-init apt-get install cloud-init # Terraform wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Proxmox \u0026amp; Cloud-init # 再透過 Terraform 自動部屬之前，我們要先看看怎麼在 Proxmox 上搭配 cloud-init 手動部屬 VM。\n這邊我們透過 promox 的 CLI 工具來完成設置，不過操作也都可以透過 GUI 完成。\n建立 VM # export VM_ID=\u0026#34;9001\u0026#34; cd /var/lib/vz/template/iso/ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img qm create $VM_ID \\ --name ubuntu-2004-focal-fossa \\ --ostype l26 \\ --memory 8192 \\ --balloon 2048 \\ --sockets 1 \\ --cores 1 \\ --vcpu 1 \\ --net0 virtio,bridge=vmbr11 \\ --onboot 1 qm importdisk $VM_ID focal-server-cloudimg-amd64.img local-lvm --format qcow2 qm set $VM_ID --scsihw virtio-scsi-pci qm set $VM_ID --scsi0 local-lvm:vm-$VM_ID-disk-0 qm set $VM_ID --boot c --bootdisk scsi0 qm set $VM_ID --ide2 local-lvm:cloudinit qm set $VM_ID --serial0 socket 前面提到 cloud-init 要使用特製的印象檔，這邊我們透過 wget 抓取印象檔，放到 /var/lib/vz/template/iso/ 路徑下，這是 proxmox 預設放置 ISO 檔的路徑，因此可以透過 GUI 到 storage/local/ISO image 的頁面看到我們剛剛下載的印象檔。\n接著透過 qm create 指令建立 VM，這邊 balloon 參數對應到 Minimum memory 的設定。\n這邊提供的 cloud image 提供的印象檔並不是通常我們用來安裝作業系統的 iso 安裝檔，而是 qcow2 文件，qcow2 是一種虛擬機硬碟快照格式，因此這邊我們透過 importdisk 指令，直接將 img 轉換成硬碟。\n接著我們要將建立好的硬碟掛載到 VM 上，這邊我們指定 scsi0 介面，將硬碟掛載上去，同時由於我們要從 cloud image 開機，因此這邊直接將 bootdisk 設定為 scsi0。\nProxmox 官方文件有提到，ubuntu 的 cloud-init 映像，如果使用 scsi 接口掛載的話，需要將控制器設置為 virtio-scsi-pci。\n接著我們需要添加兩個特殊的設備，首先是 cloudinit (GUI 顯示為 cloud driver)，這個是前面提到用於傳輸 cloud-init 設定檔的設備，當在 proxmox 上完成 cloud-init 設定後，proxmox 會生成對應的印象檔掛到 cloud driver 上，\n另外由於 cloud image 的特殊性，我們需要添加一個 srial 設備。\n到這邊設址結果如下圖：\n設定 cloud-init # 接著我們要設定 cloud-init，這邊我們透過 GUI 的方式來完成設定。\n在 proxmox 上我們可以簡單的在 GUI 完成 cloudinit 的設定 (包含帳號、密碼、SSH key 等)，接著按下 Regenerage image 按鈕，proxmox 會生成設定檔，並掛載到前面建立的 cloud driver 上。\n啟動 VM # 接著我們只要按下 Start 按鈕，VM 就會開機，並自動完成前面的 cloud-init 設定。\nCloud image 不太建議使用密碼登入，因此預設 VM 通常都會把 SSH 密碼登入關閉，因此需要透過 SSH key 登入，或著使用最後後提到的 cicustom 來修改 SSH 設定。 使用 Terraform # 接著我們就要搭配 Terraform 來將完成自動化部屬了。(Proxmox provider)\n首先前面的指令不能丟，我們在最後加上一行 qm template $VM_ID，將 VM 轉成模板用於後續的 Terraform 部屬。 這邊使用模板，目前研究起來有兩個原因，首先硬碟、cloud driver、serial 這些固定虛擬硬體和 cloud image 可以直接複製，而不用在 Terraform 上重新設定。 另外 proxmox 的 terraform provider 好像不支援 importdisk 這樣導入 qcow2 印象檔的方式。\n首先是 provider 的基礎設定\nterraform { required_providers { proxmox = { source = \u0026#34;Telmate/proxmox\u0026#34; version = \u0026#34;2.9.11\u0026#34; } } } provider \u0026#34;proxmox\u0026#34; { # Configuration options pm_api_url = \u0026#34;https://127.0.0.1:8006/api2/json\u0026#34; pm_user = \u0026#34;root@pam\u0026#34; pm_password = \u0026#34;password\u0026#34; pm_tls_insecure = true } 這邊我們直接使用 root 的帳號密碼登入 proxmox web，不過為了安全和控管的話，建議還是建立額外的使用者給 terraform 使用，以及使用 token 來取代密碼。\n# 建立 terraform 使用者 pveum role add TerraformProv -privs \u0026#34;VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit\u0026#34; pveum user add terraform-prov@pve --password \u0026lt;password\u0026gt; pveum aclmod /-user terraform-prov@pve -role TerraformProv # 建立 token pveum user token add terraform-prov@pve terraform-token 接著我們透過 proxmox_vm_qemu 資源來建立 VM\nresource \u0026#34;proxmox_vm_qemu\u0026#34; \u0026#34;resource-name\u0026#34; { name = \u0026#34;VM-name\u0026#34; target_node = \u0026#34;Node to create the VM on\u0026#34; clone = \u0026#34;ubuntu-2004-focal-fossa\u0026#34; full_clone = true os_type = \u0026#34;cloud-init\u0026#34; onboot = true cores = 8 sockets = 1 cpu = \u0026#34;host\u0026#34; memory = 8192 balloon = 2048 scsihw = \u0026#34;virtio-scsi-pci\u0026#34; bootdisk = \u0026#34;virtio0 disk { slot = 0 size = \u0026#34;65536M\u0026#34; type = \u0026#34;scsi\u0026#34; storage = \u0026#34;local-lvm\u0026#34; iothread = 1 } ipconfig0 = \u0026#34;192.168.0.1/24,gw=192.168.0.254\u0026#34; ciuser=\u0026#34;username\u0026#34; cipassword=\u0026#34;password\u0026#34; sshkeys = file (\u0026#34;/root/.ssh/id_rsa.pub\u0026#34;) } 首先當然是指定我們 VM 的名子還有要長在 proxmox cluster 的哪台機器上 (name, target_node)。 接著我們指定我們要 clone 的我們剛剛做的 VM 模板 (clone) 並指定為完整複製 (full_clone)，以及指定 OS type 為 cloud-init。\n接著是設定 VM 的 CPU、memory 等硬體規格，這邊要特別注意的是這先參數的規格，如果不指定，並不會套用模板的規格，而是 provider 預設的規格，因此我們需要指定這些參數。\n接著比較特別的是我們要重新定義我們的硬碟，前面雖然我們已經將 cloud image 轉成硬碟掛載到 VM 上了，但是這樣掛載上去硬碟大小是絕對不夠用的 (以 ubuntu 的 image 來說只有 2G 多的硬碟大小)，因此我們這邊複寫修改 scsi0 的硬碟大小，cloud-init 在第一次開機的時候能夠自我察覺並修改分割區的大小來匹配新的硬碟容量。\n最後就是 cloud-init 的設定，這邊我們指定 VM 的 IP、帳號密碼、以及 ssh key。\n最後就一樣透過指令完成自動部屬\nterraform init terraform apply 到這邊我們就完成 terraform 與 proxmox 搭配的自動部屬了。\n其他雜紀 # cicustom # 前面我們都是透過 proxmox 本身的功能來生成 cloud-init 的設定檔，但是 proxmox 提供的設置選項有限，因此有時候我們會需要直接修改 cloud-init 的設定檔， 在 proxmox 上提供兩種方式來直接設定 cloud-init 設定檔的參數，一個是直接在指令上提供參數值，另外一個是直接提供 cloud-init 的 yaml 設定檔\n在 terraform 上面，我們一樣可以透過 cicustom 設定來達到相同的事情。\nagent # 在查找資料時，在許多範例會看到指定 agent 參數為 0 或 1，這邊的 agent 指的是 Qemu-guest-agent，簡單來說就是在虛擬機內部安裝一個 agent 來當作 proxmox 直接操作虛擬機內部的後門，不過具體的功能就不在本篇的範圍內了，且預設情況下這個功能是關閉的。\n結語 # 這邊簡單紀錄了一下 terraform 和 proxmox 的搭配使用，在一開始研究的時候，cloud-init 還有使用 VM template 這兩件事，是之前在使用 terraform 或 proxmox 不會特別注意到的東西，因此會有點混亂和不知道功能，希望這篇文章能夠幫助到有需要的人。\n參考資料 # Terraform Provider for Proxmox Proxmox Cloud-Init eBPF\n","date":"October 29 2022","externalUrl":null,"permalink":"/posts/deploy-proxmox-vm-with-terraform/","section":"文章","summary":"","title":"使用 Terraform 部屬 Proxmox 虛擬機","type":"posts"},{"content":" 前言 # 在嘗試布建 openstack 的過程中，卡關最久的應該是網路的部分，由於在設置 openstack 的過程中，需要先把網路介面和線路連接設定好，所以必須要對 openstack 的網路架構，有清晰的認知，不然在設置的過程中會遇到許多障礙…\n特別是 openstack 有 flat, vlan, vxlan 以及 provider, self-service 兩種不同維度的概念分類，因此在釐清的過程中花費許多時間，因此這邊整理一下 openstack 的各種網路架構。\n這篇文章不會介紹具體的 openstack 搭建，不過之後會有一篇介紹如何使用 openstack-ansible 來部屬 openstack。\n首先為了後續討論方便我們這邊要先幫 openstack 裡面的幾個網路層級訂個名子。\n物理網路：由 openstack 節點上面的實體網卡以及結點外的網路構成 覆蓋 (Overlay) 網路：以虛擬機的視角看到的 L2 網路後續會持續出現這三個名詞，在介紹的過程中來解釋和分析這三者的差別。 Openstack 網路類型概念 # 首先我們先撇開 openstack，我們看一個最簡單在多台實體設備上面裝設虛擬機構建出來的一個網路結構。\n在這個架構裡，所有的 VM 和實體網卡都接在同一個 linux bridge 上，由於實體網卡也橋接在 bridge 上了，所以節點上的覆蓋網路是物理網路的延伸，可以看成所有實體共同構成了一個 L2 網路。 由於 bridge 本身可以給予 IP，當成是接在節點上的網路介面，因此 VM 和節點本身也都同在一個 L2 網路內彼此互相可見。 在這個網路架構中，物理網路和覆蓋網路是完全等價通透的。\nFlat network # 接著我們加入 openstack 的組件。 首先是 controller 節點，我們在 controller 節點上部屬了 openstack 的 API 服務。 接著我們在 compute 節點上增加一張實體網卡 eth1 讓節點上的 nova agent 和 neutron agent 可以與 contrroller node 溝通。 在這個架構中，所有的 service 和 agent 是直接安裝在節點上的程式，因此可以透過 eth1 實體網卡的 ip 直接進行訪問。 在圖中的 linux bridge 換了顏色，因為這個 bridge 並不是由我們手動建立出來的而是由 neutron agent 自行建立出來的。同時當 nova 建立 VM 後，neutron 也會建立 veth pair 連接 bridge 和 VM。 到此我們就建立了最簡單的 openstack 網路，在這個網路架構下雖然節點本身的網路從 bridge 分離出來成為了獨立的網卡，但是節點本身和所有的 VM 還是同處在一個 L2 網路內，因此在這個網路架構中，物理網路和覆蓋網路是完全等價通透的。\n到此我們就建構了 neutron 的第一個種網路模式 flat。Neutron 的不同模式差異在於說 neuitron 管理的 bridge 是如何接入物理網路的。在 flat 模式下對外網卡直接橋接到 bridge 上，不對封包做處理。\n在 openstack 中有 tenant networks 租戶網路的概念，我們可以在 openstack 上切出若干個獨立的 L2 網路，分給不同的 project 和 vms 使用。在使用單一個 flat 網路的時候，這件事是做不到了，因為所有 VM 都在同一個 L2 網路內。\n直觀的第一個方法當然是切出多個 flat network 給不同的 tenat 使用，然而這樣使用的限制非常大，首先我們需要替每個 flat 預先切出獨立的 L2 網路，在這個範例中我們使用獨立的網卡和交換機來分割，非常麻煩。\n因此比較可行的方法是使用 vlan 或 vxlan 來切割租戶網路，並將 vlan、vxlan 的建立管理交給 neutron agent 來處理。\nVlan networ # 首先是 vlan 模式，在 vlan 模式下我們一樣提供一個網路介面 eth0 給 neutron 使用，並一樣假設所有節點上的 eth0 介面都在 L2 互通。\n接著當我們透過 openstack 建立網路並指定為 vlan type 時，neutron 會替該網路分配一個 vlan id，並建立對應的 vlan 網路卡，vlan 網路卡是 linux 本身的功能，經過 vlan 網路卡的封包會從該網路卡榜定的原始網卡送出去 (eth0)，但是封包會加上 vlan 網卡綁定的 vlan id。\n如上圖所示，我們切割出了 vlan 356 和 vlan 357 兩個租戶網路，並統一透過 eth0 物理網路與其他節點溝通。\nVxlan network # 首先我們先介紹一下 vxlan。和 vlan 的功能類似，vxlan 的功能也是在一個網路內切割出多個覆蓋網路，不過教於 vlan，vxlan 有以下特點：\nvlan id 只能夠切出 4096 個網路，但是 vxlan id 可以切出 2 的 24 次方個網路。 vlan 只是在 ethernet header 和 IP header 中間插了一個 vlan header，因此 vlan 只能在 L2 的物理網路內傳輸。但是 vxlan 的封裝方式是將整個 L2 封包加上一個 vxlan header 後，封裝在一個 UDP 封包內，因此透過外面的 UDP 封包，vxlan 能夠跨越 L3 網路建立 L2 的通道，連結兩個不相連的 L2 網路。 在 Linux 上建立 vxlan 介面時要指定兩個東西，一個是 vni (vxlan id), 另一個是 local ip，當封包通共 vxlan 介面時，會被加上包含 vni 的 vxlan id，外部的 UDP 封包則會利用 local ip 當作 src ip，同時利用該 local ip 對應的 interface 來收發 vxlan 封包。\n在 vlan 網路裡面，由於 vlan 一樣是 L2 封包，因此封包是透過 L2 的廣播學習機制來傳遞的。然而如前面所示，vxlan 封包會封裝成 UDP 封包，因此 vxlan 靠的是 L3 的路由機制來傳遞。\n在 Linux 上這個路由機制有幾種作法\n在建立 vxlan interface 時直接指定 remote ip，建立成點對點的 vxlan tunnel\n透過 linux bridge forwarding database，下達 forwarding rule，指定當 L2 封包的 mac address 是多少的時候要送到哪個 remote ip\n最後也是 openstack 使用的方式，將封包發送到一個 L3 廣播地址 (例如 239.1.1.1)，linux 會自動在 L2 加上 multicast mac address，所有在同一個 L2 物理網路上的設備就能夠同時收到 vxlan 的封包。回到 openstack 上，在設置 vxlan 時和 vlan、flat 不太一樣，我們不用指定綁定的網路介面 (eth0)，而是要指定 local_ip 跟 vxlan_group，前者對應到綁定解面 (eth0) 的 IP，後者則是 vxlan 廣播域的 IP (239.1.1.1)。後面就和 vlan 一樣，在 openstack 上建立的網路時，netruon 為該網路建立對應的 vxlan 介面和 linux bridge，來提供給該網路的虛擬機使用。\n雖然在一台節點上面不太可能開到 4096 個 tenant，但是在整個 openstack 叢集內可以將不同 tenant 分配在不同的節點上，因此 vxlan 是有意義的。\nOpenstack 網路設置 # 在 openstack neutron 中，我們要設定每個節點上的 ml2 設定檔來提供 neutron 網路的基本資訊。路徑在 /etc/neutron/plugins/ml2/。\n# /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,vxlan,local tenant_network_types = vxlan,vlan,flat mechanism_drivers = linuxbridge extension_drivers = port_security 首先對於上述三種不同的網路類型，如果要在 openstack 內使用需要指定啟用對應的 type_drivers。(除了前面介紹的三種網路類型，還有 local 跟 gre 這兩種，前者用於單機情況 neutron 的 linux bridge 不連接到任何對外網路，後者使用 gre tunnel 來取代 vxlan tunnel，這邊不多作介紹) 接著 tenant_network_types 表示在 openstack 設可以用於建立 project 租戶網路的網路類型，同時順序也代表了在建立網路時預設使用的網路類型優先順序。 mechanism_drivers 這邊指定是 linuxbridge，前面提到 neutron 會建立 bridge 來連接 VM 跟外部網路介面，其實這邊還有很多其他選項，例如 open vSwitch，來提供更多網路管理功能，但是這邊就只以 linux bridge 為主要介紹對象。 extension_drivers 則可以載入其他 plugin 來提供 QoS、安全管理的功能。 接著我們要設定物理網路和網路介面的對應，在 flat 和 vlan 網路模式下，我們都需要將網路與一張可以連接到物理網路的網路介面做綁定 (前面的 eth0 或 eth1)，但是在實際情況中，每個節點上網路介面卡的名稱可能不相通，因此需要在每個節點上指定物理網路和網路介面的對應。\n# /etc/neutron/plugins/ml2/linuxbridge_agent.ini [linux_bridge] physical_interface_mappings = vlan1:eth0,flat1:eth1,flat2:eth2 接著對不同的網路類型有各自的網路設置，首先是 flat，要指定的東西很簡單，就是可以用於建立 flat network 的物理網路，如果所有網路都可以，則指定為 *\n# /etc/neutron/plugins/ml2/ml2_conf.ini [ml2_type_flat] flat_networks = flat1, flat2 # Example:flat_networks = * 在 vlan 部分則要指定在每個物理網路上允許的租戶網路 vlan id，格式是\u0026lt;physical_network\u0026gt;[:\u0026lt;vlan_min\u0026gt;:\u0026lt;vlan_max\u0026gt;]\n# /etc/neutron/plugins/ml2/ml2_conf.ini [ml2_type_vlan] network_vlan_ranges = vlan1:101:200,vlan1:301:400 最後 vxlan 分成兩個部分，首先和 vlan 類似，我們要指定可使用的 vxlan vni 和廣播域 ip\n# /etc/neutron/plugins/ml2/ml2_conf.ini [ml2_type_vxlan] vxlan_group = 239.1.1.1 vni_ranges = 1:1000 接著我們要設置綁定的網卡 ip\n# /etc/neutron/plugins/ml2/linuxbridge_agent.ini [vxlan] enable_vxlan = True vxlan_group = 239.1.1.1 # VXLAN local tunnel endpoint local_ip = 192.168.56.101 l2_population = False ttl = 32 這邊有一個特別的東西是 l2 population，l2 population 是一種降低廣播封包造成網路負載的方式，在傳統網路內 ARP 封包需要廣播到所有的節點上，大大增加的網路的負擔，透過 l2 population 提供的 proxy ARP 機制，ARP 會在節點上直接由 neutron 回應，而不用透過物理網路廣播到所有節點上，大大降低網路負擔，由於 openstack 內所有的 VM IP、網卡 mac address 都會受到 openstack 的管理，因此 openstack 可以做到 proxy ARP。如果需要啟用 l2 population 則需要額外的 driver，這邊一樣不做詳細介紹。\n最後是網路安全的部分，在 openstack 我們可以使用 iptables 來建立 VM 的網路管理，iptables 會阻擋所有進出虛擬機網路的流量，並透過 security group 來下達 iptables 的規則允許特定流量進出。\n# /etc/neutron/plugins/ml2/linuxbridge_agent.ini [securitygroup] firewall_driver = iptables enable_security_group = True # /etc/neutron/plugins/ml2/ml2_conf.ini [securitygroup] enable_security_group = True enable_ipset = True Provider vs Self-service networks # 接著我們要介紹 openstack 裡面另外一個重要的網路概念，provider network 跟 self-service network 的差別。\n首先要特別注意的是，這邊提到的分類方式和前面的網路類型是兩種不同的角度和分類方式。\n雖然 flat network 通常會搭配 provider networks，vlan 和 vxlan 通常會搭配 self service network 但是這並不是一定而且必要的。\n前面的網路類型我們探討的是要如何在跨節點的網路架構下提供一個或多個 L2 網路讓 VMs 之間可以彼此連接。\n這邊 provider 和 self-service 的差別則是要探討，如何提供前面切割出來的網路 L3 的網路功能 (gateway, routing…)\n在 provider 網路模式下，我們假設 L3 的網路功能可以直接由物理網路提供，因此 openstack 只負責提供 DHCP，以及將虛擬機接到物理網路上，其餘的 gateway, routing 功能，openstack 只假設存在，不多做處理。在最簡單的 flat 網路模式下這麼做當然是非常簡單可行的，最簡單的方法就是將 flat 網路直接接入節點間 openstack 管理用的網段，並與節點共用 ip subnet，直接由物理網路提供功能。\n然而在 vlan 和 vxlan 模式下使用 proider 網路就不是這麼好用了，前面提到相較於 flat，vlan 和 vxlan 的特點就是能夠動態建立獨立的租戶網路，如果要使用 provider 機制，變成說我們要導入額外的自動化方式，替每個 vlan 或 vxlan 提供 gateway 的功能，因此通常會使用 self-service network。\nself-service network 指的是 openstack self service，也就是由 openstack 本身來提供 L3 網路的 gateway, router 的功能。\n為了要提供 router 的功能，openstack 的作法是使用 network namespace。\n首先我們需要在一個節點上部署 neutron 的 L3 agent。 當我們透過 openstack 的 API 建立一個 virtual router 的時候，netron L3 agent 會在節點上建立一個獨立的 network namespace 當作這台 virtual router 的主體。\n接著我們就可以將不同的租戶網路接入 router，透過 linux 本身的 iptables, routing 的功能來提供租戶網路間 routing 的功能。\n我們也可以透過把其中一個租戶網路或 flat 網路接入物理網路，使用 provider network 的方式，其他的租戶網路可以透過 virtual router routing 到 provider network 的租戶網路來上網。\nOpenstack self-service 指令 # 這邊補充一下如何透過 cli 在 openstack 上建立 router 還有把租戶網路加入 router\nopenstack router create router1 首先建立 router1\nopenstack port create --network net1 net1-router-port 接著為了讓租戶網路能夠“接線”到 router，我們需要幫網路建立一個 port\nopenstack router add port router1 net1-router-port 最後把 port 接到 router 上\n","date":"October 5 2022","externalUrl":null,"permalink":"/posts/openstack-deployment-serial-1-network-architecure-and-config/","section":"文章","summary":"","title":"OpenStack 架設系列 (1) - 網路架構解析及設置","type":"posts"},{"content":"","date":"September 5 2022","externalUrl":null,"permalink":"/tags/cni/","section":"標籤","summary":"","title":"CNI","type":"tags"},{"content":" 前言 # 這次來嘗試寫寫看 spec 導讀，今天要講的是 Container Network Interface (CNI) Specification。CNI 是 CNCF 的一個專案，這個專案包含了今天要講的 CNI SPEC 以及基於這個 SPEC 開發出來 libraries 還有一系列的 CNI plugins。\nCNI 定義了一套 plugin-based 的網路解決方案，包含了設定還有呼叫執行的標準，使用 CNI 最知名的專案應該就是 kubernetes 了，在 k8s 環境中，容器的網路並不是直接由 container runtime (ex. Docker) 處理的，container runtime 建立好容器後，kubelet 就會依照 CNI 的設定和通訊標準去呼叫 CNI plugin，由 CNI plugin 來完成容器的網路設置。\nContainer runtime 在執行容器建立時，會依據設定檔 (後續稱為 network configuration) 的指示，依序呼叫一個或多個的 CNI plugin 來完成網路功能的設置，一個網路功能的設置可能會需要多個 CNI plugins 之間的相互合作，或著由多個 CNI plugin 提供多個不同的網路功能。\nOverview # 目前 CNI spec 的版本是 1.0.0，CNI 的 repository 裡面除了 spec 文件外也包含了基於 CNI spec 開發的 go library，用於 CNI plugin 的開發。不過 CNI spec 和 library 的版本號是獨立的，當前的 library 版本是 1.1.2。\n首先要對幾個基本名詞定義\ncontainer 是一個網路獨立的環境，在 linux 上通常透過 network namespace 的機制來切割，不過也可能是一個獨立的 VM。 network 是 endpoints 的集合，每個 endpoint 都有著一個唯一是別的的地址 (通常是 ip address) 可用於互相通訊。一個端點可能是一個容器、VM 或著路由器之類的網路設備。 runtime 指的是呼叫執行 CNI plugin 的程式，以 k8s 來說，kubelet 作為這個角色 plugin 指的是一個用來完成套用特定網路設定的程式。 CNI Spec 包含五個部分\nNetwork configuration format: CNI 網路設定的格式 Execution Protocol: runtime 和 CNI plugin 之間溝通的 protocol Execution of Network Configurations: 描述 runtime 如何解析 network configuration 和操作 CNI plugin。 Plugin Delegation: 描述 CNI plugin 如何呼叫 CNI plugin。 Result Types: 描述 CNI plugin 回傳的結果格式。 Section 1: Network configuration format # CNI 定義了一個網路設定的格式，這個格式用於 runtime 讀取的設定檔，也用於 runtime 解析後，plugin 接收的格式，通常來說設定檔是以 ** 靜態 ** 檔案的方式存在主機上，不會任意變更。\nCNI 使用了 JSON 作為設定檔的格式，並包含以下幾個主要欄位\ncniVersion (string): 對應的 CNI spec 版本，當前是 1.0.0 name (string): 一個在主機上不重複的網路名稱 disableCheck (boolean): 如果 disableCheck 是 true 的話，runtime 就不能呼叫 CHECK，CHECK 是 spec 定義的一個指令用來檢查網路是否符合 plugin 依據設定的結果，由於一個網路設定可能會呼叫多個 CNI plugins，因此可能會出現網路狀態符合管理員預期，但是 CNI plugin 之間衝突檢查失敗的情況，這時就可以設定 disableCheck plugin (list): CNI plugin 設定 (Plugin configuration object) 的列表。 Plugin configuration object # plugin configuration object 包含了一些明定的欄位，但 CNI plugin 可能根據需要增加欄位，會由 runtime 在不修改的情況下送給 CNI plugin。\n必填： type (string): CNI plugin 的執行檔名稱 可選欄位 (CNI protocol 使用): capabilities (dictionary): 定義 CNI plugin 支援的 capabilities，後面在 section 3 會介紹。 保留欄位：這些欄位是在執行過程中，由 runtime 生成出來的，因此不應該在設定檔內被定義。 runtimeConfig args 任何 cni.dev/ 開頭的 key 可選欄位：不是 protocol 定義的欄位，但是由於很多 CNI plugin 都有使用，因此具有特定的意義。 ipMasq (boolean): 如果 plugin 支援的話，會在 host 上替該網路設置 IP masquerade，如果 host 要做為該網路的 gateway 的話，可能需要該功能。 ipam (dictionary): IPAM (IP Address Management) 設置，後面在 section 4 會介紹。 dns (dictionary): DNS 設置相關設置 nameservers (list of strings): DNS server 的 IP 列表 domain (string): DNS search domain search (list of strings),: DNS search domain 列表 options (list of strings): DNS options 列表 其他欄位：CNI plugin 自己定義的額外欄位。 設定檔範例\n{ \u0026#34;cniVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;dbnet\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, //plugin specific parameters \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;keyA\u0026#34;: [\u0026#34;some more\u0026#34;, \u0026#34;plugin specific\u0026#34;, \u0026#34;configuration\u0026#34;], \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, //ipam specific \u0026#34;subnet\u0026#34;: \u0026#34;10.1.0.0/16\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.1.0.1\u0026#34;, \u0026#34;routes\u0026#34;: [ {\u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;} ] }, \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [ \u0026#34;10.1.0.1\u0026#34; ] } }, { \u0026#34;type\u0026#34;: \u0026#34;tuning\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;mac\u0026#34;: true }, \u0026#34;sysctl\u0026#34;: { \u0026#34;net.core.somaxconn\u0026#34;: \u0026#34;500\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} } ] } Section 2: Execution Protocol # CNI 的工作模式是由 container runtime 去呼叫 CNI plugin 的 binaries，CNI Protocol 定義了 runtime 和 plugin 之間的溝通標準。\nCNI plugin 的工作是完成容器網路介面的某種設置，大致上可以分成兩類\nInterface plugin: 建立容器內的網路介面，並確保其連接可用 Chained plugin: 調整修改一個已建立的介面 (可能同時會需要建立更多額外的介面) Runtime 透過兩種方式傳遞參數，一是透過環境變數，二是透過 stdin 傳遞 Section 1 定義的 configuration。如果成功的話，結果會透過 stdout 傳遞，如果失敗的話就會錯誤資訊會透過 stderr 傳遞。configuration 和結果、錯誤都是使用 JSON 格式。\nRuntime 必須在 Runtime 的網路執行，大多數情況下就是在主機的預設 network namespace/dom0\nParameters # Protocol 的參數都是透過環境變數來傳遞的，可能的參數如下\nCNI_COMMAND: 當前執行的 CNI 操作 (可能是 ADD, DEL, CHECK. VERSION) CNI_CONTAINERID: 容器的 ID CNI_NETNS: 容器網路空間的參考，如果是使用 namespaces 的方式來切割的話，就是 namespce 的路徑（e.g. /run/netns/[nsname] ) CNI_IFNAME: 要建立在容器內的介面名稱，如果 plugin 無法建立該名稱則回傳錯誤 CNI_ARGS: 其他參數，Alphanumeric 格式的 key-value pairs，使用分號隔開”e.g. FOO=BAR;ABC=123 CNI_PATH: CNI plugin 的搜尋路徑，因為 CNI 存在 CNI plugin 呼叫 CNI plugin 的情況，所以需要這個路徑。如果包含多個路徑，使用 OS 定義的分隔符號分割。Linux 使用 : ，Windows 使用 ; Errors # 如果執行成功，CNI Plugin 應該回傳 0。如果失敗則回傳非 0，並從 stderr 回傳 error result type structure。\nCNI operations # ADD # CNI plugin 會在 CNI_NETNS 內建立 CNI_IFNAME 介面或對該介面套用特定的設置\n如果 CNI plugin 執行成功，應該從 stdout 回傳一個 result structure。如果 plugin 的 stdin 輸入包含 prevResult，他必些直接把 prevResult 包在 result structure 內或對他修改後在包在 result structure 內，runtime 會把前一個 CNI 輸出的 prevResult 包在下一個 CNI 的 stdin 輸入內。\n如果 CNI plugin 嘗試建立介面時，該介面已經存在，應該發出錯誤。\nRuntime 不應該在沒有 DEL 的情況下對一個 CNI_CONTAINERID 容器的一個 CNI_IFNAME 介面多次呼叫 ADD。不過可能對同一個 container 的不同介面呼叫 ADD。\n必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 、 CNI_NETNS 和 CNI_IFNAME 。 CNI_ARGS 和 CNI_PATH 為可選項。\nDEL # 移除 CNI_NETNS 內的 CNI_IFNAME 介面，或還原 ADD 套用的網路設定\n通常來說，如果要釋放的資源已經不存在，DEL 也應該視為成功。例如容器網路已經不存在了，一個 IPAM plugin 應該還是要正常的釋放 IP 和回傳成功，除非 IPAM plugin 對容器網路存在與否有嚴格的要求。即便是 DHCP plugin，雖然執行 DEL 操作的時侯，需要透過容器網路發送 DHCP release 訊息，但是由於 DHCP leases 有 lifetime 的機制，超時後會自動回收，因此即便容器網路不存在，DHCP plugin 在執行 DEL 操作時，也應該該回傳成功。\n如果重複對一個 CNI_CONTAINERID 容器的 CNI_IFNAME 介面執行多次 DEL 操作，plguin 都應該回傳，即便介面已經不存在或 ADD 套用的修改已經還原。\n必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID 和 CNI_IFNAME。 CNI_NETNS、CNI_ARGS 和 CNI_PATH 為可選項。\nCHECK # Runtime 透過 CHECK 檢查容器的狀態，並確保容器網路符合預期。CNI spec 可以分為 plugin 和 runtime 的兩部分\nPlugin:\nplugin 必須根據 prevResult 來判斷介面和地址是否符合預期 plugin 必須接受其他 chained plugin 對介面修改的結果 如果 plugin 建立並列舉在 prevResult 的 CNI Result type 資源 (介面、地址、路由規則) 不存在或不符合預期狀態，plugin 應該回傳錯誤 如果其他不在 Result type 內的資源不存在或不符合預期也應該回垂錯誤。可能的資源如下 防火牆規則 流量控管 (Traffic shaping controls) IP 保留 (Reservation) 外部依賴，如連接所需的 daemon 如果發現容器網路是無法訪問的應該回傳錯誤 plugin 必須在完成 ADD 後能夠立即處理 CHECK 指令，應此 plguin 應該要容忍非同步完成的網路設置在一定的時間內不符合預期。 plugin 執行 CHECK 時，應該呼叫所有 delegated plugin 的 CHECK，並把 delegated plugin 的錯誤傳給 plugin 的呼叫者 Runtime:\nRuntime 不應該在未執行 ADD 前或已經 DEL 後沒在執行一次 ADD 的容器執行 CHECK 如果 configuration 的 disableCheck 為真，runtime 不應呼叫 disableCheck Runtime 呼叫 CHECK 時，configuration 必須在 prevResult 包含前一次 ADD 操作時最後一個 plugin 的 Result。Runtime 可能會使用 libcni 提供的 Result caching 功能。 如果其中一個 plugin 回傳錯誤，runtime 可能不會呼叫後面 plugin 的 CHECK Runtime 可能會在 ADD 完後的下一刻一直到 DEL 執行前的任何時間執行 CHECK Runtime 可能會假設一個 CHECK 失敗的容器會永久處於配置錯誤的狀態 必有的輸入包含 STDIN JSON configuration object、 CNI_COMMAND、CNI_CONTAINERID、CNI_NETNS 和 CNI_IFNAME。 CNI_ARGS 和 CNI_PATH 為可選項。\n除了 CNI_PATH 以外的參數必須和 ADD 時一致。\nVERSION # Plugin 透過 stdout 輸出 JSON 格式的 version result type object，用於查看 CNI plugin 的版本。\nStdin 輸入的 JSON 物件只包含 cniVersion。 環境變數參數只需要 CNI_COMMAND。\nSection 3: Execution of Network Configurations # 這個章節描述了 runtime 如何解析 network configuration，並執行 CNI plugins。Runtime 可能會想要新增、刪除、檢查 configuration，並對應到 CNI plugin 的 ADD, DELETE, CHECK 操作。這個章節也定義 configuration 是如何改變並提供給 plugin 的。\n對容器的 network configuration 操作稱之為 attachment，一個 attachment 通常對應到一個特定 CNI_CONTAINERID 容器的 CNI_IFNAME 介面。\n生命週期 # Runtime 必須在呼叫任何 CNI Plugin 之前，為容器建立新的 network namespace Runtime 一定不能同時在一個容器執行多個 plugin 命令，但是同時處理多個容器是可以的。因此 plugin 必須能夠處理多容器 concurrency 的問題，並在共享的資源 (e.g. IPAM DB) 實作 lock 機制 Runtime 必須確保 ADD 操作後必定執行一次 DEL 操作，即便 ADD 失敗。唯一可能的例外是如結點直接丟失之類的災難性事件。 DEL 操作可能連續多次執行 network configuration 在 ADD 和 DEL 操作之間，以及不同的為 attachment 之間應該保持一致不變 runtime 必須負責清除容器的 network namespace Attachment Parameters # Network configuration 在不同的 attachments 間應該保持一致。不過 runtime 會傳遞其他每個 attachment 獨立的參數。\nContainer ID: 對應到 Section 2 CNI_CONTAINERID 環境變數 Namespace: 對應到 CNI_NETNS 環境變數 Container interface name: 對應到 CNI_IFNAME 環境變數 Generic Arguments: 對應到 CNI_ARGS 環境變數 Capability Arguments: CNI plugins search path: 對應到 CNI_PATH 環境變數 Adding an attachment # 對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟\n根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 生成送給 plugin sdtin 的 configuration 只有第一個執行的 plugin 不會帶 prevResult 欄位，後續執行的 plugin 都會把前一個的 plugin 的結果放在 prevResult 執行 plugin 的執行檔。設置 CNI_COMMAND=ADD，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Runtime 必須持久保存最後一個 plugin 的結果，用於 check 和 delete 操作。\nDeleting an attachment # 刪除 attachment 和添加基本上差不多的，差別是\nplugin 的執行順序是反過來的，從最後一個開始 prevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 對 configuration 的 plugins 欄位反序的每個 plugin configuration 執行以下步驟\n根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=DEL，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Checking an attachment # 如同 Section 2 所述，Runtime 會透過 CNI plugin 檢查每個 attachment 是否正常運作中。 需注意的是 Runtime 必須使用和 add 操作時一致的 attachment parameters\n檢查和添加只有兩個差別\nprevResult 欄永遠是上一次 add 操作時，最後一個 plugin 的結果 如果 network configuation 中 disableCheck 為真，則直接回傳成功 對 configuration 的 plugins 欄位的每個 plugin configuration 執行以下步驟\n根據 type 欄位尋找 CNI plugin 的執行檔，如果找不到則回傳錯誤 根據 plugin configuration 和上次 ADD 執行的 result 生成送給 plugin sdtin 的 configuration 執行 plugin 的執行檔。設置 CNI_COMMAND=CHECK，提供前面定義的環境變數，以及透過 standard in 傳輸生成出來的 configuration 如果 plugin 回傳錯誤，中斷執行並回傳錯誤給 caller Deriving execution configuration from plugin configuration # 在 add, delete, check 操作時，runtime 必須根據 network configuration 生成出 plugin 可以存取的 execution configuration (基本對應到 plugin configuration)，並填入內容。\n如同 section 2 所述，execution configuration 使用 JSON 格式並透過 stdin 傳給 CNI plugin。\n必要的欄位如下： cniVersion: 同 network configuraion 中 cniVersion 的值 name: 同 network configuraion 中 name 的值 runtimeConfig: runtime 需要和 plugin 可提供的 capabilities 的聯集 (capability 會在後面討論) prevResult: CNI plugin 回傳的 Result type 結果 capabilities 欄位必須被移除 其他 plugin configuration 的欄位應該被放入 execution configuration Deriving runtimeConfig # 相對於靜態的 network configuration 來說，runtime 可能會需要根據每個不同的 attachment 產生動態參數。 雖然 runtime 可以透過 CNI_ARGS 傳遞動態參數給 CNI plugin，但是我們沒辦法預期說 CNI plugin 會不會接收這個參數。透過 capabilities 欄位，可以明定 plugin 支援的功能，runtime 根據 capabilities 及需求，動態生成設定並填入 runtimeConfig。CNI spec 沒有定義 capability，但是比較通用的 capability 有列舉在另外一份 文件。\n以 kubernetes 常用的 Node port 功能來說，需要 CNI plugin 支援 portMappings 這個 capability。 在 section 1 的定義中，plugin configuration 包含了 capabilities 欄位，在這個欄位填入 portMappings，讓 runtime 知道可以透過該 plugin 處理 port mapping。\n{ \u0026#34;type\u0026#34;: \u0026#34;myPlugin\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } Runtime 執行 CNI plugin 時，會根據 capabilities 生成 runtimeConfig，並填入對應的動態參數。\n{ \u0026#34;type\u0026#34;: \u0026#34;myPlugin\u0026#34;, \u0026#34;runtimeConfig\u0026#34;: { \u0026#34;portMappings\u0026#34;: [ { \u0026#34;hostPort\u0026#34;: 8080, \u0026#34;containerPort\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34; } ] } ... } Section 4: Plugin Delegation # 雖然 CNI 的主要的架構是一系列的 CNI plkugin 依次執行，但這樣的方式有些時候沒辦法滿足 CNI 的需求。CNI plugin 可能會需要將某些功能委託給另外一個 CNI plugin，並存在 plugin 呼叫 plugin 的情況，最常見的案例就是 IP 地址的分配管理。\n通常來說，CNI plugin 應該要指定並維護容器的 IP 地址以及下達必要的 routing rules，雖然由 CNI plugin 自主完成可以讓 CNI plugin 有更大的彈性但是也加重了 CNI plugin 的職責和開發難度，讓不同的 CNI plugin 需要重複開發相同的 IP 地址管理邏輯，因此許多 CNI 將 IP 管理的邏輯委託給另一個獨立的 plugin，讓相關邏輯可以直接被複用。對此除了前面提到的 interface plugin 和 chanined plugin，我們定義了第三類的 plugin - IP Address Management Pligin (IPAM plugin)。\n由主要的 CNI plugin 去呼叫 IPAM plugin，IPAM plugin 判斷網路介面的 IP 地址、gateway、routing rules，並回傳資訊給主要的 CNI plugin 去完成相對應設置。(IPAM plugin 可能會透過 dhcp 之類的 protocl, 儲存在本地的檔案系統資訊或 network configuration 的 ipam section 取得資訊)\nDelegated Plugin protocol # 和 Runtime 執行 CNI plugin 的方式一樣，delegated plugin 也是透過執行 CNI plugin 可執行程式的方式。主要的 plugin 在 CNI_PATH 路徑下搜尋 CNI plugin。delegated plugin 必須接收和主要 plugin 完全一致的環境變數參數，以及主要 plugin 透過 stdin 接收到的完整 execute configuration。 如果執行成功則回傳 0，並透過 stdout 返回 Success result type output。\nDelegated plugin execution procedure # 當 CNI plugin 執行 delegated plugin 時： 在 CNI_PATH 路徑下搜尋 plugin 的可執行程式 使用 CNI plugin 的環境變數參數和 execute configuration，作為 delegated plugin 的輸入 確保 delegated plugin 的 stderr 會輸出到 CNI plugin 的 stderr 當 plugin 執行 delete 和 check 時，必須執行所有的 delegated plugin，並將 delegated plugin 的錯誤回傳給 runtime 當 ADD 失敗時，plugin 應該在回傳錯誤前，先執行 delegated plugin 的 DEL Section 5: Result Types # Plugin 的回傳結果使用 JSON 格式，並有三種 Success Error Version Success # 如果 plugin 的輸入包含 prevResult，輸出必須包含該欄位的值，並加上該 plugin 對網路修改的資訊，如果該 plugin 沒有任何操作，則該欄位必須保持原輸入內容。 Success Type Result 的欄位如下\ncniVersion: 同輸入的 cniVersion 版本 interfaces: 一個該 plugin 建立的 interface 資訊的陣列，包含 host 的 interface。 name: interface 的名子 mac: interface 的 mac address sandbox: 該介面的網路環境參考，例如 network namespace 的路徑，如果是 host 介面則該欄位為空，容器內的介面該值應為 CNI_NETNS ips: 該 plugin 指定的 ip address: CIDR 格式的 ip address (e.g. 192.168.1.1/24) gateway: default gateway (如果存在的話) interface: 介面的 index，對應到前述 interfaces 陣列 routes: plugin 建立的 routing rules dst: route 的目的 (CIDR) gw: nexthop 地址 dns nameservers: DNS server 位置陣列 (ipv4 或 ipv6 格式) domain: DNS 搜尋的 local domain search (list of strings): 有優先度的 search domain options (list of strings): 其他給 dns resolver 的參數 Delgated plugin 可能會忽略不需要的欄位 IPAM 必須回傳一個 abbreviated Success Type result (忽略 interfaces 和 ips 裡面的 interface 欄位) Error # plugin 回傳的錯誤資訊，欄位有 cniVersion, code, msg, details。\n{ \u0026#34;cniVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;code\u0026#34;: 7, \u0026#34;msg\u0026#34;: \u0026#34;Invalid Configuration\u0026#34;, \u0026#34;details\u0026#34;: \u0026#34;Network 192.168.0.0/31 too small to allocate from.\u0026#34; } Error code 0-99 被保留給通用的錯誤，100 以上是 plugin 自定義的錯誤。\nError code 描述 1 不支援的 CNI 版本 2 不支援的 network configuration 欄位，error message 會包含不支援的欄位名稱和數值 3 未知或不存在的容器，這個錯誤同時代表 runtime 不需要執行 DEL 之類的清理操作 4 無效的環境環境變數參數，error message 會包含無效的欄位名稱 5 IO 錯誤，例如無法讀取 stdin 的 execute configuration 6 解析錯誤，例如無效的 execute configuration JSON 格式 7 無效的 network configuration 11 稍後在嘗試，存在暫時無法操作的資源，runtime 應該稍後重試 此外，stderr 也可被用於非 JSON 結構的錯誤訊息，如 log Version # Version Type Result 的欄位如下 cniVersion: 同輸入的 cniVersion 版本 supportedVersions: 一個支援的 CNI 版本陣列 { \u0026#34;cniVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;supportedVersions\u0026#34;: [\u0026#34;0.1.0\u0026#34;, \u0026#34;0.2.0\u0026#34;, \u0026#34;0.3.0\u0026#34;, \u0026#34;0.3.1\u0026#34;, \u0026#34;0.4.0\u0026#34;, \u0026#34;1.0.0\u0026#34;] } Appendix: # 在 CNI SPEC 裏面包含了一些 configuration 和執行過程中 plugin 輸入輸出的範例，可以參考 原始文件 另外還有一份 CNI 的文件 CONVENTIONS，描述許多 spec 裡面沒有定義但是許多 plugin 常用的欄位。\n小節 # 以上是對 CNI spec 1.0.0 的導讀，希望對大家有幫助。\n","date":"September 5 2022","externalUrl":null,"permalink":"/posts/cni-spec-guiding/","section":"文章","summary":"","title":"CNI-Spec-Guiding","type":"posts"},{"content":"","date":"September 5 2022","externalUrl":null,"permalink":"/categories/kubernetes/","section":"分類","summary":"","title":"Kubernetes","type":"categories"},{"content":"","date":"September 5 2022","externalUrl":null,"permalink":"/tags/%E6%96%87%E4%BB%B6%E7%BF%BB%E8%AD%AF/","section":"標籤","summary":"","title":"文件翻譯","type":"tags"},{"content":"","date":"August 28 2022","externalUrl":null,"permalink":"/categories/onos/","section":"分類","summary":"","title":"ONOS","type":"categories"},{"content":"這篇文章會介紹一下，在 ONOS 上 P4 相關的模組和功能，以及怎麼開發 ONOS APP 與設置 ONOS，讓 ONOS 可以控制 p4 switch 的 pipeline。\n背景介紹 # P4Runtime # P4Runtime 是 P4 API Working Group 制定的一套基於 Protobuf 以及 gRPC 的傳輸協定，他可以提供不同的 P4 交換機和不同的 SDN 控制器一套統一的 API 標準，提供控制器直接透過 p4runtime 將編譯出來 p4 pipeline 直接上傳到 p4 switch 上、設置 p4 pipeline 的 table entry 以及接收 packet-in 的封包和 counter 的資訊等功能。\nONOS 架構 # ONOS 使用分層架構和介面抽象的方式隱藏了底層交換機和和控制協定的具體內容，讓上層的應用可以用統一的 API 來管理網路的行為，因此上層網路可以用完全相同的方式來控制 Openflow switch 及 P4 switch，也可以不用理會各個 switch table 順序和具體規則的下達方式。\nONOS Flow programmable # 即便是在北向提供給使用者的 API，ONOS 也對其做了很多層級的抽象，讓使用者可以自由決定要使用哪一個層級的 API。\n在 flow rule 的部分，ONOS 大致上提供了三個層級的抽象，分別是 Flow Rule, Flow Objective 及 Intent ，不論在哪一個層級，我們主要都是要操作兩個集合 Selector 和 Treatment\nSelector 決定了哪些封包受這條 flow rule 管理。一個 Selector 包含了若干個 Criterion ，ONOS 透過 Enum 定義了常用的 Criterion Type，來對應封包的不同欄位，例如 IPV4_SRC , ETH_SRC 等。\nTreatment 則是 Instruction 的集合，一個 instruction 通常對應到對封包的某個欄位進行修改，或著指定封包在交換機的 output port。\n三個抽象層積的差別在於這兩個集合套用到的對象，在最高層級的 Intent ，我們操作的對象是整個網路流，除了 Selector 和 Treatment，我們還要定義整個網路流在 SDN 網路的入口 (Ingress Point) 和出口 (Egress Point)，ONOS 核心的 Intent Service 會幫我們把一個 Intent 編譯成多個 FlowObjective。\n由於 Intent 操作的是整個網路流，在這個層級定義 Output port 是沒有意義的，但是由於在 ONOS 使用的 JAVA API 是共通的，所以 intent service 會忽略掉這個 instruction，這個在 ONOS 的實作上是很重要的觀念，對 treatment 裡的 instructions，底層的編譯器只會取他可以處理的 instructions 往更底層送，對於不可以處理的 instructions，有些會有 warnning log，有些會直接跳 exception，更有的會直接忽略，因此如果 selector 和 treatment 的執行結果不符合我們的預期，有可能是有些不支援的 instruction 在轉換成交換機可以懂得規則的過程中被忽略的。\nFlowObjective 操作的對象是一台網路設備 (通常是一台交換機)，同樣我們定義一個 Selector 和 Treatment ，告訴這台交換機我們要處理哪些封包和怎麼處理。\n最底下到了 Flow Rule 這個層級，Flow rule 這個層級對象是交換機上的一張 table，因此他加入了 table id 這個欄位。一個 FlowObjective 可能會包含多個不同的 instruction，例如我們要修改封包的 mac address，修改 ip 的 ttl 欄位，同時也要決定這個封包的 output port，這些 instruction 在 flow objective 層級可以包含在一個 treatment 內，但是在實際的交換機上這些 instruction 可能分別屬於不同的 table，因此一個 flow objective 會需要對應到一條或多條得 flow rule，這依據底下交換機的不同、傳輸協定的不同而不同，因此 ONOS 引入了 Pipeliner ，Driver 可以實作 Pipeliner 的介面，讓 ONOS 知道如何把 flow objective 轉換成 flow rules。\nP4 in ONOS # 上圖是 p4 在 ONOS 南向架構上的組件\n在 Protocol layer 由 P4Runtime 組件實作 p4runtime procotol，維護 switch 的連線和具體的 gRPC/Protobuf 傳輸內容\n接著是 Driver layer，不同的 p4 switch 在 pipeline 的結構等方面存在差異，因此在 ONOS 設定交換機資訊時要根據不同的 Switch 選擇 driver\n如果是 bmv2 switch 使用 org.onosproject.bmv2，如果是 tofino 交換機使用 org.onosproject.barefoot 最上面是 ONOS 核心，這邊有 translation services 和 pipeconf 架構。p4 交換機的特色是能透過 p4lang 定義出完全不同的 pipeline，在使用 ONOS 控制 p4 switch 的時候，我們就需要針對 pipeline 定義註冊 pipeconf，ONOS 核心可以調用 pipeconf 將 flow objective 或 flow rule 的 flow rule 轉換成真正可以下達到 p4 pipeline table 上的 entry。 在 ONOS 核心定義的這套 pipeconf 及轉換架構被稱之為 Pipeline Independent (PI) framework，因此 ONOS 相關的 class 和 interface 都會有一個 PI 的前綴。\n另外就要提到 Pipeline-agnostic APP 和 Pipeline-aware APP 的差別，這邊指的都是北向介面上面處理網路邏輯的 APP，差別在於 Pipeline-agnostic app 完全不考慮底下的 pipeline，因此通常操作的是 flow objective，而 pipeline aware app 必須知道底層 pipeline 的架構，直接產出特定的 flow rule。\n開發 Pipeline-agnostic APP 的好處是他足夠抽象因此可以應用在各種不同的交換機和網路，但是我們就需要額外實作 pipeliner 等編譯器來做轉換，因此直接如果只針對單一 pipeline 的情況下，直接開發 pipeline aware app 會比較簡單。\nPipeconf 開發 # 在使用 ONOS 控制 p4 switch 時，最基本要做的就是撰寫 pipeline 對應的 pipeconf。一個完整的 Pipeconf 會包含\n從 p4 compiler 拿到 pipeline 資訊檔案，p4info, bmv2 json, Tofino binary…. PipeconfLoader: 一個 pipeconf 的進入點，向 PiPipeconfService 註冊一個或多個 pipeconf，定義 pipeconf 的 id, 對應的 interpreter, pipeliner, p4info 檔案路徑等資訊。 Interpreter: 主要負責兩件事 提供 ONOS 核心資訊並協助將 common flow rule 轉換成 protocol independent flow rule，包含了 table id 的 mapping, 欄位名稱和數值的轉換等 處理 packet-in/packet-out 的封包，當封包從 p4 switch packet-in 到 controller 時，會把 metadata (input port 等資訊) 當作 packet 的一個 header 附加在 packet 中，一起送到 controller，pipeconf 需要解析封包，將封包資訊提取出來。當封包 packet-out 時，同樣需要 metadata 轉換成 pipeline 定義的 packet-out header，附加在封包內送至交換機，pipeline parser 才能重新將資訊解析出來處理。 Pipeliner: 負責將 flow objective 轉換成 flow rule 但是 Interpreter 和 Pipeliner 的功能是不一定要實作的，如果對應的功能沒有被實作，那北向的 APP 就只能呼叫比較底層的 API 而無法調動 flow objective 等功能。\n開發目標 # 下面會用一個非常簡單的 p4 pipeline 作為範例，我們預期要實作一個基本的 Layer 2 switch pipeline，使 ProxyARP APP 和 Reactive Forwarding APP 能夠正常的運作。(使用 bmv2 和 ONOS v2.7.0 開發)\n建立 ONOS APP # 跟任何其他 ONOS 模組一樣，pipeconf 可以作為一個獨立的 ONOS APP 開發，再安裝到 ONOS 上，所以首先建立我們的 simepleswitch APP\nonos-create-app Define value for property \u0026#39;groupId\u0026#39;: me.louisif.pipelines Define value for property \u0026#39;artifactId\u0026#39;: simpleswitch Define value for property \u0026#39;version\u0026#39; 1.0-SNAPSHOT: : Define value for property \u0026#39;package\u0026#39; me.louisif: : me.louisif.pipelines.simpleswitch pom.xml # 在 pipeconf 的開發中，使用到 p4 相關的 api 並沒有被包含在 onos 標準 api 內，需要額外加入 p4runtime-model 這個 dependency。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.onosproject\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;onos-protocols-p4runtime-model\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${onos.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 另外可以在 onos 的 dependency app 列表內加入 pipeline 對應的 driver，這樣啟動 pipeconf 時就會自動啟用相關的 driver app，而不用事前手動啟動。\n\u0026lt;properties\u0026gt; \u0026lt;onos.app.requires\u0026gt;org.onosproject.drivers.bmv2\u0026lt;/onos.app.requires\u0026gt; \u0026lt;/properties\u0026gt; P4 撰寫 # 接著撰寫我們的 p4 檔案，路徑是 src/main/resource/simpleswitch.p4。resource 資料夾在編譯的時候會被附加到編譯出來的 oar 裡面，所以可以直接在 ONOS 執行的時候存取到編譯出來的 p4info 等檔案。完整的檔案在 github 上。\n在 simpleswitch 的 ingress pipeline 內只包含一張 table0，用於 L2 的 packet forwarding，使用 send 這個 action 來將封包丟到指定的 output port，在 bmv2 switch 會定義一個 cpu port，當 egress_port 為該 port number 時，封包就會被送至 ONOS，因此 send_to_cpu 這個 action 只單純做 set egress port 這個動作。\ncontrol table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send(port_t port) { standard_metadata.egress_spec = port; } action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; } action drop() { mark_to_drop (standard_metadata); } table table0 { key = { standard_metadata.ingress_port : ternary; hdr.ethernet.src_addr : ternary; hdr.ethernet.dst_addr : ternary; hdr.ethernet.ether_type : ternary; } actions = { send; send_to_cpu; drop; } default_action = drop; size = 512; } apply { table0.apply (); } } control MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { // 這個後面在介紹 //if (standard_metadata.ingress_port == CPU_PORT) { // standard_metadata.egress_spec = hdr.packet_out.egress_port; // hdr.packet_out.setInvalid (); // exit; // } else { table0_control.apply (hdr, meta, standard_metadata); // } } } P4 編譯 # 編譯 bmv2 pipeline 可以直接複製 ONOS 內建的 basic pipeline 使用的 編譯腳本，將 Makefile 和 bmv2-compile.sh 這兩個檔案複製到 resources 資料夾下，然後修改 Makefile 把 basic 改成 simpleswitch，並刪除 int pipeline。可以簡單下 make 來完成 pipeline 的編譯。\nROOT_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST))))/.. all: p4 constants p4: simpleswitch.p4 @./bmv2-compile.sh \u0026#34;simpleswitch\u0026#34; \u0026#34;\u0026#34; constants: docker run -v $(ONOS_ROOT):/onos -w /onos/tools/dev/bin \\ -v $(ROOT_DIR):/source \\ --entrypoint ./onos-gen-p4-constants opennetworking/p4mn:stable \\ -o /source/java/me/louisif/pipelines/simpleswitch/SimpleswitchConstants.java \\ simpleswitch /source/resources/p4c-out/bmv2/simpleswitch_p4info.txt clean: rm -rf p4c-out/bmv2/* 這個 makefile 主要分為兩個部分。p4 會呼叫 bmv2-compile，編譯出 bmv2 的 p4info 和描述 pipeline 的 json 檔案。constants 則會使用 p4mn 這個 container 生成出一個 SimpleswitchConstants.java 的檔案，這個檔案會把 pipeline 所有 table 名稱、欄位、action 名稱列舉出來，方便 pipeconf 的程式碼直接調用，以 table0 來說，它的完整名稱為 MyIngress.table0_control.table0 ，可以使用 MY_INGRESS_TABLE0_CONTROL_TABLE0 變數來代表。\npublic static final PiTableId MY_INGRESS_TABLE0_CONTROL_TABLE0 = PiTableId.of (\u0026#34;MyIngress.table0_control.table0\u0026#34;); 最小可執行 Pipeconf # 撰寫 PipeconfLoader # 接著我們要先寫一個最小可以動的 pipeconf，只包含 PipeconfLoader.java 這個檔案，路徑是 src/main/java/me/louisif/simpleswitch/PipeconfLoader.java，前文提到 PipeconfLoader 的工作是向 PipeconfService 註冊 pipeconf 的資訊，因此我們幫 PipeconfLoader 加上 Component 的 Annotation，讓 activate function 在 APP 啟動時被呼叫，接著在 activate function 裡面去註冊 pipeconf。\n以我們的例子而言，我們使用的是 bmv2 的 pipeline，所以我們可以這樣寫\nfinal URL jsonUrl = PipeconfLoader.class.getResource (\u0026#34;/p4c-out/bmv2/simpleswitch.json\u0026#34;); final URL p4InfoUrl = PipeconfLoader.class.getResource (\u0026#34;/p4c-out/bmv2/simpleswitch_p4info.txt\u0026#34;); PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build (); piPipeconfService.register (pipeconf); 檔案路徑要填寫相對於 resource 這個資料夾的路徑，另外 addExtension 的內容會根據 switch 的不同而不同，如果我們今天使用的是 tonifo 的 pipeline 那就要改成\nfinal URL binUrl = PipeconfLoader.class.getResource (\u0026#34;/p4c-out/tofino.bin\u0026#34;); final URL p4InfoUrl = PipeconfLoader.class.getResource (\u0026#34;/p4c-out/p4info.txt\u0026#34;); final URL contextJsonUrl = PipeconfLoader.class.getResource (\u0026#34;/p4c-out/context.json\u0026#34;); DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (parseP4Info (p4InfoUrl)) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (TOFINO_BIN, binUrl) .addExtension (TOFINO_CONTEXT_JSON, contextJsonUrl) .build (); piPipeconfService.register (pipeconf); 測試 # 完成 PipeconfLoader.java 就構成了一個可以運作的 pipeconf，為了測試我們使用 opennetworking/p4mn 這個 container 來實驗，p4mn 是一個 mininet 的 docker image，可以很簡單的啟動一個 mininet 的測試拓譜，並使用 bmv2 switch 取代 mininet 原本使用的 openflow switch。\ndocker run -v /tmp/p4mn:/tmp --privileged --rm -it -p 50001:50001 opennetworking/p4mn:stable 使用這個指令可以啟動一個包含一個叫做 bmv2-s1 的 bmv2 switch 和兩個 host\n同時會生成一個 onos 的 netcfg 檔案，路徑 /tmp/p4mn/bmv2-s1-netcfg.json\n{ \u0026#34;devices\u0026#34;: { \u0026#34;device:bmv2:s1\u0026#34;: { \u0026#34;ports\u0026#34;: { \u0026#34;1\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;s1-eth1\u0026#34;, \u0026#34;speed\u0026#34;: 10000, \u0026#34;enabled\u0026#34;: true, \u0026#34;number\u0026#34;: 1, \u0026#34;removed\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;copper\u0026#34; }, \u0026#34;2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;s1-eth2\u0026#34;, \u0026#34;speed\u0026#34;: 10000, \u0026#34;enabled\u0026#34;: true, \u0026#34;number\u0026#34;: 2, \u0026#34;removed\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;copper\u0026#34; } }, \u0026#34;basic\u0026#34;: { \u0026#34;managementAddress\u0026#34;: \u0026#34;grpc://localhost:50001?device_id=1\u0026#34;, \u0026#34;driver\u0026#34;: \u0026#34;bmv2\u0026#34;, \u0026#34;pipeconf\u0026#34;: \u0026#34;me.louisif.pipelines.simpleswitch\u0026#34; } } } } 這個檔案包含了 ONOS P4 需要的所有資訊，主要分為兩個部分，ports 定義了這個交換機所有的 port 資訊。basic 部分，managementAddress 是 ONOS 用來使用 p4runtime 連線到 switch 的路徑，pipeconf 指定了這個 switch 使用的 pipeconf，預設會是 org.onosproject.pipelines.basic，在我們的範例中需要改為 me.louisif.pipelines.simpleswitch。\n只要將 me.louisif.pipelines.simpleswitch 的 app 啟用，並上傳 bmv2-s1-netcfg.json，ONOS 就可以成功連線到這個 bmv2 switch，並正常提供北向的 APP 服務了。\n如何下 flow rule # 完成 pipeconf 後，我們就可以透過下 flow rule 的方式來讓 switch 工作了，為了要讓 h1 和 h2 能夠互相溝通，最簡單的方法就是將所有 port 1 進來的封包送到 port 2、所有從 port 2 進來的封包送到 port 1。\n為此我們需要兩條 table0 的 entry，分別是\nstandard_metadata.ingress_port == 1 (mask 0x1ff) → send port=2 standard_metadata.ingress_port == 2 (mask 0x1ff) → send port=1 由於 standard_metadata.ingress_port 這個 key 是 ternary，因此我們需要包含 mask, port 這個 type 的長度是 9 bits，因為要完全一致，所以 mask 是 0x1ff。\nfinal PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (PiMatchFieldId.of (\u0026#34;standard_metadata.ingress_port\u0026#34;), inPortNumber.toLong (), 0x1ff); final PiAction piAction = PiAction.builder ().withId (PiActionId.of (\u0026#34;MyIngress.table0_control.send\u0026#34;)) .withParameter (new PiActionParam(PiActionParamId.of (\u0026#34;port\u0026#34;), outPortNumber.toLong ())) .build (); final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (PiTableId.of (\u0026#34;MyIngress.table0_control.table0\u0026#34;)).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build (); flowRuleService.applyFlowRules (flowRule); 我們可以透過這樣的方式來下達第一條 table entry，可以發現和平常的 flow entry 不一樣的地方是 table id 使用的是 PiTableId 這個 type，並指定了 table0 的完整 id MyIngress.table0_control.table0，另外 Selector 和 Treatment 分別使用了 matchPi 和 piTableAction 這兩個特別的函數。\n我們這邊將只使用 PiTableId, PiCriterion 和 PiAction 定義的 flow rule 稱之為 PI flow rule，PI flow rule 是 onos 的 p4runtime 可以直接處理的 flow rule，所有的欄位名稱都唯一對應到 p4 pipeline 的某個欄位，前面提到這些 PiTableId, PiMatchFieldId 等都會在 SimpleswitchConstants.java 內被定義，因此可以直接使用來縮短程式碼長度。\nimport static me.louisif.pipelines.simpleswitch.SimpleswitchConstants.*; final PiCriterion.Builder criterionBuilder = PiCriterion.builder () .matchTernary (INGRESS_PORT, inPortNumber.toLong (), 0x1ff); final PiAction piAction = PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (PORT, outPortNumber.toLong ())) .build (); final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchPi (criterionBuilder.build ()).build ()) .withTreatment (DefaultTrafficTreatment.builder ().piTableAction (piAction).build ()).build (); flowRuleService.applyFlowRules (flowRule); 在範例程式碼裡面包含了簡單的 cli 指令實作，因此可以在 ONOS CLI 使用 add-pi-flow-rule \u0026lt;device id\u0026gt; \u0026lt;input port\u0026gt; \u0026lt;output port\u0026gt; 的方式來下達上面的 flow rule。詳情可以 參考檔案。\n當然這樣編寫出來的 flow rule 會產生一個問題，如果相同的 pipeline，我們希望能從 bmv2 移植到 tonifo 上面去使用，由於欄位的名稱會存在差異，因此所有下達 flow rule 的 APP 都需要重新寫，顯然這樣不是一個很好的做法，增加了程式碼維護上的困難，因此 ONOS 加入前面提到的 Interpreter 還有 translator 機制，下一節會介紹如何為 pipeline 編寫 Interpreter 還有用比較通用的方法來下 flow rule，在完成 interpreter 後上述的 flow rule，可以用下面這個我們比較熟悉的方法來下達。\nprivate static final int TABLE0_TABLE_ID = 0; final FlowRule flowRule = DefaultFlowRule.builder () .fromApp (coreService.getAppId (APP_NAME)) .forDevice (deviceId) .forTable (TABLE0_TABLE_ID).makePermanent ().withPriority (65535) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build (); flowRuleService.applyFlowRules (flowRule); 撰寫 Interpreter # 在前一節我們完成了基本的 pipeconf 註冊，並使用 PI flow rule 的方式來控制交換機，但是直接使用 PI flow rule 會降低 APP 的彈性，使移植到不同 pipeline 的困難度提高。另外 SDN 的一個特色是可以使用 packet-in/packet-out 的方式來讓 controller 即時性的處理封包，因此本結會介紹如何實作 Interpreter 來提供 packet-in/packet-out，以及 flow rule translation 的功能。\n要提供一個 pipeline interpreter，我們需要實作 PiPipelineInterpreter 這個介面，要注意的是 Interpreter class 需要繼承 AbstractHandlerBehaviour，然後再 PipeconfLoader 去指定這個實作\npublic class SimpleSwitchInterpreterImpl extends AbstractHandlerBehaviour implements PiPipelineInterpreter { } PiPipeconf pipeconf = DefaultPiPipeconf.builder () .withId (PIPECONF_ID) .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) .addBehaviour (PiPipelineInterpreter.class, SimpleSwitchInterpreterImpl.class) .addExtension (P4_INFO_TEXT, p4InfoUrl) .addExtension (BMV2_JSON, jsonUrl) .build (); 當一條 flow rule 被加入到 ONOS 時，PI framework 會將其翻譯成 PI flow rule，也就是將非 PI * 的欄位轉換成 PI flow rule 的欄位。\nfinal FlowRule flowRule = DefaultFlowRule.builder () .forTable (0) .withSelector (DefaultTrafficSelector.builder ().matchInPort (inPortNumber).build ()) .withTreatment (DefaultTrafficTreatment.builder ().setOutput (outPortNumber).build ()) .build (); Table Id translation # 以前面提到的 port 1 送到 port 2 的 flow rule 來示範，我們需要把 table id 0，轉換成 PiTableId.of (\u0026quot;MyIngress.table0_control.table0\u0026quot;) ，這對應到 Interpreter 的 mapFlowRuleTableId 函數，我們可以定義一個 map 來記錄，table index 跟 Pi table id 之間的關係，然後實作 mapFlowRuleTableId。這邊的 id 0 並不具有特定的意義，只是單純我們 interpreter 定義的 table index，因此當 APP 使用時，需要知道這個 index 對應到的 table 具體是什麼功能，當然通常我們會再加上一層 pipeliner，透過 flow objective 來隱藏 table id 的細節。\nprivate static final Map\u0026lt;Integer, PiTableId\u0026gt; TABLE_MAP = new ImmutableMap.Builder\u0026lt;Integer, PiTableId\u0026gt;() .put (0, MY_INGRESS_TABLE0_CONTROL_TABLE0) .build (); @Override public Optional\u0026lt;PiTableId\u0026gt; mapFlowRuleTableId(int flowRuleTableId) { return Optional.ofNullable (TABLE_MAP.get (flowRuleTableId)); } Selector Translation # 我們通常使用 DefaultTrafficSelector.builder 來定義 Selector。 matchInPort 會在 selector 內加入一個 PortCriterion ，他的 criterion tpye 是 Criterion.Type.IN_PORT ，為此 Interpreter 需要根據 Criterion type，將其轉換成 p4 table 對應的 key，同樣我們使用 map 的方式來維護其關係。\nprivate static final Map\u0026lt;Criterion.Type, PiMatchFieldId\u0026gt; CRITERION_MAP = new ImmutableMap.Builder\u0026lt;Criterion.Type, PiMatchFieldId\u0026gt;() .put (Criterion.Type.IN_PORT, HDR_STANDARD_METADATA_INGRESS_PORT) .put (Criterion.Type.ETH_SRC, HDR_HDR_ETHERNET_SRC_ADDR) .put (Criterion.Type.ETH_DST, HDR_HDR_ETHERNET_DST_ADDR) .put (Criterion.Type.ETH_TYPE, HDR_HDR_ETHERNET_ETHER_TYPE) .build (); @Override public Optional\u0026lt;PiMatchFieldId\u0026gt; mapCriterionType(Criterion.Type type) { return Optional.ofNullable (CRITERION_MAP.get (type)); } 可能有些人會有些疑惑說，p4 table key 可能會是 exact 或是 ternary，那 ONOS 要怎麼把 matchInPort 轉換成 ternary mask 0x1ff\n前面提到在註冊 pipeconf 時，我們透過 .withPipelineModel (P4InfoParser.parse (p4InfoUrl)) 載入 p4info (在 ONOS 內稱之為 PiPipelineModel)，每個 key 具體的類型和資料長度會包含在 p4info 內，PI framework 在轉換時會根據不同 criterion type 和 key type 的不同和需求自動做轉換。\nTreatment Translation # 不像 table id 和 criterion，在 p4 內，每個 action 是一個可帶參數的 function，和 ONOS 定義的 treatment 不存在簡單的對應關係，因此 interpreter 定義了 mapTreatment，輸入是 treatment 和 table id，輸出是 PiAction，讓 interpreter 完整的處理整個 treatment。\n@Override public PiAction mapTreatment(TrafficTreatment treatment, PiTableId piTableId) throws PiInterpreterException { // 檢查 table id 是否有效，由於只有 table0 一張 table，因此這邊直接檢查 table id 是不是 table 0 if (!piTableId.equals (MY_INGRESS_TABLE0_CONTROL_TABLE0)) { throw new PiInterpreterException(\u0026#34;Unsupported table id: \u0026#34; + piTableId); } // 我們的 pipeline 只支援 set output port 一個 instruction if (treatment.allInstructions ().size () != 1 || !treatment.allInstructions ().get (0).type ().equals (Instruction.Type.OUTPUT)) { throw new PiInterpreterException(\u0026#34;Only output instruction is supported\u0026#34;); } PortNumber port = ((Instructions.OutputInstruction) treatment.allInstructions ().get (0)).port (); // 像 controller、flooding 這些特別的 port，稱之為 Logical port if (port.isLogical ()) { if (port.exactlyEquals (PortNumber.CONTROLLER)) { // 我們支援使用 send_to_controller 這個 action 將封包送到 ONOS return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND_TO_CPU) .build (); } else { throw new PiInterpreterException(\u0026#34;Unsupported logical port: \u0026#34; + port); } } 一般的使用用 send 這個 action 來傳送封包 \u0026lt; br\u0026gt; return PiAction.builder ().withId (MY_INGRESS_TABLE0_CONTROL_SEND) .withParameter (new PiActionParam(PORT, port.toLong ())) .build (); } 到此我們已經完成了 flow rule 的轉換，可以使用 ONOS 標準的 flow rule 來操作 pipeline 了。\n如果希望對轉換機制有更詳細的瞭解可以查看 ONOS 原始碼\nPacket-in/Packet-out # Interpreter 另外一個重要的功能是使 pipeline 支援 packet-in/packet-out 的功能。為此我們需要先修改我們的 p4 pipeline。\n首先我們會先加入兩個特別的 header，並使用 controller_header anotation 標記，@controller_header (\u0026quot;packet_in\u0026quot;) 來得知這個 packet_in_header_t 對應的是 packet-in 時，附加在這個封包的 meta data，通常會定義 ingress port 來表示封包的 input port。同樣的 packet_out_header_t 是 packet-out 時，ONOS 送來 pipeline 處理的 meta data\n@controller_header (\u0026#34;packet_in\u0026#34;) header packet_in_header_t { bit\u0026lt;7\u0026gt; _padding; bit\u0026lt;9\u0026gt; ingress_port; } @controller_header (\u0026#34;packet_out\u0026#34;) header packet_out_header_t { bit\u0026lt;7\u0026gt; _padding; bit\u0026lt;9\u0026gt; egress_port; } 接著我們要修改 header、parser，從 ONOS packet out 出來的封包對 p4 交換機來說相當於從特定一個 port 送進來的封包，因此一樣會經過整個 pipeline\n以 bmv2 來說，controller 的 port number 可以自由指定，在我們使用的 p4mn container 內這個 port 被定義成 255，為此我們在 pipeline 的 p4 檔案內定義了 CPU_PORT 巨集為 255\n我們將 packet-in/packet-out header 加入到 headers 內，當 packet-out 時，packet_out 會在封包的開頭，因此在 parser 的 start state，我們先根據 ingress port 是不是 CPU_PORT 來決定是不是要 parse packet_out header。\nstruct headers_t { packet_in_header_t packet_in; packet_out_header_t packet_out; ethernet_h ethernet; } parser MyParser(packet_in packet, out headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { state start { transition select(standard_metadata.ingress_port) { CPU_PORT: parse_packet_out; default: parse_ethernet; } } state parse_packet_out { packet.extract (hdr.packet_out); transition parse_ethernet; } state parse_ethernet { packet.extract (hdr.ethernet); transition accept; } } 在 Ingress 部分，如果是 packet-out packet，我們沒有必要讓他經過整個 ingress pipeline，為此我們直接將 egress_spec 設置為 packet_out header 內的 egress_port，然後直接呼叫 exit。\ncontrol MyIngress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.ingress_port == CPU_PORT) { standard_metadata.egress_spec = hdr.packet_out.egress_port; hdr.packet_out.setInvalid (); exit; } table0_control.apply (hdr, meta, standard_metadata); } } 我們的 table0 則加入了 send_to_cpu 這個 action，做的事情就是把 egress_spec 設定成 CPU port。\ncontrol table0_control(inout headers_t hdr, inout local_metadata_t local_metadata, inout standard_metadata_t standard_metadata) { action send_to_cpu() { standard_metadata.egress_spec = CPU_PORT; } } 當封包要 packet-in 到 ONOS 時，需要將 packet-in header 設為 valid，並填入對應的資料，這個部分會再 Egress pipeline 完成\ncontrol MyEgress(inout headers_t hdr, inout local_metadata_t meta, inout standard_metadata_t standard_metadata) { apply { if (standard_metadata.egress_port == CPU_PORT) { hdr.packet_in.setValid (); hdr.packet_in.ingress_port = standard_metadata.ingress_port; hdr.packet_in._padding = 0; } } } 最後為了讓 packet-in header 能後被傳到 ONOS，需要再 deparser 加入該 header，注意 packet-out header 是為了讓 pipeline 能夠根據該 header 來處理 pecket-out header 用的，因此他不應該被加入到 deparser。\ncontrol MyDeparser(packet_out packet, in headers_t hdr) { apply { packet.emit (hdr.packet_in); packet.emit (hdr.ethernet); } } 接著回到我們的 interpreter，在 interpreter 內定義了兩個函數 mapInboundPacket 和 mapOutboundPacket，分別對應 packet-in 和 packet-out 封包的處理。先前我們在 p4 pipeline 定義了 packet_in 和 packet_out 的 header，這兩個函數最基本的功能是讀取和寫入這兩個 header 的資訊。由於這兩個函數的功能相對固定，因此可以直接從 ONOS 的 basic pipeline interpreter 複製過來修改。\n@Override public Collection\u0026lt;PiPacketOperation\u0026gt; mapOutboundPacket(OutboundPacket packet) throws PiInterpreterException { TrafficTreatment treatment = packet.treatment (); // 由於 outbound packet 的內容通常不用在 switch 上在做修改，因此我們只需要取得 //set output port 的 instruction // 當然如果有特別的功能需求，可以透過修改 pipeline 來支援更多 instruction List\u0026lt;Instructions.OutputInstruction\u0026gt; outInstructions = ... ImmutableList.Builder\u0026lt;PiPacketOperation\u0026gt; builder = ImmutableList.builder (); for (Instructions.OutputInstruction outInst : outInstructions) { ... // 這邊透過呼叫 createPiPacketOperation 來填入 packet_out header 的資訊 builder.add (createPiPacketOperation (packet.data (), outInst.port ().toLong ())); ... } return builder.build (); } private PiPacketOperation createPiPacketOperation(ByteBuffer data, long portNumber) throws PiInterpreterException { PiPacketMetadata metadata = createPacketMetadata (portNumber); return PiPacketOperation.builder () .withType (PACKET_OUT) .withData (copyFrom (data)) .withMetadatas (ImmutableList.of (metadata)) .build (); } @Override public InboundPacket mapInboundPacket(PiPacketOperation packetIn, DeviceId deviceId) throws PiInterpreterException { Ethernet ethPkt; ... ethPkt = Ethernet.deserializer ().deserialize (packetIn.data ().asArray (), 0, ... //packet_in header 的資訊會以 key-value 的方式存在 packetIn.metadatas Optional\u0026lt;PiPacketMetadata\u0026gt; packetMetadata = packetIn.metadatas () .stream ().filter (m -\u0026gt; m.id ().equals (INGRESS_PORT)) .findFirst () // 從中提取出 input port number ImmutableByteSequence portByteSequence = packetMetadata.get ().value (); short s = portByteSequence.asReadOnlyBuffer ().getShort (); ConnectPoint receivedFrom = new ConnectPoint(deviceId, PortNumber.portNumber (s)); ByteBuffer rawData = ByteBuffer.wrap (packetIn.data ().asArray ()); return new DefaultInboundPacket(receivedFrom, ethPkt, rawData); ... } 到此我們已經完成了 interpreter 的實現了，Interpreter 目前還包含 mapLogicalPortNumber 和 getOriginalDefaultAction ，不過基本的 interpreter 不需要實現這兩個功能，所以這邊就不再展開介紹。\n同樣使用範例程式碼時，可以在 ONOS CLI 使用 add-common-flow-rule \u0026lt;device id\u0026gt; \u0026lt;input port\u0026gt; \u0026lt;output port\u0026gt; 的方式來使用下達 ONOS 標準的 flow rule。詳情可以 參考檔案。\n撰寫 Pipeliner # 到目前為止我們已經完成了 pipeconf 的基本功能，可以下 flow rule 還有使用 packet-in/packet-out 的功能，不過到目前我們還是需要直接使用 flow rule，要知道 table id 對應的功能，為了能夠隱藏 table 的細節還有銜接 ONOS 內建的網路功能 APP，我們需要實作 pipeliner 讓我們的交換機支援 flow objective 的功能。\n和 Interpreter 類似，我們需要實作 Pipeliner 這個介面並繼承 AbstractHandlerBehaviour 然後在 PipeconfLoader 透過 addBehaviour (Pipeliner.class, SimpleSwitchPipeliner.class) 的方式加入。\npublic class SimpleSwitchPipeliner extends AbstractHandlerBehaviour implements Pipeliner { private final Logger log = getLogger (getClass ()); private FlowRuleService flowRuleService; private DeviceId deviceId; @Override public void init(DeviceId deviceId, PipelinerContext context) { this.deviceId = deviceId; this.flowRuleService = context.directory ().get (FlowRuleService.class); } } 首先我們實作 init 方法，pipeliner 和交換機之間是 1 對 1 的關係，因此當交換機被初始化的時候，可以透過 init 方法取得 device 的 id。context 最主要的部份是使用 directory ().get 方法。平常我們在 APP 開發時是使用 Reference annotation 來取得 onos 的 service，這邊我們可以直接透過 get 方法來取得 service，由於 pipeliner 需要完成 flow ojbective 到 flow rule 的轉換，並直接送到 flow rule service，因此這邊先取得 flow rule service。\n在 ONOS Flow Objective Service 的架構內，其實總共有三種 objective，分別是 forward、filter 和 next，他們都需要透過 pipeliner 來和 ONOS 核心互動，由於我們只想要實作 forwarding objective 的部分，因此 filter 和 next 可以單純回應不支援的錯誤訊息\n@Override public void filter(FilteringObjective obj) { obj.context ().ifPresent (c -\u0026gt; c.onError (obj, ObjectiveError.UNSUPPORTED)); } @Override public void next(NextObjective obj) { obj.context ().ifPresent (c -\u0026gt; c.onError (obj, ObjectiveError.UNSUPPORTED)); } @Override public List\u0026lt;String\u0026gt; getNextMappings(NextGroup nextGroup) { // We do not use nextObjectives or groups. return Collections.emptyList (); } 接著就到了我們的主角 forward objective，在實作邏輯上其實與 interpreter 對 treatment 的處理方式類似，forward 方法會取得一個 ForwardingObjective 物件，我們根據 treatment 和 selector 生成出一條或多條 flow rule，然後透過 flow rule service 下放到交換機上。\n@Override public void forward(ForwardingObjective obj) { if (obj.treatment () == null) { obj.context ().ifPresent (c -\u0026gt; c.onError (obj, ObjectiveError.UNSUPPORTED)); } // Simply create an equivalent FlowRule for table 0. final FlowRule.Builder ruleBuilder = DefaultFlowRule.builder () .forTable (MY_INGRESS_TABLE0_CONTROL_TABLE0) .forDevice (deviceId) .withSelector (obj.selector ()) .fromApp (obj.appId ()) .withPriority (obj.priority ()) .withTreatment (obj.treatment ()); if (obj.permanent ()) { ruleBuilder.makePermanent (); } else { ruleBuilder.makeTemporary (obj.timeout ()); } switch (obj.op ()) { case ADD: flowRuleService.applyFlowRules (ruleBuilder.build ()); break; case REMOVE: flowRuleService.removeFlowRules (ruleBuilder.build ()); break; default: log.warn (\u0026#34;Unknown operation {}\u0026#34;, obj.op ()); } obj.context ().ifPresent (c -\u0026gt; c.onSuccess (obj)); } 最後我們還需要實作 purgeAll，當刪除所有 flow obejctive 的時候，刪除所有的 flow rule，這邊我們只需要簡單呼叫 flow rule service 的 purgeFlowRules 就好。\n@Override public void purgeAll(ApplicationId appId) { flowRuleService.purgeFlowRules (deviceId, appId); } 到此我們已經完成了整個 pipeconf 的實作，可以透過 flow objective 的方式來管理交換機並與 ONOS 內建的 APP 整合，因此我們可以透過使用 proxyarp 和 fwd 兩個 APP 來讓我們的交換機能夠正常的工作。\n小結 # 以上就是 ONOS P4 Pipeconf 的基本開發教學，使用的範例程式碼在 github，如果有遇到任何問題或有說明不清楚的地方，歡迎留言提問，我會盡力為大家解答。\n參考資料 # https://hackmd.io/@cnsrl/onos_p4\nhttps://wiki.onosproject.org/pages/viewpage.action?pageId=16122675\nhttps://github.com/p4lang/tutorials/blob/master/exercises/basic/solution/basic.p4\n","date":"August 28 2022","externalUrl":null,"permalink":"/posts/onos-p4-switch-pipeconf-development/","section":"文章","summary":"","title":"ONOS-P4-Switch-Pipeconf-Development","type":"posts"},{"content":"","date":"August 28 2022","externalUrl":null,"permalink":"/tags/p4/","section":"標籤","summary":"","title":"P4","type":"tags"},{"content":"","date":"August 28 2022","externalUrl":null,"permalink":"/tags/%E8%BB%9F%E9%AB%94%E9%96%8B%E7%99%BC%E6%95%99%E5%AD%B8/","section":"標籤","summary":"","title":"軟體開發教學","type":"tags"},{"content":" ONOS 踩坑日記\n前言 # 最近嘗試使用 ONOS 目前最新的 2.7 版來開發 APP，用 OpenFlow 來讓交換機實現 router 的功能。結果踩到 ONOS Packet-in 封包處理實作未完全的坑。\n當封包經過 router 時，會根據 routing table 和封包的目標決定要往哪個 interface 送出，同時將封包的 source mac address 改為交換機的 mac address、封包的 destination mac address 改為 nexthop 的 mac address。因此我們需要在交換機上安裝一條 flow rule，selector 是 destination mac address，treatment 有三個 instructions 分別是：修改 src mac、dst mac 和決定 output port。\n為了減少交換機上的 flow entry 的數量，所以採用 reactive 的方式，也就是當交換機收到第一封包時，先將封包送 (packet-in) 給 SDN controller，controller 根據 routing table，直接修改該封包的 mac address，並從交換機特定的 port 送出 (packet-out)，同時生成對應的 flow rule 並安裝到交換機上，後續的封包就可以直接根據 flow rule 轉送而不用再經過 controller。\n問題 # 然而問題就出現在第一個封包上，根據 tcpdump 看到的結果，封包的 source 和 destination mac address 都沒有被修改到。\n由於我是使用 OVS 來模擬 Openflow 交換機，因此首先懷疑是不是 OVS 本身實作限制，不支援同時包含上述三個 instructions 導致。然而，後續經過 flow rule 直接送出的封包，都有成功修改到 mac address。由於只有第一個 packet-in 到 controller，再 packet-out 回 switch 的封包沒有被修改，因此開始懷疑是 ONOS 的問題。\n追蹤 # 在 ONOS 裡面，一般使用 PacketProcessor 的方式來處理 packet-in 到 controller 的封包。首先實作 PacketProcessor 介面，然後向 PacketService 註冊，ONOS 就會調用 processor 處理 packet-in 的封包。\nprivate PacketProcessor processor = new PacketProcessor() { @Override public void process(PacketContext context) { //.... 處理封包的邏輯 // 修改設定封包的 mac address 和決定 output port context.treatmentBuilder () .setEthSrc (srcMac) .setEthDst (dstMac) .setOutput (outPort.port ()); context.send (); // 將封包 packet-out 回交換機 } }; @Activate protected void activate() { packetService.addProcessor (processor, PacketProcessor.director (1)); } PacketContext 會包含 packet-in 進來的封包內容，並可透過 context.treatmentBuilder 修改封包和決定要往哪個 port 送出去，最後透過 send 指令，packet-out 回交換機。\n搜查一下 ONOS 的原始碼，會在 core/api 下面找到 DefaultPacketContext，這個 class 實作了 PacketContext 這個 Interface，但是這個 class 是一個 abstract class，因此一定有人繼承了它，繼續搜查 PacketContext 這個字會找到兩個跟 Openflow 相關的，DefaultOpenFlowPacketContext 和 OpenFlowCorePacketContext，但是後者才有繼承 DefaultPacketContext 和實作 PacketContext 介面，因此 PacketProcesser 在處理 openflow packet-in 進來的封包時，拿到的 PacketContext 具體應該是 OpenFlowCorePacketContext 這個 class。\n打開 OpenFlowCorePacketContext.java 會看到它實現了 send 這個 function，經過簡單的檢查後呼叫 sendPacket 這個 function，然後你就會看到…\nprivate void sendPacket(Ethernet eth) { List\u0026lt;Instruction\u0026gt; ins = treatmentBuilder ().build ().allInstructions (); OFPort p = null; // TODO: support arbitrary list of treatments must be supported in ofPacketContext for (Instruction i : ins) { if (i.type () == Type.OUTPUT) { p = buildPort (((OutputInstruction) i).port ()); break; //for now... } } ....... } 謎底揭曉，原來 ONOS 只有實作 output 這個 instruction (決定 output port)，因此它直接忽略的 set source mac 和 set destination mac 兩個指令，交換機送出來的封包當然就只有往對的 port 送，而沒有改到 mac address。\n結論 # 結論就是在當前 ONOS 2.7 環境下，PacketProcesser 在處理 Openflow 交換機封包 packet-out 的時候，只能決定該封包的 output port，其餘對該封包的修改都是無效的。\n參考資料 # ONOS Source code ","date":"August 12 2022","externalUrl":null,"permalink":"/posts/analyze-why-onos-packet-processor-treatment-not-work/","section":"文章","summary":"","title":"分析 ONOS Packet Processor Treatment 無效之原因","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/awesome/","section":"","summary":"","title":"Awesome","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]